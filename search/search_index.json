{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to UAB Research Computing Docs \u00b6 Welcome to UAB Research Computing! The Research Computing System (RCS) provides a framework for sharing data, accessing compute power, and collaborating with peers on campus and around the globe. Our goal is to construct a dynamic \"network of services\" that you can use to organize, study and share your research data. Research Computing provides services to researchers in these core areas: Data Analysis : using the High Performance Computing (HPC) fabric Cheaha for batch data processing. Data Sharing : supporting trusted information exchange to spark new ideas via our Storage system. Application Development : providing virtual machines and web-hosted development tools empowering researcher via our cloud.rc fabric. Contact Us \u00b6 To create a support ticket please reach out to us via email at support@listserv.uab.edu . You can also visit us in our Zoom office hours held weekly: Mondays 10:00 AM to 12:00 PM: Zoom Thursdays 10:00 AM to 12:00 PM: Zoom For additional information about support please see Support . Support and Development \u00b6 RCS is developed and supported by UAB IT's Research Computing Group. We are developing a core set of applications to help you to easily incorporate our services into your research processes and this documentation collection to help you leverage the resources already available. We follow the best practices of the Open Source community and develop our software in an open-source fashion. RCS is an out growth of the UABgrid pilot, launched in September 2007 which has focused on demonstrating the utility of unlimited analysis, storage, and application for research. RCS is built on the same technology foundations used by major cloud vendors and decades of distributed systems computing research, technology that powered the last ten years of large scale systems serving prominent national and international initiatives like the Open Science Grid , XSEDE , the LHC Computing Grid , and NCIP . Outreach \u00b6 The UAB IT Research Computing Group has collaborated with a number of prominent research projects at UAB to identify use cases and develop requirements. Our collaborators include, but are not limited to, the Center for Clinical and Translational Science (CCTS), Heflin Genomics Center, the Comprehensive Cancer Center (CCC), the Department of Computer and Information Sciences (CIS), the Department of Mechanical Engineering (ME), Lister Hill Library, the School of Optometry's Center for the Development of Functional Imaging, and Health System Information Services (HSIS). As part of the process of building RC, the UAB IT Research Computing Group has hosted an annual campus symposium on research computing and cyber-infrastructure (CI) developments and accomplishments. Starting as CyberInfrastructure (CI) Days in 2007, the name was changed to UAB Research Computing Day in 2011 to reflect the broader mission to support research. IT Research Computing also participates in other campus wide symposiums including UAB Research Core Day.","title":"Home"},{"location":"#welcome-to-uab-research-computing-docs","text":"Welcome to UAB Research Computing! The Research Computing System (RCS) provides a framework for sharing data, accessing compute power, and collaborating with peers on campus and around the globe. Our goal is to construct a dynamic \"network of services\" that you can use to organize, study and share your research data. Research Computing provides services to researchers in these core areas: Data Analysis : using the High Performance Computing (HPC) fabric Cheaha for batch data processing. Data Sharing : supporting trusted information exchange to spark new ideas via our Storage system. Application Development : providing virtual machines and web-hosted development tools empowering researcher via our cloud.rc fabric.","title":"Welcome to UAB Research Computing Docs"},{"location":"#contact-us","text":"To create a support ticket please reach out to us via email at support@listserv.uab.edu . You can also visit us in our Zoom office hours held weekly: Mondays 10:00 AM to 12:00 PM: Zoom Thursdays 10:00 AM to 12:00 PM: Zoom For additional information about support please see Support .","title":"Contact Us"},{"location":"#support-and-development","text":"RCS is developed and supported by UAB IT's Research Computing Group. We are developing a core set of applications to help you to easily incorporate our services into your research processes and this documentation collection to help you leverage the resources already available. We follow the best practices of the Open Source community and develop our software in an open-source fashion. RCS is an out growth of the UABgrid pilot, launched in September 2007 which has focused on demonstrating the utility of unlimited analysis, storage, and application for research. RCS is built on the same technology foundations used by major cloud vendors and decades of distributed systems computing research, technology that powered the last ten years of large scale systems serving prominent national and international initiatives like the Open Science Grid , XSEDE , the LHC Computing Grid , and NCIP .","title":"Support and Development"},{"location":"#outreach","text":"The UAB IT Research Computing Group has collaborated with a number of prominent research projects at UAB to identify use cases and develop requirements. Our collaborators include, but are not limited to, the Center for Clinical and Translational Science (CCTS), Heflin Genomics Center, the Comprehensive Cancer Center (CCC), the Department of Computer and Information Sciences (CIS), the Department of Mechanical Engineering (ME), Lister Hill Library, the School of Optometry's Center for the Development of Functional Imaging, and Health System Information Services (HSIS). As part of the process of building RC, the UAB IT Research Computing Group has hosted an annual campus symposium on research computing and cyber-infrastructure (CI) developments and accomplishments. Starting as CyberInfrastructure (CI) Days in 2007, the name was changed to UAB Research Computing Day in 2011 to reflect the broader mission to support research. IT Research Computing also participates in other campus wide symposiums including UAB Research Core Day.","title":"Outreach"},{"location":"contributor_guide/","text":"Contributor Guide \u00b6 We welcome contributions from our community. To ensure a high-quality documentation experience, we have some guidelines for contributors who wish to create. Construction This contributor guide is a work in progress and may change and develop over time. Please bear with us, and feel free to offer suggestions. Prerequisites \u00b6 We are using Visual Studio Code (VSCode) for development with the following extensions installed. While VSCode is not required, it can help with automating formatting, linting and Anaconda environment management. VSCode may be obtained from Visual Studio Code and documentation is available at VSCode: Docs . Several extensions are useful for this project and are listed below. Extensions may be obtained by searching the Extensions Menu in VSCode by pressing ctrl + shift + x . More information on managing extensions is available at VSCode: Extension Marketplace . Python [ ms-python.python ] (for Anaconda environment management) Markdown All in One [ yzhang.markdown-all-in-one ] markdownlint [ DavidAnson.vscode-markdownlint ] Prettier - Code formatter [ esbenp.prettier-vscode ] To make the best use of formatting extensions for this project, please add the following block to your settings.json file. These changes can be made: Within the VSCode project file in the .vscode folder, affecting only this project --OR-- To the global VSCode settings file, affecting all projects. To change the global file, press ctrl + shift + p to open the Command Palette, then search for Preferences: Open Settings (JSON) and append the following content. VSCode settings.json Additions \u00b6 \"[html]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"[markdown]\" : { \"editor.defaultFormatter\" : \"yzhang.markdown-all-in-one\" }, \"[yaml]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"markdownlint.config\" : { \"MD046\" : { \"style\" : \"fenced\" } }, \"markdown.extension.list.indentationSize\" : \"inherit\" Before you can get started working on contributions, you'll need a copy of the repository. The first step, done only once, is to fork the repository in GitHub to your personal account. The repository is located at https://github.com/uabrc/uabrc.github.io . More in-depth documentation on forking can be found at GitHub: Fork a Repo . Once the fork has been created, you can clone your fork using the Command Palette ( ctrl + shift + p ) and Git: Clone... in VSCode, or at the command line. More information on cloning can be found at GitHub: Cloning a Repository . Local Machine Setup (Laptop/Desktop) \u00b6 The Python extension will activate when you open any Python file. The file test.py has been added for convenience, simply open that file to activate the extension. The extension will show the currently activated environment near the bottom-left corner of the VSCode window. Please create an Anaconda environment using build_env.yml using the following. conda env create -f build_env.yml Activate the environment in VSCode by clicking the currently activated environment in the bottom-left of the VSCode window. A menu will appear allowing you to select from discovered environments. You may need to reload VSCode to get the environment to appear if it was just created. To build the documentation locally, use mkdocs build . Be sure to fix all warnings before submitting a pull request. GitHub Setup \u00b6 To view your changes as they would appear on the official documentation website, you'll need to set up GitHub Pages in your fork. Navigate to your fork repository URL and click the \"Settings\" tab. Click \"Pages\" under \"Code and automation\". Follow the instructions here to set up a source: GitHub Pages Docs . Workflow \u00b6 Open the cloned repository in VSCode. Create a branch with a short but meaningful name for your intended changes. Make changes to markdown files. Commit changes to the new branch. Push the branch to your remote repository on GitHub. Check your contributions match Make a pull request to the upstream repository. From here your pull request will go through a review process. The following criteria are checked. No linting errors Correct formatting Image alternate text (alt text) Images must use the gallery functionality, formatted as ![!alttext](path/to/file) . Note the leading ! in the alttext. Valid internal and external links We will also make an attempt to check your information for accuracy, as well as proofread the text. Bear in mind our time is limited and we are not infallible, so please double-check your pull requests! File Organization \u00b6 To Be Determined Formatting \u00b6 Links must be one of the following formats including all punctuation and brackets: Bare: <https://google.com> <support@listserv.uab.edu> Named with a schema: [website](https://google.com) [email](mailto:support@listserv.uab.edu) note the mailto: schema! Relative internal [relative internal](help/faq.md) All internal links must be relative. For example, use ./file.md not /docs/file.md . Redirects \u00b6 If a page name must change, or the location of a page must change, it is necessary to create a redirect for that page so we don't break bookmarks and incoming links targeting our documentation. Redirecting pages is possible using the plugin at https://github.com/datarobot/mkdocs-redirects . To redirect a page, add a line in mkdocs.yml under the following keys. The line takes the form original page location: new page location , and each side of : must be a full path under docs/ . An example is below. plugins : - redirects : redirect_maps : account_management/uab_researcher.md : account_management/cheaha_account.md Section Index Pages \u00b6 To create a section index page: Create a file called index.md in a reasonable directory location for that section. Add something like the following to that section entry under nav: in mkdocs.yml . nav: ... - Cheaha Guide: - cheaha/index.md # add the index.md here. - Hardware: ... - ... ... When a site visitor clicks Cheaha Guide in the nav pane, the page cheaha/index.md will be loaded. Linting Known Issues \u00b6 There are known issues with the markdown linter and some of our non-standard plugins, especially admonitions (specifically a conflict involving fenced vs indented code blocks). To fix these cases please use one of the following methods. The <lint warning code> can be found by hovering over the yellow squiggles in VSCode to bring up the warning lens. Please do not use these to silence all linter warnings, only for fixing known issues. Please read the warning lenses given by VSCode to identify the cause of the warning. Silence Linter Warning for a Block \u00b6 <!-- markdownlint-disable <lint warning code> --> `linter error here` `maybe multiple lines` <!-- markdownlint-enable <lint warning code> --> Silence Linter Warning for a Single Line \u00b6 We encourage denoting the warning being silenced here by filling out the <lint warning code> , though it isn't required for the single line case. <!-- markdownlint-disable-next-line <lint warning code> --> `linter error here just for this line` False Positive Lint Warnings from Admonitions \u00b6 We allow and encourage the use of admonitions in our documentation, where appropriate. Because these are created using a plugin and are \"non-standard\" markdown , the VSCode markdownlint extension does not recognize admonitions and may produce a false positive warning about inconsistent code block styles. Two styles of code block are allowed in markdown : fenced and indented . To work around the false positive warning about admonitions, we require all code blocks to be fenced . This is enforced by adding an entry to the VSCode settings.json file . Now all admonitions will be consistently assigned the warning MD046 , which can be disabled by placing all admonitions in between the following comment block fences. The comment lines must be indented to the same level as the start of the admonition. <!-- markdownlint-disable MD046 --> <!-- markdownlint-enable MD046 --> The process can be simplified in VSCode using snippets. Bring up the command palette and type snippets and open Preferences: Configure User Snippets . Then type markdown and open it. A json file will be opened. Add the following content between the outermost braces and then save the file. \"Disable Markdown Lint MD046 for a Block\" : { \"prefix\" : \"md046 disable\" , \"body\" : [ \"<!-- markdownlint-disable MD046 -->\" , \"$TM_SELECTED_TEXT\" , \"<!-- markdownlint-enable MD046 -->\" ], \"description\" : \"Disables warning Markdown Lint MD046 for the selected block.\" } The snippet will surround selected text with the appropriate linter disable fencing for MD046. To use the snippet, opne the IntelliSense lens using ctrl + space (or cmd + space ), then type md046 until the prefix shows up as the first entry in the list. Then press tab . The workaround is needed because markdownlint has no plans to add support for admonitions. There is no markdownlint plugin for that support either, and we don't have the ability to develop such a plugin. Slurm Hardware, Partitions, QoS Tables \u00b6 Building hardware tables is a semi-automated script based on a manually curated table. The repository is located here: https://gitlab.rc.uab.edu/rc-data-science/data-science-internal/cluster-fabric-docs . Be warned that the scripts may move to a github.com repo in the future. Building Partition and QoS tables is automated based on scontrol output. The repository is located here: https://github.com/wwarriner/slurm_status_tools . Be warned the link may change in the future when we fork to the UABRC group that houses the docs repo. Accessibility \u00b6 Color vision deficiency checker: https://www.toptal.com/designers/colorfilter/ Contrast checker: https://webaim.org/resources/contrastchecker/ Branding Guidance \u00b6 Brand main page: https://www.uab.edu/toolkit/branding Brand colors: https://www.uab.edu/toolkit/brand-basics/colors Copyright guidance: https://www.uab.edu/toolkit/trademarks-licensing/uab-trademarks Terminology \u00b6 Research Computing (RC) for the IT group supporting campus HPC resources. UAB Campus Network for the hard-wired network physically located on UAB campus, and the UAB Secure wifi. UAB Campus VPN for the VPN to tunnel remote connections through the UAB Campus Network.","title":"Contributing To The Docs"},{"location":"contributor_guide/#contributor-guide","text":"We welcome contributions from our community. To ensure a high-quality documentation experience, we have some guidelines for contributors who wish to create. Construction This contributor guide is a work in progress and may change and develop over time. Please bear with us, and feel free to offer suggestions.","title":"Contributor Guide"},{"location":"contributor_guide/#prerequisites","text":"We are using Visual Studio Code (VSCode) for development with the following extensions installed. While VSCode is not required, it can help with automating formatting, linting and Anaconda environment management. VSCode may be obtained from Visual Studio Code and documentation is available at VSCode: Docs . Several extensions are useful for this project and are listed below. Extensions may be obtained by searching the Extensions Menu in VSCode by pressing ctrl + shift + x . More information on managing extensions is available at VSCode: Extension Marketplace . Python [ ms-python.python ] (for Anaconda environment management) Markdown All in One [ yzhang.markdown-all-in-one ] markdownlint [ DavidAnson.vscode-markdownlint ] Prettier - Code formatter [ esbenp.prettier-vscode ] To make the best use of formatting extensions for this project, please add the following block to your settings.json file. These changes can be made: Within the VSCode project file in the .vscode folder, affecting only this project --OR-- To the global VSCode settings file, affecting all projects. To change the global file, press ctrl + shift + p to open the Command Palette, then search for Preferences: Open Settings (JSON) and append the following content.","title":"Prerequisites"},{"location":"contributor_guide/#vscode-settingsjson-additions","text":"\"[html]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"[markdown]\" : { \"editor.defaultFormatter\" : \"yzhang.markdown-all-in-one\" }, \"[yaml]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"markdownlint.config\" : { \"MD046\" : { \"style\" : \"fenced\" } }, \"markdown.extension.list.indentationSize\" : \"inherit\" Before you can get started working on contributions, you'll need a copy of the repository. The first step, done only once, is to fork the repository in GitHub to your personal account. The repository is located at https://github.com/uabrc/uabrc.github.io . More in-depth documentation on forking can be found at GitHub: Fork a Repo . Once the fork has been created, you can clone your fork using the Command Palette ( ctrl + shift + p ) and Git: Clone... in VSCode, or at the command line. More information on cloning can be found at GitHub: Cloning a Repository .","title":"VSCode settings.json Additions"},{"location":"contributor_guide/#local-machine-setup-laptopdesktop","text":"The Python extension will activate when you open any Python file. The file test.py has been added for convenience, simply open that file to activate the extension. The extension will show the currently activated environment near the bottom-left corner of the VSCode window. Please create an Anaconda environment using build_env.yml using the following. conda env create -f build_env.yml Activate the environment in VSCode by clicking the currently activated environment in the bottom-left of the VSCode window. A menu will appear allowing you to select from discovered environments. You may need to reload VSCode to get the environment to appear if it was just created. To build the documentation locally, use mkdocs build . Be sure to fix all warnings before submitting a pull request.","title":"Local Machine Setup (Laptop/Desktop)"},{"location":"contributor_guide/#github-setup","text":"To view your changes as they would appear on the official documentation website, you'll need to set up GitHub Pages in your fork. Navigate to your fork repository URL and click the \"Settings\" tab. Click \"Pages\" under \"Code and automation\". Follow the instructions here to set up a source: GitHub Pages Docs .","title":"GitHub Setup"},{"location":"contributor_guide/#workflow","text":"Open the cloned repository in VSCode. Create a branch with a short but meaningful name for your intended changes. Make changes to markdown files. Commit changes to the new branch. Push the branch to your remote repository on GitHub. Check your contributions match Make a pull request to the upstream repository. From here your pull request will go through a review process. The following criteria are checked. No linting errors Correct formatting Image alternate text (alt text) Images must use the gallery functionality, formatted as ![!alttext](path/to/file) . Note the leading ! in the alttext. Valid internal and external links We will also make an attempt to check your information for accuracy, as well as proofread the text. Bear in mind our time is limited and we are not infallible, so please double-check your pull requests!","title":"Workflow"},{"location":"contributor_guide/#file-organization","text":"To Be Determined","title":"File Organization"},{"location":"contributor_guide/#formatting","text":"Links must be one of the following formats including all punctuation and brackets: Bare: <https://google.com> <support@listserv.uab.edu> Named with a schema: [website](https://google.com) [email](mailto:support@listserv.uab.edu) note the mailto: schema! Relative internal [relative internal](help/faq.md) All internal links must be relative. For example, use ./file.md not /docs/file.md .","title":"Formatting"},{"location":"contributor_guide/#redirects","text":"If a page name must change, or the location of a page must change, it is necessary to create a redirect for that page so we don't break bookmarks and incoming links targeting our documentation. Redirecting pages is possible using the plugin at https://github.com/datarobot/mkdocs-redirects . To redirect a page, add a line in mkdocs.yml under the following keys. The line takes the form original page location: new page location , and each side of : must be a full path under docs/ . An example is below. plugins : - redirects : redirect_maps : account_management/uab_researcher.md : account_management/cheaha_account.md","title":"Redirects"},{"location":"contributor_guide/#section-index-pages","text":"To create a section index page: Create a file called index.md in a reasonable directory location for that section. Add something like the following to that section entry under nav: in mkdocs.yml . nav: ... - Cheaha Guide: - cheaha/index.md # add the index.md here. - Hardware: ... - ... ... When a site visitor clicks Cheaha Guide in the nav pane, the page cheaha/index.md will be loaded.","title":"Section Index Pages"},{"location":"contributor_guide/#linting-known-issues","text":"There are known issues with the markdown linter and some of our non-standard plugins, especially admonitions (specifically a conflict involving fenced vs indented code blocks). To fix these cases please use one of the following methods. The <lint warning code> can be found by hovering over the yellow squiggles in VSCode to bring up the warning lens. Please do not use these to silence all linter warnings, only for fixing known issues. Please read the warning lenses given by VSCode to identify the cause of the warning.","title":"Linting Known Issues"},{"location":"contributor_guide/#silence-linter-warning-for-a-block","text":"<!-- markdownlint-disable <lint warning code> --> `linter error here` `maybe multiple lines` <!-- markdownlint-enable <lint warning code> -->","title":"Silence Linter Warning for a Block"},{"location":"contributor_guide/#silence-linter-warning-for-a-single-line","text":"We encourage denoting the warning being silenced here by filling out the <lint warning code> , though it isn't required for the single line case. <!-- markdownlint-disable-next-line <lint warning code> --> `linter error here just for this line`","title":"Silence Linter Warning for a Single Line"},{"location":"contributor_guide/#false-positive-lint-warnings-from-admonitions","text":"We allow and encourage the use of admonitions in our documentation, where appropriate. Because these are created using a plugin and are \"non-standard\" markdown , the VSCode markdownlint extension does not recognize admonitions and may produce a false positive warning about inconsistent code block styles. Two styles of code block are allowed in markdown : fenced and indented . To work around the false positive warning about admonitions, we require all code blocks to be fenced . This is enforced by adding an entry to the VSCode settings.json file . Now all admonitions will be consistently assigned the warning MD046 , which can be disabled by placing all admonitions in between the following comment block fences. The comment lines must be indented to the same level as the start of the admonition. <!-- markdownlint-disable MD046 --> <!-- markdownlint-enable MD046 --> The process can be simplified in VSCode using snippets. Bring up the command palette and type snippets and open Preferences: Configure User Snippets . Then type markdown and open it. A json file will be opened. Add the following content between the outermost braces and then save the file. \"Disable Markdown Lint MD046 for a Block\" : { \"prefix\" : \"md046 disable\" , \"body\" : [ \"<!-- markdownlint-disable MD046 -->\" , \"$TM_SELECTED_TEXT\" , \"<!-- markdownlint-enable MD046 -->\" ], \"description\" : \"Disables warning Markdown Lint MD046 for the selected block.\" } The snippet will surround selected text with the appropriate linter disable fencing for MD046. To use the snippet, opne the IntelliSense lens using ctrl + space (or cmd + space ), then type md046 until the prefix shows up as the first entry in the list. Then press tab . The workaround is needed because markdownlint has no plans to add support for admonitions. There is no markdownlint plugin for that support either, and we don't have the ability to develop such a plugin.","title":"False Positive Lint Warnings from Admonitions"},{"location":"contributor_guide/#slurm-hardware-partitions-qos-tables","text":"Building hardware tables is a semi-automated script based on a manually curated table. The repository is located here: https://gitlab.rc.uab.edu/rc-data-science/data-science-internal/cluster-fabric-docs . Be warned that the scripts may move to a github.com repo in the future. Building Partition and QoS tables is automated based on scontrol output. The repository is located here: https://github.com/wwarriner/slurm_status_tools . Be warned the link may change in the future when we fork to the UABRC group that houses the docs repo.","title":"Slurm Hardware, Partitions, QoS Tables"},{"location":"contributor_guide/#accessibility","text":"Color vision deficiency checker: https://www.toptal.com/designers/colorfilter/ Contrast checker: https://webaim.org/resources/contrastchecker/","title":"Accessibility"},{"location":"contributor_guide/#branding-guidance","text":"Brand main page: https://www.uab.edu/toolkit/branding Brand colors: https://www.uab.edu/toolkit/brand-basics/colors Copyright guidance: https://www.uab.edu/toolkit/trademarks-licensing/uab-trademarks","title":"Branding Guidance"},{"location":"contributor_guide/#terminology","text":"Research Computing (RC) for the IT group supporting campus HPC resources. UAB Campus Network for the hard-wired network physically located on UAB campus, and the UAB Secure wifi. UAB Campus VPN for the VPN to tunnel remote connections through the UAB Campus Network.","title":"Terminology"},{"location":"policies/","text":"UAB Research Computing Policies \u00b6 Construction This page is a stub and is under construction. UAB IT Policies \u00b6 All users of UAB IT systems, including UAB Research Computing systems, must agree to all UAB relevant UAB IT policies. UAB IT policies may be found at https://www.uab.edu/it/home/policies . The list of policies below are those that may be most relevant to typical use cases on UAB Research Computing systems. The list below is not an exhaustive list, and it is the responsibility of each individual using the system to be aware of and follow all relevant policies, whether listed here or not. Acceptable Use Policy (AUP) \u00b6 Acceptable Use Policy Data Classification \u00b6 Data Classification","title":"Policies"},{"location":"policies/#uab-research-computing-policies","text":"Construction This page is a stub and is under construction.","title":"UAB Research Computing Policies"},{"location":"policies/#uab-it-policies","text":"All users of UAB IT systems, including UAB Research Computing systems, must agree to all UAB relevant UAB IT policies. UAB IT policies may be found at https://www.uab.edu/it/home/policies . The list of policies below are those that may be most relevant to typical use cases on UAB Research Computing systems. The list below is not an exhaustive list, and it is the responsibility of each individual using the system to be aware of and follow all relevant policies, whether listed here or not.","title":"UAB IT Policies"},{"location":"policies/#acceptable-use-policy-aup","text":"Acceptable Use Policy","title":"Acceptable Use Policy (AUP)"},{"location":"policies/#data-classification","text":"Data Classification","title":"Data Classification"},{"location":"account_management/cheaha_account/","text":"Cheaha Account Management \u00b6 These instructions are intended to guide researchers on creating new accounts and managing existing accounts. Creating a New Account \u00b6 Creating a new account is a simple, automated, self-service process. To start, navigate to https://rc.uab.edu , our Open OnDemand web portal, and authenticate. The authentication process differs depending on your affiliation. Accounts are available to researchers with the following situations. If you are affiliated with UAB and have a BlazerID, please authenticate using Single Sign-On (SSO). If you are affiliated with UAB Medicine, you will need to use your BlazerID to authenticate via Single Sign-On (SSO) instead of your UABMC authentication process. If you are an external collaborator and have a XIAS account with access to Cheaha, please authenticate using your XIAS credentials. If you are an external collaborator and do not have a XIAS account, you will need a UAB-affiliated sponsor and will need to follow our XIAS Guest Account Instructions . Your sponsor will need to follow our XIAS Sites and XIAS Users documentation pages. Once you have authenticated, you should see a page that looks like the following. The form should be prefilled with your BlazerID or XIAS ID, full name, and email address. If any of these details are incorrect please Contact Support . Please also fill out the reason you wish to create an account. To create a Cheaha account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Create Account\" to start the account creation process. You should see a popup notification that looks like the following. After a few moments you should be redirected to our Open OnDemand web portal. If not, please Contact Support . Welcome to Cheaha and to Research Computing! Managing an Existing Account \u00b6 If you already have an account and wish to check it's status, please visit your account status page at https://rc.uab.edu/account . Account in Good Standing \u00b6 If your account is in good standing you should see a page like the following. Account Requires Certification \u00b6 We review accounts periodically to ensure the system is being used fairly and as intended. Part of this process is to certify that researchers with accounts still wish to make use of Cheaha. Once per year every researcher will be required to certify their account before making use of Cheaha. If you account requires certification, when logging in to our Open OnDemand web portal you will see the following notification page. . To certify your account, click the button to be taken to the certification form, which should look like the following. While the certification form looks similar to the new account creation form, please be sure to review all of the information carefully. To certify your account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Certify Account\" to start the account creation process. You should see a popup notification confirming the process is working. After a few moments your account should be certified and you will be free to use Cheaha again. If not, please Contact Support . Account on Hold \u00b6 Mistakes happen, and sometimes what we thought we programmed wasn't quite what we actually programmed. When these kinds of mistakes occur, excess resources may get used. If this impacts performance or other users excessively, we may put a hold on your account. We may also put a hold on your account if you do not complete Account Certification when required. Other reasons for holds include, but are not limited to: Misuse (intentional or not) of Research Computing resources IT Policy violations HIPAA or FERPA violations related to use of Cheaha As part of a required investigation In rare circumstances, we may also place a hold on your account if you possess the sole copy of data not owned by you. If your account is on hold, you will see a page like the following. If you SSH into the cluster while your account is on hold you will see the following text in your terminal. If your account is on hold and we have not already contacted you, or you believe the hold to be in error, please Contact Support . Authorization Error \u00b6 Periodically, we review all researcher accounts to ensure they are authorized to use Cheaha based on affiliation status. If we find a researcher is no longer affiliated with UAB, we may disable the account. If you are not authorized to use Cheaha, you will see a page like the following. If you believe this to be in error, please Contact Support .","title":"Cheaha Account"},{"location":"account_management/cheaha_account/#cheaha-account-management","text":"These instructions are intended to guide researchers on creating new accounts and managing existing accounts.","title":"Cheaha Account Management"},{"location":"account_management/cheaha_account/#creating-a-new-account","text":"Creating a new account is a simple, automated, self-service process. To start, navigate to https://rc.uab.edu , our Open OnDemand web portal, and authenticate. The authentication process differs depending on your affiliation. Accounts are available to researchers with the following situations. If you are affiliated with UAB and have a BlazerID, please authenticate using Single Sign-On (SSO). If you are affiliated with UAB Medicine, you will need to use your BlazerID to authenticate via Single Sign-On (SSO) instead of your UABMC authentication process. If you are an external collaborator and have a XIAS account with access to Cheaha, please authenticate using your XIAS credentials. If you are an external collaborator and do not have a XIAS account, you will need a UAB-affiliated sponsor and will need to follow our XIAS Guest Account Instructions . Your sponsor will need to follow our XIAS Sites and XIAS Users documentation pages. Once you have authenticated, you should see a page that looks like the following. The form should be prefilled with your BlazerID or XIAS ID, full name, and email address. If any of these details are incorrect please Contact Support . Please also fill out the reason you wish to create an account. To create a Cheaha account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Create Account\" to start the account creation process. You should see a popup notification that looks like the following. After a few moments you should be redirected to our Open OnDemand web portal. If not, please Contact Support . Welcome to Cheaha and to Research Computing!","title":"Creating a New Account"},{"location":"account_management/cheaha_account/#managing-an-existing-account","text":"If you already have an account and wish to check it's status, please visit your account status page at https://rc.uab.edu/account .","title":"Managing an Existing Account"},{"location":"account_management/cheaha_account/#account-in-good-standing","text":"If your account is in good standing you should see a page like the following.","title":"Account in Good Standing"},{"location":"account_management/cheaha_account/#account-requires-certification","text":"We review accounts periodically to ensure the system is being used fairly and as intended. Part of this process is to certify that researchers with accounts still wish to make use of Cheaha. Once per year every researcher will be required to certify their account before making use of Cheaha. If you account requires certification, when logging in to our Open OnDemand web portal you will see the following notification page. . To certify your account, click the button to be taken to the certification form, which should look like the following. While the certification form looks similar to the new account creation form, please be sure to review all of the information carefully. To certify your account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Certify Account\" to start the account creation process. You should see a popup notification confirming the process is working. After a few moments your account should be certified and you will be free to use Cheaha again. If not, please Contact Support .","title":"Account Requires Certification"},{"location":"account_management/cheaha_account/#account-on-hold","text":"Mistakes happen, and sometimes what we thought we programmed wasn't quite what we actually programmed. When these kinds of mistakes occur, excess resources may get used. If this impacts performance or other users excessively, we may put a hold on your account. We may also put a hold on your account if you do not complete Account Certification when required. Other reasons for holds include, but are not limited to: Misuse (intentional or not) of Research Computing resources IT Policy violations HIPAA or FERPA violations related to use of Cheaha As part of a required investigation In rare circumstances, we may also place a hold on your account if you possess the sole copy of data not owned by you. If your account is on hold, you will see a page like the following. If you SSH into the cluster while your account is on hold you will see the following text in your terminal. If your account is on hold and we have not already contacted you, or you believe the hold to be in error, please Contact Support .","title":"Account on Hold"},{"location":"account_management/cheaha_account/#authorization-error","text":"Periodically, we review all researcher accounts to ensure they are authorized to use Cheaha based on affiliation status. If we find a researcher is no longer affiliated with UAB, we may disable the account. If you are not authorized to use Cheaha, you will see a page like the following. If you believe this to be in error, please Contact Support .","title":"Authorization Error"},{"location":"account_management/gitlab_account/","text":"UAB GitLab Overview and Registration \u00b6 Welcome to UAB GitLab! This is a UAB-specific GitLab . GitLab is similar to GitHub , but hosted here at UAB on secure servers. GitLab Use Cases \u00b6 For Researchers \u00b6 GitLab can be used: For reproducibility Analysis and software code can be kept in one, central repository everyone can use instead of spread across multiple computers/places. Code can be versioned and tracked as it changes over time. Software versions can be recorded, virtual environments can be documented, and containers can be recorded to help future-proof analyses. Collaboration GitLab is a central place to create code, edit, and track needed code changes (issues) with your lab and collaborators. Multiple people can use, modify, and merge changes in code while communicating with the broader team all along the way. Security Unlimited private repositories for internal code projects. Set behind UAB authentication. For Software Developers (and Researchers!) \u00b6 UAB GitLab is useful for software developers. It is a single application for the entire software development lifecycle. From project planning and source code management to continuous integration (CI) and continuous deployment (CD), monitoring, and security. Our GitLab instance may be found at https://gitlab.rc.uab.edu . UAB GitLab Registration \u00b6 UAB-Affiliated Researcher Registration \u00b6 If you are a UAB affiliated researcher and have a BlazerID, you may create an account by logging in at the site above using the ldap tab. Please use your single sign-on (SSO) credentials. Please use BlazerID and password instead of UABMC credentials Please use your BlazerID and BlazerID password for UAB GitLab. UABMC credentials are a different sign in system and will likely not work. Central IT groups like Research Computing do not have a way to access UABMC credentials. UABMC Researcher Registration \u00b6 Please use your BlazerID and BlazerID credentials to sign in following the directions for UAB-Affiliated Researchers. UABMC credentials should not be used for UAB GitLab. XIAS External Collaborator Registration \u00b6 If you are a collaborator with a XIAS account you'll need to follow a different procedure. Ensure that your sponsor has included https://gitlab.rc.uab.edu in the list of approved URIs on the XIAS configuration page. Email support@listserv.uab.edu providing your full name, XIAS account email address, and sponsor. UAB Research Computing will create the account. You will recieve an email from gitlab.rc.uab.edu with a link to create a password. Navigate to https://gitlab.rc.uab.edu . Click the Standard tab. In the Username or email field type the part of your XIAS email address before the @ symbol. Do not include the @ symbol or anything after it. Fill out the Password field with the GitLab password you created in Step #4. Click Sign in . Warning XIAS account researchers can only be granted access if their sponsor adds the GitLab URL to the list of approved URIs. Please see XIAS Sites for more information.","title":"Gitlab Account"},{"location":"account_management/gitlab_account/#uab-gitlab-overview-and-registration","text":"Welcome to UAB GitLab! This is a UAB-specific GitLab . GitLab is similar to GitHub , but hosted here at UAB on secure servers.","title":"UAB GitLab Overview and Registration"},{"location":"account_management/gitlab_account/#gitlab-use-cases","text":"","title":"GitLab Use Cases"},{"location":"account_management/gitlab_account/#for-researchers","text":"GitLab can be used: For reproducibility Analysis and software code can be kept in one, central repository everyone can use instead of spread across multiple computers/places. Code can be versioned and tracked as it changes over time. Software versions can be recorded, virtual environments can be documented, and containers can be recorded to help future-proof analyses. Collaboration GitLab is a central place to create code, edit, and track needed code changes (issues) with your lab and collaborators. Multiple people can use, modify, and merge changes in code while communicating with the broader team all along the way. Security Unlimited private repositories for internal code projects. Set behind UAB authentication.","title":"For Researchers"},{"location":"account_management/gitlab_account/#for-software-developers-and-researchers","text":"UAB GitLab is useful for software developers. It is a single application for the entire software development lifecycle. From project planning and source code management to continuous integration (CI) and continuous deployment (CD), monitoring, and security. Our GitLab instance may be found at https://gitlab.rc.uab.edu .","title":"For Software Developers (and Researchers!)"},{"location":"account_management/gitlab_account/#uab-gitlab-registration","text":"","title":"UAB GitLab Registration"},{"location":"account_management/gitlab_account/#uab-affiliated-researcher-registration","text":"If you are a UAB affiliated researcher and have a BlazerID, you may create an account by logging in at the site above using the ldap tab. Please use your single sign-on (SSO) credentials. Please use BlazerID and password instead of UABMC credentials Please use your BlazerID and BlazerID password for UAB GitLab. UABMC credentials are a different sign in system and will likely not work. Central IT groups like Research Computing do not have a way to access UABMC credentials.","title":"UAB-Affiliated Researcher Registration"},{"location":"account_management/gitlab_account/#uabmc-researcher-registration","text":"Please use your BlazerID and BlazerID credentials to sign in following the directions for UAB-Affiliated Researchers. UABMC credentials should not be used for UAB GitLab.","title":"UABMC Researcher Registration"},{"location":"account_management/gitlab_account/#xias-external-collaborator-registration","text":"If you are a collaborator with a XIAS account you'll need to follow a different procedure. Ensure that your sponsor has included https://gitlab.rc.uab.edu in the list of approved URIs on the XIAS configuration page. Email support@listserv.uab.edu providing your full name, XIAS account email address, and sponsor. UAB Research Computing will create the account. You will recieve an email from gitlab.rc.uab.edu with a link to create a password. Navigate to https://gitlab.rc.uab.edu . Click the Standard tab. In the Username or email field type the part of your XIAS email address before the @ symbol. Do not include the @ symbol or anything after it. Fill out the Password field with the GitLab password you created in Step #4. Click Sign in . Warning XIAS account researchers can only be granted access if their sponsor adds the GitLab URL to the list of approved URIs. Please see XIAS Sites for more information.","title":"XIAS External Collaborator Registration"},{"location":"account_management/xias_guest/","text":"Guest Instructions \u00b6 These instructions are for guests who have been registered by UAB faculty and staff to use internal UAB resources. Once a request for a XIAS account has been made by your UAB sponsor, you will need to follow these instructions to complete the XIAS registration and obtain access to UAB resources. All of the links used on this page are available at the UAB XIAS Guest Users page. Create Account \u00b6 The first email you receive should be a notification that a request has been made to add you as a XIAS user. This email will include the project(s)/site(s) you're being added to. The next email you receive should contain instructions on how to register your account. This email may take an hour or so to arrive after the first. It will contain an invite code that you must enter at the XIAS website, along with the email address used to register you. Navigate to the link in the email. Please practice good internet hygiene and copy the link text, instead of clicking the link! As of the time of writing the link will be to the UAB XIAS Guest Users page. Once at the main page, click the \"Enter Invite or Reset Code\" link. You will be taken to the \"Register XIAS Account\" page. Enter the email address used to register you for a XIAS account, and the code from the email you received with registration instructions. Then click proceed. Enter your first and last names, then click proceed. Enter a password that will be used with your XIAS account. This password can be changed later, and your account can be recovered if the password is lost. Click proceed. You will be taken to a confirmation page. If everything is acceptable, click proceed. Otherwise click edit next to the incorrect field. Your XIAS email cannot be changed. If the email is not correct you will need to communicate with your sponsor to start the entire process over from the beginning. You should be taken to a page indicating success. Please carefully read the page and follow any instructions. If you do not see a success page, please contact your sponsor about next steps. Following this step, your account registration is complete and you should be able to access the resources you have been granted permission to use. Most internal UAB systems use a Single Sign-On (SSO) to simplify and standardize logging in. For those sites that don't you will need to activate your account manually. To manually activate accounts for resources that do not use SSO click the \"Activate (Sync) Accounts\" link on the left hand navigation pane. Fill out the form using the email used to register the XIAS account and the current password. Change Password and Recover From Lost Password \u00b6 To change your password, or recover your account in case of a lost password, please click the \"Change XIAS Password\" link in the left hand panel of the main page. Once there, follow the instructions on the form. Resend Invite Code \u00b6 If your invite code has expired, you can have a new invite code sent to you by clicking the \"Resend Invite Code\" link in the left hand panel of the main page. Once there, follow the instructions on the form. Guest IT Info \u00b6 For more information on UAB IT policies and other useful and helpful information, please click the \"UABIT Guest User info\" link.","title":"Instructions for Guests"},{"location":"account_management/xias_guest/#guest-instructions","text":"These instructions are for guests who have been registered by UAB faculty and staff to use internal UAB resources. Once a request for a XIAS account has been made by your UAB sponsor, you will need to follow these instructions to complete the XIAS registration and obtain access to UAB resources. All of the links used on this page are available at the UAB XIAS Guest Users page.","title":"Guest Instructions"},{"location":"account_management/xias_guest/#create-account","text":"The first email you receive should be a notification that a request has been made to add you as a XIAS user. This email will include the project(s)/site(s) you're being added to. The next email you receive should contain instructions on how to register your account. This email may take an hour or so to arrive after the first. It will contain an invite code that you must enter at the XIAS website, along with the email address used to register you. Navigate to the link in the email. Please practice good internet hygiene and copy the link text, instead of clicking the link! As of the time of writing the link will be to the UAB XIAS Guest Users page. Once at the main page, click the \"Enter Invite or Reset Code\" link. You will be taken to the \"Register XIAS Account\" page. Enter the email address used to register you for a XIAS account, and the code from the email you received with registration instructions. Then click proceed. Enter your first and last names, then click proceed. Enter a password that will be used with your XIAS account. This password can be changed later, and your account can be recovered if the password is lost. Click proceed. You will be taken to a confirmation page. If everything is acceptable, click proceed. Otherwise click edit next to the incorrect field. Your XIAS email cannot be changed. If the email is not correct you will need to communicate with your sponsor to start the entire process over from the beginning. You should be taken to a page indicating success. Please carefully read the page and follow any instructions. If you do not see a success page, please contact your sponsor about next steps. Following this step, your account registration is complete and you should be able to access the resources you have been granted permission to use. Most internal UAB systems use a Single Sign-On (SSO) to simplify and standardize logging in. For those sites that don't you will need to activate your account manually. To manually activate accounts for resources that do not use SSO click the \"Activate (Sync) Accounts\" link on the left hand navigation pane. Fill out the form using the email used to register the XIAS account and the current password.","title":"Create Account"},{"location":"account_management/xias_guest/#change-password-and-recover-from-lost-password","text":"To change your password, or recover your account in case of a lost password, please click the \"Change XIAS Password\" link in the left hand panel of the main page. Once there, follow the instructions on the form.","title":"Change Password and Recover From Lost Password"},{"location":"account_management/xias_guest/#resend-invite-code","text":"If your invite code has expired, you can have a new invite code sent to you by clicking the \"Resend Invite Code\" link in the left hand panel of the main page. Once there, follow the instructions on the form.","title":"Resend Invite Code"},{"location":"account_management/xias_guest/#guest-it-info","text":"For more information on UAB IT policies and other useful and helpful information, please click the \"UABIT Guest User info\" link.","title":"Guest IT Info"},{"location":"account_management/xias_sites/","text":"Creating a UAB XIAS Project/Site \u00b6 XIAS Project/Sites, or simply sites, tie external users to specific resources at UAB. By connecting people to the resource they use, UAB can maintain security and accountability. Creating a site is the first step to giving access to external collaborators, and the process can be thought of as \"create once, use many times\". All sites must have an expiration date for security reasons. To create a site you'll need at least one Uniform Resource Identifier (URI) relating to resources used by the site. If you aren't sure what URI(s) to list for your site, please contact UserServices@uab.edu . To start go to the UAB XIAS Project/Site Management Webpage . Click \"New\" to open a form for creating a new Project/Site. Fill in the form. All fields are required. Short name for project/site - A memorable name for your project or site. Longer description - A complete yet concise description of the project or site and its resources. Start date - The start date, can be today. End date - An expiration date for the project or site. URIs - One or more uniform resource locators (URIs) associated with the site, to increase accountability. Cheaha URI: https://rc.uab.edu Cloud URI: https://cloud.rc.uab.edu GitLab URI: https://gitlab.rc.uab.edu Click \"Add\" to submit the form. You should be taken to a page summarizing the created Project/Site. When you visit the \"Manage Projects/Sites\" page in the future, you will see a table with the newly created Project/Site listed. Click \"View\" to return to the page seen in the previous step. Click \"Edit\" to return to the form from [link]. Click \"Users\" to manage users for this site.","title":"Managing Projects & Sites"},{"location":"account_management/xias_sites/#creating-a-uab-xias-projectsite","text":"XIAS Project/Sites, or simply sites, tie external users to specific resources at UAB. By connecting people to the resource they use, UAB can maintain security and accountability. Creating a site is the first step to giving access to external collaborators, and the process can be thought of as \"create once, use many times\". All sites must have an expiration date for security reasons. To create a site you'll need at least one Uniform Resource Identifier (URI) relating to resources used by the site. If you aren't sure what URI(s) to list for your site, please contact UserServices@uab.edu . To start go to the UAB XIAS Project/Site Management Webpage . Click \"New\" to open a form for creating a new Project/Site. Fill in the form. All fields are required. Short name for project/site - A memorable name for your project or site. Longer description - A complete yet concise description of the project or site and its resources. Start date - The start date, can be today. End date - An expiration date for the project or site. URIs - One or more uniform resource locators (URIs) associated with the site, to increase accountability. Cheaha URI: https://rc.uab.edu Cloud URI: https://cloud.rc.uab.edu GitLab URI: https://gitlab.rc.uab.edu Click \"Add\" to submit the form. You should be taken to a page summarizing the created Project/Site. When you visit the \"Manage Projects/Sites\" page in the future, you will see a table with the newly created Project/Site listed. Click \"View\" to return to the page seen in the previous step. Click \"Edit\" to return to the form from [link]. Click \"Users\" to manage users for this site.","title":"Creating a UAB XIAS Project/Site"},{"location":"account_management/xias_users/","text":"Managing UAB XIAS Users \u00b6 UAB XIAS User management allow UAB faculty and staff to grant external collaborators access to specific resources on the internal UAB Campus Network. All XIAS users must be connected with at least one site, so you'll need to create one at the UAB XIAS User Management Webpage . All XIAS Users must also have an expiration date. Adding Users \u00b6 Before adding users, have a list of user emails handy for the site you wish to add users to, as well as expiration dates for each user. To start go to the UAB XIAS User Management Webpage and click Manage Users in the left menu. Select the Project/Site you wish to add users to from the drop down box. Click \"Register\" to open a form for adding new users. Fill in the form. All fields are required. Checkbox list - Leave the site checked. End date - An expiration date for the users being added. Cannot be longer than the end date for the selected Project/Site. Text box - Enter a list of e-mail addresses for users to add. Click \"Submit\" to move to a confirmation page. Check the emails are correct and click \"Add\" to submit the information Emails will be sent to all email addresses for next steps. You will be redirected to the UAB XIAS User Management Webpage, which should now have the text \"Registration successful.\" near the top. To complete their registration, please direct your external collaborators to the UAB XIAS Guest Users page . When they have completed their registration, you should receive an email like the following. Discovering and Managing Users \u00b6 There are two ways to discover XIAS users you are currently sponsoring. The first is to search by email address. The second is to list all users associated with a site. Discovering Users \u00b6 To locate users by e-mail address: type their email into the \"Locate specific user(s) by e-mail address\" text field on the \"Manage Users\" page. To manage users by site: select the site from the drop-down box and click the \"List\" button. The page will reload with a table containing name, email, and start and end dates. The end date is when the XIAS user registration expires. To change the end date for user(s), click the \"Sel\" checkbox next to their names, enter a date in the \"Change end date for selected users to\" text field, and click \"Update\". Revoking User Privileges \u00b6 Warning THIS INFORMATION IS PENDING TESTING Users cannot have their XIAS account deleted. However, privileges may be revoked. To revoke user privileges, follow the instructions for managing users by site. Update the desired user(s)' end date to a date earlier than the current date. Important If you need to urgently revoke privileges, please also notify UAB IT by emailing AskIT@uab.edu as soon as possible. Please be clear about what is needed and when.","title":"Managing Guests"},{"location":"account_management/xias_users/#managing-uab-xias-users","text":"UAB XIAS User management allow UAB faculty and staff to grant external collaborators access to specific resources on the internal UAB Campus Network. All XIAS users must be connected with at least one site, so you'll need to create one at the UAB XIAS User Management Webpage . All XIAS Users must also have an expiration date.","title":"Managing UAB XIAS Users"},{"location":"account_management/xias_users/#adding-users","text":"Before adding users, have a list of user emails handy for the site you wish to add users to, as well as expiration dates for each user. To start go to the UAB XIAS User Management Webpage and click Manage Users in the left menu. Select the Project/Site you wish to add users to from the drop down box. Click \"Register\" to open a form for adding new users. Fill in the form. All fields are required. Checkbox list - Leave the site checked. End date - An expiration date for the users being added. Cannot be longer than the end date for the selected Project/Site. Text box - Enter a list of e-mail addresses for users to add. Click \"Submit\" to move to a confirmation page. Check the emails are correct and click \"Add\" to submit the information Emails will be sent to all email addresses for next steps. You will be redirected to the UAB XIAS User Management Webpage, which should now have the text \"Registration successful.\" near the top. To complete their registration, please direct your external collaborators to the UAB XIAS Guest Users page . When they have completed their registration, you should receive an email like the following.","title":"Adding Users"},{"location":"account_management/xias_users/#discovering-and-managing-users","text":"There are two ways to discover XIAS users you are currently sponsoring. The first is to search by email address. The second is to list all users associated with a site.","title":"Discovering and Managing Users"},{"location":"account_management/xias_users/#discovering-users","text":"To locate users by e-mail address: type their email into the \"Locate specific user(s) by e-mail address\" text field on the \"Manage Users\" page. To manage users by site: select the site from the drop-down box and click the \"List\" button. The page will reload with a table containing name, email, and start and end dates. The end date is when the XIAS user registration expires. To change the end date for user(s), click the \"Sel\" checkbox next to their names, enter a date in the \"Change end date for selected users to\" text field, and click \"Update\".","title":"Discovering Users"},{"location":"account_management/xias_users/#revoking-user-privileges","text":"Warning THIS INFORMATION IS PENDING TESTING Users cannot have their XIAS account deleted. However, privileges may be revoked. To revoke user privileges, follow the instructions for managing users by site. Update the desired user(s)' end date to a date earlier than the current date. Important If you need to urgently revoke privileges, please also notify UAB IT by emailing AskIT@uab.edu as soon as possible. Please be clear about what is needed and when.","title":"Revoking User Privileges"},{"location":"cheaha/conda/","text":"Anaconda on Cheaha \u00b6 This page details specific information for using Anaconda on Cheaha . For general information on usage please see Anaconda Environments . Loading Anaconda \u00b6 Anaconda is installed on Cheaha as a family of modules, and does not need to be installed by Researchers. Instead, the most recent version of Anaconda installed on Cheaha may be loaded using the command module load Anaconda3 . Other versions may be discovered using the command module avail Anaconda . We recommend always using the latest version. Using Anaconda \u00b6 Anaconda on Cheaha works like it does on any other system, once the module has been loaded, with a couple of important differences in the callouts below. Note The base environment is installed in a shared location and cannot be modified by researchers. Other environments are installed in your home directory by default. Warning The Cheaha operating system has a version of Python installed. This version is used by python calls when Anaconda has not been loaded. Danger Do not use conda init on Cheaha! Anaconda is managed as a module , including script setup. Using conda init can cause hard-to-diagnose issues with Open OnDemand Interactive Jobs . Please see our FAQ for how to undo what conda init does. For more information on usage with examples, see Anaconda Environments . Using Mamba \u00b6 Mamba is not installed in the base environment on Cheaha, and cannot be installed in the base environment by researchers. We are working on installing Mamba in the base environment as part of our module installation process change. In the meantime if you need the speed Mamba provides, you can follow these instructions to make it available for yourself. module load Anaconda3 Create a new environment with only Mamba using conda create --name mamba -c conda-forge mamba conda activate mamba Use Mamba to install environments as needed. Be aware you must use the flag --prefix=~/.conda/envs/ to put the environment in the correct location to be seen by Anaconda.","title":"Anaconda on Cheaha"},{"location":"cheaha/conda/#anaconda-on-cheaha","text":"This page details specific information for using Anaconda on Cheaha . For general information on usage please see Anaconda Environments .","title":"Anaconda on Cheaha"},{"location":"cheaha/conda/#loading-anaconda","text":"Anaconda is installed on Cheaha as a family of modules, and does not need to be installed by Researchers. Instead, the most recent version of Anaconda installed on Cheaha may be loaded using the command module load Anaconda3 . Other versions may be discovered using the command module avail Anaconda . We recommend always using the latest version.","title":"Loading Anaconda"},{"location":"cheaha/conda/#using-anaconda","text":"Anaconda on Cheaha works like it does on any other system, once the module has been loaded, with a couple of important differences in the callouts below. Note The base environment is installed in a shared location and cannot be modified by researchers. Other environments are installed in your home directory by default. Warning The Cheaha operating system has a version of Python installed. This version is used by python calls when Anaconda has not been loaded. Danger Do not use conda init on Cheaha! Anaconda is managed as a module , including script setup. Using conda init can cause hard-to-diagnose issues with Open OnDemand Interactive Jobs . Please see our FAQ for how to undo what conda init does. For more information on usage with examples, see Anaconda Environments .","title":"Using Anaconda"},{"location":"cheaha/conda/#using-mamba","text":"Mamba is not installed in the base environment on Cheaha, and cannot be installed in the base environment by researchers. We are working on installing Mamba in the base environment as part of our module installation process change. In the meantime if you need the speed Mamba provides, you can follow these instructions to make it available for yourself. module load Anaconda3 Create a new environment with only Mamba using conda create --name mamba -c conda-forge mamba conda activate mamba Use Mamba to install environments as needed. Be aware you must use the flag --prefix=~/.conda/envs/ to put the environment in the correct location to be seen by Anaconda.","title":"Using Mamba"},{"location":"cheaha/getting_started/","text":"Getting Started \u00b6 Cheaha is a High Performance Computing (HPC) resource intended primarily for batch processing. We offer a user-friendly portal website Open OnDemand with graphical interfaces to the most common features, all in one place. Getting Help \u00b6 Please Contact Us with requests for support. Tips on getting effective support are here , and our frequently asked questions are here . Account Creation \u00b6 UAB Users \u00b6 Please visit https://rc.uab.edu to create an account. More information can be found here . Outside Collaborator \u00b6 Collaborators outside UAB will need sponsorship from a current UAB researcher through XIAS. The UAB researcher will need to create a project to associate the outside collaborator with and then add that user to the project. The outside collaborator will then be able to sign up to use Cheaha. Use the following instructions to help set up a XIAS account: For the UAB sponsor: Create a XIAS Site Add the external collaborator For the external collaborator after the sponsor has completed the previous instructions: Create an account and access Cheaha Accessing Cheaha \u00b6 The primary method for accessing Cheaha is through our online portal website, Open OnDemand, available at https://rc.uab.edu . We have more detailed documentation on using Open OnDemand located further in . The portal features a file browser , job composer and various interactive applications including a remote desktop, Jupyter, RStudio and MATLAB, among others. There is also a terminal usable directly in the browser for very basic functions such as file management. Hardware \u00b6 A full list of the available hardware can be found furthur in . Storage \u00b6 User Storage \u00b6 Each user is allocated 5 TB of personal storage by default. This storage quota is shared between the USER_DATA ( /data/user/<blazerid> ) and the HOME ( /home/<blazerid> ) directories. More information on storage can be found here . Project Directories \u00b6 In addition to personal storage, Primary Investigators may request additional shared storage for their lab personnel. This space is given a default size of 25 TB. Each PI may have one project space. To request project storage space, the PI should email support at support@listserv.uab.edu with the name of the project as well as the Blazer IDs of the researchers to give access to. Any future requests for giving or removing access must come from the PI. Danger There are no automatic backups of any data on Cheaha . If the system undergoes a complete crash, data stored on Cheaha may be lost and will be unrecoverable. Make backups of raw data and analysis scripts such as Box and Github or Gitlab . Etiquette \u00b6 Quotas are in place to ensure any one user can't monopolize all resources. Running Tasks on Compute Nodes \u00b6 There two main node types on Cheaha for researchers, the login node and many compute nodes. All expensive compute tasks must be run on compute nodes. Tasks running on the login node slow down processes for everyone, and in extreme cases can cause service outages affecting your work and the work of many of your colleagues. We will contact you if we find processes on the login node to help move your tasks to compute nodes. You are on compute nodes if: using Open OnDemand Interactive Apps using Open OnDemand Job Composer terminal prompt looks like [<blazerid>@c0001 ~]$ You are on the login node if: terminal prompt looks like [<blazerid>@login001 ~]$ Important If you are doing more than minor file management, you will need to use a compute node. Please request an interactive session at https://rc.uab.edu or through a job submitted using Slurm . Slurm \u00b6 Slurm is our job queueing software used for submitting any number of job scripts to run on the cluster. We have documentation on how to set up job scripts and submit them further in . More complete documentation is available at https://slurm.schedmd.com/ . Software \u00b6 A large variety of software is available on Cheaha as modules. To view and use these modules see the following documentation . For new software installation, please try searching Anaconda for packages first. If you still need help, please send a support ticket Conda Packages \u00b6 A significant amount of open-source software is distributed as Anaconda or Python libraries. These libraries can be installed by the user without permission from Research Computing using Anaconda environments. To read more about using Anaconda virtual environments see our Anaconda page . If the software installation instructions tell you to use either conda install or pip install commands, the software and its dependencies can be installed using a virtual environment.","title":"Getting Started"},{"location":"cheaha/getting_started/#getting-started","text":"Cheaha is a High Performance Computing (HPC) resource intended primarily for batch processing. We offer a user-friendly portal website Open OnDemand with graphical interfaces to the most common features, all in one place.","title":"Getting Started"},{"location":"cheaha/getting_started/#getting-help","text":"Please Contact Us with requests for support. Tips on getting effective support are here , and our frequently asked questions are here .","title":"Getting Help"},{"location":"cheaha/getting_started/#account-creation","text":"","title":"Account Creation"},{"location":"cheaha/getting_started/#uab-users","text":"Please visit https://rc.uab.edu to create an account. More information can be found here .","title":"UAB Users"},{"location":"cheaha/getting_started/#outside-collaborator","text":"Collaborators outside UAB will need sponsorship from a current UAB researcher through XIAS. The UAB researcher will need to create a project to associate the outside collaborator with and then add that user to the project. The outside collaborator will then be able to sign up to use Cheaha. Use the following instructions to help set up a XIAS account: For the UAB sponsor: Create a XIAS Site Add the external collaborator For the external collaborator after the sponsor has completed the previous instructions: Create an account and access Cheaha","title":"Outside Collaborator"},{"location":"cheaha/getting_started/#accessing-cheaha","text":"The primary method for accessing Cheaha is through our online portal website, Open OnDemand, available at https://rc.uab.edu . We have more detailed documentation on using Open OnDemand located further in . The portal features a file browser , job composer and various interactive applications including a remote desktop, Jupyter, RStudio and MATLAB, among others. There is also a terminal usable directly in the browser for very basic functions such as file management.","title":"Accessing Cheaha"},{"location":"cheaha/getting_started/#hardware","text":"A full list of the available hardware can be found furthur in .","title":"Hardware"},{"location":"cheaha/getting_started/#storage","text":"","title":"Storage"},{"location":"cheaha/getting_started/#user-storage","text":"Each user is allocated 5 TB of personal storage by default. This storage quota is shared between the USER_DATA ( /data/user/<blazerid> ) and the HOME ( /home/<blazerid> ) directories. More information on storage can be found here .","title":"User Storage"},{"location":"cheaha/getting_started/#project-directories","text":"In addition to personal storage, Primary Investigators may request additional shared storage for their lab personnel. This space is given a default size of 25 TB. Each PI may have one project space. To request project storage space, the PI should email support at support@listserv.uab.edu with the name of the project as well as the Blazer IDs of the researchers to give access to. Any future requests for giving or removing access must come from the PI. Danger There are no automatic backups of any data on Cheaha . If the system undergoes a complete crash, data stored on Cheaha may be lost and will be unrecoverable. Make backups of raw data and analysis scripts such as Box and Github or Gitlab .","title":"Project Directories"},{"location":"cheaha/getting_started/#etiquette","text":"Quotas are in place to ensure any one user can't monopolize all resources.","title":"Etiquette"},{"location":"cheaha/getting_started/#running-tasks-on-compute-nodes","text":"There two main node types on Cheaha for researchers, the login node and many compute nodes. All expensive compute tasks must be run on compute nodes. Tasks running on the login node slow down processes for everyone, and in extreme cases can cause service outages affecting your work and the work of many of your colleagues. We will contact you if we find processes on the login node to help move your tasks to compute nodes. You are on compute nodes if: using Open OnDemand Interactive Apps using Open OnDemand Job Composer terminal prompt looks like [<blazerid>@c0001 ~]$ You are on the login node if: terminal prompt looks like [<blazerid>@login001 ~]$ Important If you are doing more than minor file management, you will need to use a compute node. Please request an interactive session at https://rc.uab.edu or through a job submitted using Slurm .","title":"Running Tasks on Compute Nodes"},{"location":"cheaha/getting_started/#slurm","text":"Slurm is our job queueing software used for submitting any number of job scripts to run on the cluster. We have documentation on how to set up job scripts and submit them further in . More complete documentation is available at https://slurm.schedmd.com/ .","title":"Slurm"},{"location":"cheaha/getting_started/#software","text":"A large variety of software is available on Cheaha as modules. To view and use these modules see the following documentation . For new software installation, please try searching Anaconda for packages first. If you still need help, please send a support ticket","title":"Software"},{"location":"cheaha/getting_started/#conda-packages","text":"A significant amount of open-source software is distributed as Anaconda or Python libraries. These libraries can be installed by the user without permission from Research Computing using Anaconda environments. To read more about using Anaconda virtual environments see our Anaconda page . If the software installation instructions tell you to use either conda install or pip install commands, the software and its dependencies can be installed using a virtual environment.","title":"Conda Packages"},{"location":"cheaha/hardware/","text":"Hardware Information \u00b6 The following hardware summaries may be useful for grant proposal writing. If any information is missing that would be helpful to you, please be sure to contact us or create an issue on our tracker . Tip The tables in this section are wide and can be scrolled horizontally to display more information. Cheaha HPC Cluster \u00b6 The HPC cluster is comprised of 8192 compute cores connected by low-latency Fourteen Data Rate (FDR) and Enhanced Data Rate (EDR) InfiniBand networks. In addition to the basic compute cores, there are also 72 NVIDIA Tesla P100 GPUs available. There is a total of just under 49 TB of memory across the cluster. A description of the available hardware generations are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info hpc 7 gpu pascalnodes 504 4608 72 28 256 18 Intel Xeon E5-2680 v4 2.40 GHz NVIDIA Tesla P100 16 GB hpc 8 cpu cpu 504 4032 24 192 21 Intel Xeon E5-2680 v4 2.50 GHz hpc 8 high memory largemem 240 7680 24 768 10 Intel Xeon E5-2680 v4 2.50 GHz hpc 8 high memory largemem 96 6144 24 1536 4 Intel Xeon E5-2680 v4 2.50 GHz hpc 9 cpu cpu 2496 30056 48 578 52 Intel Xeon Gold 6248R 3.00 GHz hpc 10 cpu cpu 4352 17408 128 512 34 AMD Epyc 7713 Milan 2.00 GHz 8192 69928 72 139 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 7 1.08 17.06 18.14 18 326.43 8 0.96 0.96 21 20.16 8 0.96 0.96 10 9.6 8 0.96 0.96 4 3.84 9 2.30 2.30 52 119.81 10 4.10 4.10 34 139.26 619.1 The full table can be downloaded here . For information on using Cheaha, see our dedicated section . Partitions \u00b6 Partition Nodes Nodes Per Researcher Time Limit Priority Tier interactive 52 1 2 hours 20 express 52 UNLIMITED 2 hours 20 short 52 44 12 hours 16 pascalnodes 18 UNLIMITED 12 hours 16 pascalnodes-medium 7 UNLIMITED 2 days, 0 hours 15 medium 52 44 2 days, 2 hours 12 long 52 5 6 days, 6 hours 8 intel-dcb 21 5 6 days, 6 hours 8 amd-hdr100 33 5 6 days, 6 hours 8 largemem 14 10 2 days, 2 hours 6 largemem-long 5 10 6 days, 6 hours 6 The full table can be downloaded here . Quality of Service (QoS) Limits \u00b6 Quality of Service (QoS) allows us to balance usage across the cluster, so that no single researcher can consume all of the resources. Each set of QoS limits is applied to one or more partitions according to the table below. Each limit is applied to every researcher on Cheaha. The partitions within a group all share the same limits, so that a researcher can use 1.5 TB on both express and short , but can't use 2 TB on both at the same time. Partition Core Count Quota Memory (GB) Quota GPU Count Quota express, short, medium, long, intel-dcb, amd-hdr100 264 3072 interactive 48 largemem, largemem-long 290 7168 pascalnodes, pascalnodes-medium 56 500 8 The full table can be downloaded here . Cloud Service at cloud.rc \u00b6 The Cloud service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info cloud 1 cpu 240 960 48 192 5 Intel Xeon Gold 6248R 3.00 GHz cloud 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB 752 5056 32 9 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 5 11.52 1 4.61 77.97 82.58 4 330.3 341.82 The full table can be downloaded here . For information on using our Cloud service at cloud.rc, see our dedicated section . Kubernetes Container Service \u00b6 Important The Kubernetes fabric is still in deployment and not ready for researcher use. We will be sure to inform you when the service is ready. The following information is planned hardware. The Kubernetes container service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info container 1 cpu 144 576 48 192 3 Intel Xeon Gold 6248R 3.00 GHz container 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB 656 4672 32 7 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 3 6.91 1 4.61 77.97 82.58 4 330.3 337.21 The full table can be downloaded here . Full Hardware Details \u00b6 Detailed hardware information including processor and GPU makes and models, core clock frequencies, and other information for current hardware are in the table below. Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Cores Per Die Dies Per Node Die Brand Die Name Die Frequency Ghz Memory Per Node Gb Gpu Per Node Gpu Brand Gpu Name Gpu Memory Gb Nodes 1 cpu 128 1024 2 1 2 AMD Opteron 242 1.6 16 64 2 cpu 192 1152 8 4 2 Intel Xeon E5450 3 48 24 3 cpu 384 1536 12 6 2 Intel Xeon X5650 2.66 48 32 3 cpu 192 1536 12 6 2 Intel Xeon X5650 2.66 96 16 4 cpu 48 1152 16 8 2 Intel Xeon X5650 2.7 384 3 5 cpu 192 1152 16 8 2 Intel Xeon E2650 2 96 12 6 cpu cpu 336 5376 24 12 2 Intel Xeon E5-2680 v3 2.5 384 14 6 cpu cpu 912 9728 24 12 2 Intel Xeon E5-2680 v3 2.5 256 38 6 cpu cpu 1056 5632 24 12 2 Intel Xeon E5-2680 v3 2.5 128 44 7 gpu pascalnodes 504 4608 72 28 14 2 Intel Xeon E5-2680 v4 2.4 256 4 NVIDIA Tesla P100 16 18 8 cpu cpu 504 4032 24 12 2 Intel Xeon E5-2680 v4 2.5 192 21 8 high memory largemem 240 7680 24 12 2 Intel Xeon E5-2680 v4 2.5 768 10 8 high memory largemem 96 6144 24 12 2 Intel Xeon E5-2680 v4 2.5 1536 4 9 cpu cpu 2496 30056 48 24 2 Intel Xeon Gold 6248R 3 578 52 10 cpu cpu 4352 17408 128 64 2 AMD Epyc 7713 Milan 2 512 34 1 cpu 240 960 48 12 4 Intel Xeon Gold 6248R 3 192 5 1 gpu 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 1 cpu 144 576 48 12 4 Intel Xeon Gold 6248R 3 192 3 1 gpu 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 The full table can be downloaded here .","title":"Hardware"},{"location":"cheaha/hardware/#hardware-information","text":"The following hardware summaries may be useful for grant proposal writing. If any information is missing that would be helpful to you, please be sure to contact us or create an issue on our tracker . Tip The tables in this section are wide and can be scrolled horizontally to display more information.","title":"Hardware Information"},{"location":"cheaha/hardware/#cheaha-hpc-cluster","text":"The HPC cluster is comprised of 8192 compute cores connected by low-latency Fourteen Data Rate (FDR) and Enhanced Data Rate (EDR) InfiniBand networks. In addition to the basic compute cores, there are also 72 NVIDIA Tesla P100 GPUs available. There is a total of just under 49 TB of memory across the cluster. A description of the available hardware generations are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info hpc 7 gpu pascalnodes 504 4608 72 28 256 18 Intel Xeon E5-2680 v4 2.40 GHz NVIDIA Tesla P100 16 GB hpc 8 cpu cpu 504 4032 24 192 21 Intel Xeon E5-2680 v4 2.50 GHz hpc 8 high memory largemem 240 7680 24 768 10 Intel Xeon E5-2680 v4 2.50 GHz hpc 8 high memory largemem 96 6144 24 1536 4 Intel Xeon E5-2680 v4 2.50 GHz hpc 9 cpu cpu 2496 30056 48 578 52 Intel Xeon Gold 6248R 3.00 GHz hpc 10 cpu cpu 4352 17408 128 512 34 AMD Epyc 7713 Milan 2.00 GHz 8192 69928 72 139 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 7 1.08 17.06 18.14 18 326.43 8 0.96 0.96 21 20.16 8 0.96 0.96 10 9.6 8 0.96 0.96 4 3.84 9 2.30 2.30 52 119.81 10 4.10 4.10 34 139.26 619.1 The full table can be downloaded here . For information on using Cheaha, see our dedicated section .","title":"Cheaha HPC Cluster"},{"location":"cheaha/hardware/#partitions","text":"Partition Nodes Nodes Per Researcher Time Limit Priority Tier interactive 52 1 2 hours 20 express 52 UNLIMITED 2 hours 20 short 52 44 12 hours 16 pascalnodes 18 UNLIMITED 12 hours 16 pascalnodes-medium 7 UNLIMITED 2 days, 0 hours 15 medium 52 44 2 days, 2 hours 12 long 52 5 6 days, 6 hours 8 intel-dcb 21 5 6 days, 6 hours 8 amd-hdr100 33 5 6 days, 6 hours 8 largemem 14 10 2 days, 2 hours 6 largemem-long 5 10 6 days, 6 hours 6 The full table can be downloaded here .","title":"Partitions"},{"location":"cheaha/hardware/#quality-of-service-qos-limits","text":"Quality of Service (QoS) allows us to balance usage across the cluster, so that no single researcher can consume all of the resources. Each set of QoS limits is applied to one or more partitions according to the table below. Each limit is applied to every researcher on Cheaha. The partitions within a group all share the same limits, so that a researcher can use 1.5 TB on both express and short , but can't use 2 TB on both at the same time. Partition Core Count Quota Memory (GB) Quota GPU Count Quota express, short, medium, long, intel-dcb, amd-hdr100 264 3072 interactive 48 largemem, largemem-long 290 7168 pascalnodes, pascalnodes-medium 56 500 8 The full table can be downloaded here .","title":"Quality of Service (QoS) Limits"},{"location":"cheaha/hardware/#cloud-service-at-cloudrc","text":"The Cloud service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info cloud 1 cpu 240 960 48 192 5 Intel Xeon Gold 6248R 3.00 GHz cloud 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB 752 5056 32 9 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 5 11.52 1 4.61 77.97 82.58 4 330.3 341.82 The full table can be downloaded here . For information on using our Cloud service at cloud.rc, see our dedicated section .","title":"Cloud Service at cloud.rc"},{"location":"cheaha/hardware/#kubernetes-container-service","text":"Important The Kubernetes fabric is still in deployment and not ready for researcher use. We will be sure to inform you when the service is ready. The following information is planned hardware. The Kubernetes container service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info container 1 cpu 144 576 48 192 3 Intel Xeon Gold 6248R 3.00 GHz container 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB 656 4672 32 7 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 3 6.91 1 4.61 77.97 82.58 4 330.3 337.21 The full table can be downloaded here .","title":"Kubernetes Container Service"},{"location":"cheaha/hardware/#full-hardware-details","text":"Detailed hardware information including processor and GPU makes and models, core clock frequencies, and other information for current hardware are in the table below. Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Cores Per Die Dies Per Node Die Brand Die Name Die Frequency Ghz Memory Per Node Gb Gpu Per Node Gpu Brand Gpu Name Gpu Memory Gb Nodes 1 cpu 128 1024 2 1 2 AMD Opteron 242 1.6 16 64 2 cpu 192 1152 8 4 2 Intel Xeon E5450 3 48 24 3 cpu 384 1536 12 6 2 Intel Xeon X5650 2.66 48 32 3 cpu 192 1536 12 6 2 Intel Xeon X5650 2.66 96 16 4 cpu 48 1152 16 8 2 Intel Xeon X5650 2.7 384 3 5 cpu 192 1152 16 8 2 Intel Xeon E2650 2 96 12 6 cpu cpu 336 5376 24 12 2 Intel Xeon E5-2680 v3 2.5 384 14 6 cpu cpu 912 9728 24 12 2 Intel Xeon E5-2680 v3 2.5 256 38 6 cpu cpu 1056 5632 24 12 2 Intel Xeon E5-2680 v3 2.5 128 44 7 gpu pascalnodes 504 4608 72 28 14 2 Intel Xeon E5-2680 v4 2.4 256 4 NVIDIA Tesla P100 16 18 8 cpu cpu 504 4032 24 12 2 Intel Xeon E5-2680 v4 2.5 192 21 8 high memory largemem 240 7680 24 12 2 Intel Xeon E5-2680 v4 2.5 768 10 8 high memory largemem 96 6144 24 12 2 Intel Xeon E5-2680 v4 2.5 1536 4 9 cpu cpu 2496 30056 48 24 2 Intel Xeon Gold 6248R 3 578 52 10 cpu cpu 4352 17408 128 64 2 AMD Epyc 7713 Milan 2 512 34 1 cpu 240 960 48 12 4 Intel Xeon Gold 6248R 3 192 5 1 gpu 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 1 cpu 144 576 48 12 4 Intel Xeon Gold 6248R 3 192 3 1 gpu 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 The full table can be downloaded here .","title":"Full Hardware Details"},{"location":"cheaha/job_efficiency/","text":"Job Efficiency \u00b6 Efficient jobs save you time. Many factors go into queue wait time, but you can control your job requests. Optimizing queue wait times relies on getting resource requests close to actual resource usage. For example, if your task runs as fast with 2 cores as with 4 cores, requesting 4 cores will increase your wait time for no benefit. Other researchers time will be wasted due to locked up, unused resources. So please read this page to learn how to increase your efficiency and save time. As with any new skill, developing an intuition for efficiency and resource estimation requires experimentation, practice, and feedback. Please DO: run subsets of your data with varying resource requests to develop intuition make use of seff to validate contact us for advice if you're lost Estimating Compute Resources \u00b6 Being able to estimate the resources a job will need is critical. Requesting substantially more resources than necessary bottlenecks the cluster by preventing jobs from using resources that are reserved, but going unused. Of course requesting too few resources may cause the tasks to perform unacceptably slowly, or to fail altogether. Questions to ask yourself before requesting resources: Can my scripts take advantage of multiple cores? If yes, then request more cores. If no, then request only one core. Example: RStudio generally runs on a single thread. Any cores beyond the first will go unused and unusable. How large is the data I'm working with? Start by requesting memory equal to double the size of one file, no less than 2 GB per core. If that isn't enough, increase the request by 50% until there are no more memory errors. Example: If your data file is 4 GB, try starting out by requesting 8 GB of memory, then 12 GB, 16 GB, etc. Do my pipelines keep large amounts of data in memory? If yes, you may need to request even more memory than above. Example: Without careful programming, MATLAB code will often make and retain copies of data until finished. How long should my job take? Example: If my laptop is able to run the code on one data file in 2 hours, it will take about that long on Cheaha. Example: Requesting 50 hours of time for a 15 hour process will lengthen the queue time. Don't request too little! Include a buffer to account for scheduler and network issues. How is the software I'm using programmed? Can it use a GPU? Request one. Can it use multiple cores? Request more than one core. Is it single-threaded? Request only one core. Does it use MPI? Request multiple nodes. Be sure to check all of the flags, configuration, and options for the software, or these changes may not work. Which partition is most appropriate? More than 40 GB memory and queue wait times are long? Try largemem* . Need a GPU? Use pascalnodes* . Software works with AMD? Try amd-hdr100 . Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. We are happy to help guide you to an efficient usage of the cluster. Use seff to verify that your code is as efficient as possible. Verifying Job Efficiency \u00b6 It's important to evaluate the efficiency of your job in terms of resource usage after it completes. Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. As well, because each researcher has a maximum amount of resources they can use at a given time, having inefficient jobs can increase analysis runtime across many jobs, and increase queue wait times. In order to look at job efficieny, use the seff command. seff <jobid> The output will look like: The job had poor CPU efficiency, requesting 2 CPUs which were only busy for 30% of runtime. Requesting only a single core may have made more sense here. The job also had poor memory efficiency, using less than 1 GB total memory of the requested 16 GB (5.73%). For subsequent jobs using a similar analysis and dataset size, decreasing the requested memory to about 1200 MB and a single CPU would be more efficient, and get the job queued faster. Tip Aim for between 75% and 90% memory efficiency. Lower than that is a waste of resources, but too close to 100% could result in job failure due to an unexpected out-of-memory issue.","title":"Job Efficiency"},{"location":"cheaha/job_efficiency/#job-efficiency","text":"Efficient jobs save you time. Many factors go into queue wait time, but you can control your job requests. Optimizing queue wait times relies on getting resource requests close to actual resource usage. For example, if your task runs as fast with 2 cores as with 4 cores, requesting 4 cores will increase your wait time for no benefit. Other researchers time will be wasted due to locked up, unused resources. So please read this page to learn how to increase your efficiency and save time. As with any new skill, developing an intuition for efficiency and resource estimation requires experimentation, practice, and feedback. Please DO: run subsets of your data with varying resource requests to develop intuition make use of seff to validate contact us for advice if you're lost","title":"Job Efficiency"},{"location":"cheaha/job_efficiency/#estimating-compute-resources","text":"Being able to estimate the resources a job will need is critical. Requesting substantially more resources than necessary bottlenecks the cluster by preventing jobs from using resources that are reserved, but going unused. Of course requesting too few resources may cause the tasks to perform unacceptably slowly, or to fail altogether. Questions to ask yourself before requesting resources: Can my scripts take advantage of multiple cores? If yes, then request more cores. If no, then request only one core. Example: RStudio generally runs on a single thread. Any cores beyond the first will go unused and unusable. How large is the data I'm working with? Start by requesting memory equal to double the size of one file, no less than 2 GB per core. If that isn't enough, increase the request by 50% until there are no more memory errors. Example: If your data file is 4 GB, try starting out by requesting 8 GB of memory, then 12 GB, 16 GB, etc. Do my pipelines keep large amounts of data in memory? If yes, you may need to request even more memory than above. Example: Without careful programming, MATLAB code will often make and retain copies of data until finished. How long should my job take? Example: If my laptop is able to run the code on one data file in 2 hours, it will take about that long on Cheaha. Example: Requesting 50 hours of time for a 15 hour process will lengthen the queue time. Don't request too little! Include a buffer to account for scheduler and network issues. How is the software I'm using programmed? Can it use a GPU? Request one. Can it use multiple cores? Request more than one core. Is it single-threaded? Request only one core. Does it use MPI? Request multiple nodes. Be sure to check all of the flags, configuration, and options for the software, or these changes may not work. Which partition is most appropriate? More than 40 GB memory and queue wait times are long? Try largemem* . Need a GPU? Use pascalnodes* . Software works with AMD? Try amd-hdr100 . Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. We are happy to help guide you to an efficient usage of the cluster. Use seff to verify that your code is as efficient as possible.","title":"Estimating Compute Resources"},{"location":"cheaha/job_efficiency/#verifying-job-efficiency","text":"It's important to evaluate the efficiency of your job in terms of resource usage after it completes. Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. As well, because each researcher has a maximum amount of resources they can use at a given time, having inefficient jobs can increase analysis runtime across many jobs, and increase queue wait times. In order to look at job efficieny, use the seff command. seff <jobid> The output will look like: The job had poor CPU efficiency, requesting 2 CPUs which were only busy for 30% of runtime. Requesting only a single core may have made more sense here. The job also had poor memory efficiency, using less than 1 GB total memory of the requested 16 GB (5.73%). For subsequent jobs using a similar analysis and dataset size, decreasing the requested memory to about 1200 MB and a single CPU would be more efficient, and get the job queued faster. Tip Aim for between 75% and 90% memory efficiency. Lower than that is a waste of resources, but too close to 100% could result in job failure due to an unexpected out-of-memory issue.","title":"Verifying Job Efficiency"},{"location":"cheaha/uabgrid_getting_started/","text":"Getting Started \u00b6 Cheaha is a cluster computing environment for UAB researchers. Information about the history and future plans for Cheaha is available on the Cheaha page. Access (Cluster Account Request) \u00b6 To get started using Cheaha , simply visit our Open OnDemand portal at https://rc.uab.edu . This is the primary entry point for Cheaha and provides access to all cluster services directly from your web browser, including graphical desktops, Jupyter Notebooks, and even the traditional command-line. If you don't already have an account, you will be prompted to create one the first time you log into the portal. If you are creating an account, please share some of your interests in using Cheaha as this help us understand the science interests of our users. Please note : Usage of Cheaha is governed by UAB's Acceptable Use Policy (AUP) for computer resources. External Collaborator \u00b6 To request an account for an external collaborator, please follow the steps here. Login \u00b6 Overview \u00b6 Once your account has been created, you'll receive an email containing your user ID, generally your Blazer ID. You can log into Cheaha via your web browser using the new web-based HPC experience. You can also log into Cheaha via a traditional SSH client. Most UAB Windows workstations already have an SSH client installed, possibly named SSH Secure Shell Client or PuTTY . Linux and Mac OS X systems should have an SSH client installed by default. Usage of Cheaha is governed by UAB's Acceptable Use Policy (AUP) for computer and network resources. Client Configuration \u00b6 This section will cover steps to configure Windows, Linux and Mac OS X clients to connect to Cheaha. The official DNS name of Cheaha's frontend machine is cheaha.rc.uab.edu . If you want to refer to the machine as cheaha , you'll have to either add the \"rc.uab.edu\" to you computer's DNS search path. On Unix-derived systems (Linux, Mac) you can edit your computers /etc/resolv.conf as follows (you'll need administrator access to edit this file) search rc.uab.edu Or you can customize your SSH configuration to use the short name \"cheaha\" as a connection name. On systems using OpenSSH you can add the following to your ~/.ssh/config file Host cheaha Hostname cheaha.rc.uab.edu Linux \u00b6 Linux systems, regardless of the flavor (RedHat, SuSE, Ubuntu, etc...), should already have an SSH client on the system as part of the default install. Start a terminal (on RedHat click Applications -> Accessories -> Terminal, on Ubuntu Ctrl+Alt+T) At the prompt, enter the following command to connect to Cheaha ( Replace blazerid with your Cheaha userid ) ssh blazerid@cheaha.rc.uab.edu Mac OS X \u00b6 Mac OS X is a Unix operating system (BSD) and has a built in ssh client. Start a terminal (click Finder, type Terminal and double click on Terminal under the Applications category) At the prompt, enter the following command to connect to Cheaha ( Replace blazerid with your Cheaha userid ) ssh blazerid@cheaha.rc.uab.edu Windows \u00b6 There are many SSH clients available for Windows, some commercial and some that are free (GPL). This section will cover two clients that are commonly found on UAB Windows systems. MobaXterm \u00b6 MobaXterm is a free (also available for a price in a Profession version) suite of SSH tools. Of the Windows clients we've used, MobaXterm is the easiest to use and feature complete. Features include (but not limited to): SSH client (in a handy web browser like tabbed interface) Embedded Cygwin (which allows Windows users to run many Linux commands like grep, rsync, sed) Remote file system browser (graphical SFTP) X11 forwarding for remotely displaying graphical content from Cheaha Installs without requiring Windows Administrator rights Start MobaXterm and click the Session toolbar button (top left). Click SSH for the session type, enter the following information and click OK. Once finished, double click cheaha.rc.uab.edu in the list of Saved sessions under PuTTY sessions: Field Cheaha Settings Remote host cheaha.rc.uab.edu Port 22 PuTTY \u00b6 PuTTY is a free suite of SSH and telnet tools written and maintained by Simon Tatham . PuTTY supports SSH, secure FTP (SFTP), and X forwarding (XTERM) among other tools. Start PuTTY (Click START -> All Programs -> PuTTY -> PuTTY). The 'PuTTY Configuration' window will open Use these settings for each of the clusters that you would like to configure Field Cheaha Settings Host Name (or IP address) cheaha.rc.uab.edu Port 22 Protocol SSH Saved Sessions cheaha.rc.uab.edu Click Save to save the configuration, repeat the previous steps for the other clusters The next time you start PuTTY, simply double click on the cluster name under the 'Saved Sessions' list SSH Secure Shell Client \u00b6 SSH Secure Shell is a commercial application that is installed on many Windows workstations on campus and can be configured as follows: Start the program (Click START -> All Programs -> SSH Secure Shell -> Secure Shell Client). The 'default - SSH Secure Shell' window will open Click File -> Profiles -> Add Profile to open the 'Add Profile' window Type in the name of the cluster (for example: cheaha) in the field and click 'Add to Profiles' Click File -> Profiles -> Edit Profiles to open the 'Profiles' window Single click on your new profile name Use these settings for the clusters Field Cheaha Settings Host name cheaha.rc.uab.edu User name blazerid (insert your blazerid here) Port 22 Protocol SSH Encryption algorithm MAC algorithm Compression Terminal answerback vt100 Leave 'Connect through firewall' and 'Request tunnels only' unchecked Click OK to save the configuration, repeat the previous steps for the other clusters The next time you start SSH Secure Shell, click 'Profiles' and click the cluster name Logging in to Cheaha \u00b6 No matter which client you use to connect to the Cheaha, the first time you connect, the SSH client should display a message asking if you would like to import the hosts public key. Answer Yes to this question. Connect to Cheaha using one of the methods listed above Answer Yes to import the cluster's public key Enter your BlazerID password After successfully logging in for the first time, You may see the following message just press ENTER for the next three prompts, don't type any passphrases! It doesn't appear that you have set up your ssh key. This process will make the files: /home/joeuser/.ssh/id_rsa.pub /home/joeuser/.ssh/id_rsa /home/joeuser/.ssh/authorized_keys Generating public/private rsa key pair. Enter file in which to save the key (/home/joeuser/.ssh/id_rsa): Enter file in which to save the key (/home/joeuser/.ssh/id_rsa): Press Enter Enter passphrase (empty for no passphrase): Press Enter Enter same passphrase again: Press Enter Your identification has been saved in /home/joeuser/.ssh/id_rsa. Your public key has been saved in /home/joeuser/.ssh/id_rsa.pub. The key fingerprint is: f6:xx:xx:xx:xx:dd:9a:79:7b:83:xx:f9:d7:a7:d6:27 joeuser@cheaha.rc.uab.edu Users without a blazerid (collaborators from other universities) \u00b6 If you were issued a temporary password, enter it (Passwords are CaSE SensitivE!!!) You should see a message similar to this You are required to change your password immediately ( password aged ) WARNING: Your password has expired. You must change your password now and login again! Changing password for user joeuser. Changing password for joeuser ( current ) UNIX password: (current) UNIX password: Enter your temporary password at this prompt and press enter New UNIX password: Enter your new strong password and press enter Retype new UNIX password: Enter your new strong password again and press enter After you enter your new password for the second time and press enter, the shell may exit automatically. If it doesn't, type exit and press enter Log in again, this time use your new password Congratulations, you should now have a command prompt and be ready to start submitting jobs !!! Hardware \u00b6 See Hardware for more information. Cluster Software \u00b6 BrightCM 7.2 CentOS 7.2 x86_64 Slurm 15.08 Queuing System \u00b6 All work on Cheaha must be submitted to our queuing system ( Slurm ) . A common mistake made by new users is to run 'jobs' on the login node. This section gives a basic overview of what a queuing system is and why we use it. What is a queuing system? \u00b6 Software that gives users fair allocation of the cluster's resources Schedules jobs based using resource requests (the following are commonly requested resources, there are many more that are available) Number of processors (often referred to as \"slots\") Maximum memory (RAM) required per slot Maximum run time Common queuing systems: Slurm Sun Grid Engine (Also know as SGE, OGE, GE) OpenPBS Torque LSF (load sharing facility) Slurm is a queue management system and stands for Simple Linux Utility for Resource Management. Slurm was developed at the Lawrence Livermore National Lab and currently runs some of the largest compute clusters in the world. Slurm is now the primary job manager on Cheaha, it replaces SUN Grid Engine ([ SGE ]) the job manager used earlier. Instructions of using SLURM and writing SLURM scripts for jobs submission on Cheaha can be found here . Typical Workflow \u00b6 Stage data to $USER_SCRATCH (your scratch directory) Research how to run your code in \"batch\" mode. Batch mode typically means the ability to run it from the command line without requiring any interaction from the user. Identify the appropriate resources needed to run the job. The following are mandatory resource requests for all jobs on Cheaha Maximum memory (RAM) required per slot Maximum runtime Write a job script specifying queuing system parameters, resource requests and commands to run program Submit script to queuing system (sbatch script.job) Monitor job (squeue) Review the results and resubmit as necessary Clean up the scratch directory by moving or deleting the data off of the cluster Resource Requests \u00b6 Accurate resource requests are extremely important to the health of the over all cluster. In order for Cheaha to operate properly, the queing system must know how much runtime and RAM each job will need. Mandatory Resource Requests \u00b6 -t, --time= Set a limit on the total run time of the job allocation. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). For Array jobs, this represents the maximum run time for each task For serial or parallel jobs, this represents the maximum run time for the entire job --mem-per-cpu= Mimimum memory required per allocated CPU in MegaBytes. Other Common Resource Requests \u00b6 -N, --nodes= Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. -n, --ntasks= sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This option advises the Slurm controller that job steps run within the allocation will launch a maximum of number tasks and to provide for sufficient resources. The default is one task per node --mem= Specify the real memory required per node in MegaBytes. -c, --cpus-per-task= Advise the Slurm controller that ensuing job steps will require ncpus number of processors per task. Without this option, the controller will just try to allocate one processor per task. -p, --partition= Request a specific partition for the resource allocation. Available partitions are: express(max 2 hrs), short(max 12 hrs), medium(max 50 hrs), long(max 150 hrs), sinteractive(0-2 hrs) Submitting Jobs \u00b6 Batch Jobs are submitted on Cheaha by using the \"sbatch\" command. The full manual for sbtach is available by running the following command man sbatch Job Script File Format \u00b6 To submit a job to the queuing systems, you will first define your job in a script (a text file) and then submit that script to the queuing system. The script file needs to be formatted as a UNIX file , not a Windows or Mac text file. In geek speak, this means that the end of line (EOL) character should be a line feed (LF) rather than a carriage return line feed (CRLF) for Windows or carriage return (CR) for Mac. If you submit a job script formatted as a Windows or Mac text file, your job will likely fail with misleading messages, for example that the path specified does not exist. Windows Notepad does not have the ability to save files using the UNIX file format. Do NOT use Notepad to create files intended for use on the clusters. Instead use one of the alternative text editors listed in the following section. Converting Files to UNIX Format \u00b6 Dos2Unix Method \u00b6 The lines below that begin with $ are commands, the $ represents the command prompt and should not be typed! The dos2unix program can be used to convert Windows text files to UNIX files with a simple command. After you have copied the file to your home directory on the cluster, you can identify that the file is a Windows file by executing the following (Windows uses CR LF as the line terminator, where UNIX uses only LF and Mac uses only CR): $ file testfile.txt testfile.txt: ASCII text, with CRLF line terminators Now, convert the file to UNIX $ dos2unix testfile.txt dos2unix: converting file testfile.txt to UNIX format ... Verify the conversion using the file command $ file testfile.txt testfile.txt: ASCII text Alternative Windows Text Editors \u00b6 There are many good text editors available for Windows that have the capability to save files using the UNIX file format. Here are a few: [ Geany ] is an excellent free text editor for Windows and Linux that supports Windows, UNIX and Mac file formats, syntax highlighting and many programming features. To convert from Windows to UNIX click Document click Set Line Endings and then Convert and Set to LF (Unix) [ Notepad++ ] is a great free Windows text editor that supports Windows, UNIX and Mac file formats, syntax highlighting and many programming features. To convert from Windows to UNIX click Format and then click Convert to UNIX Format [ TextPad ] is another excellent Windows text editor. TextPad is not free, however. Example Batch Job Script \u00b6 A shared cluster environment like Cheaha uses a job scheduler to run tasks on the cluster to provide optimal resource sharing among users. Cheaha uses a job scheduling system call Slurm to schedule and manage jobs. A user needs to tell Slurm about resource requirements (e.g. CPU, memory) so that it can schedule jobs effectively. These resource requirements along with actual application code can be specified in a single file commonly referred as 'Job Script/File'. Following is a simple job script that prints job number and hostname. Note: Jobs must request the appropriate partition (ex: --partition=short ) to satisfy the jobs resource request (maximum runtime, number of compute nodes, etc...) #!/bin/bash # #SBATCH --job-name=test #SBATCH --output=res.txt #SBATCH --ntasks=1 #SBATCH --partition=express #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS srun hostname srun sleep 60 Lines starting with '#SBATCH' have a special meaning in the Slurm world. Slurm specific configuration options are specified after the '#SBATCH' characters. Above configuration options are useful for most job scripts and for additional configuration options refer to Slurm commands manual. A job script is submitted to the cluster using Slurm specific commands. There are many commands available, but following three commands are the most common: sbatch - to submit job scancel - to delete job squeue - to view job status We can submit above job script using sbatch command: $ sbatch HelloCheaha.sh Submitted batch job 52707 When the job script is submitted, Slurm queues it up and assigns it a job number (e.g. 52707 in above example). The job number is available inside job script using environment variable $JOB_ID. This variable can be used inside job script to create job related directory structure or file names. Interactive Resources \u00b6 Login Node (the host that you connected to when you setup the SSH connection to Cheaha) is supposed to be used for submitting jobs and/or lighter prep work required for the job scripts. Do not run heavy computations on the login node . If you have a heavier workload to prepare for a batch job (eg. compiling code or other manipulations of data) or your compute application requires interactive control, you should request a dedicated interactive node for this work. Interactive resources are requested by submitting an \"interactive\" job to the scheduler. Interactive jobs will provide you a command line on a compute resource that you can use just like you would the command line on the login node. The difference is that the scheduler has dedicated the requested resources to your job and you can run your interactive commands without having to worry about impacting other users on the login node. Interactive jobs, that can be run on command line, are requested with the srun command. srun --ntasks = 1 --cpus-per-task = 4 --mem-per-cpu = 4096 --time = 08 :00:00 --partition = medium --job-name = JOB_NAME --pty /bin/bash This command requests for 4 cores (--cpus-per-task) for a single task (--ntasks) with each cpu requesting size 4GB of RAM (--mem-per-cpu) for 8 hrs (--time). More advanced interactive scenarios to support graphical applications are available using VNC or X11 tunneling X-Win32 2014 for Windows Interactive jobs that requires running a graphical application, are requested with the sinteractive command, via Terminal on your VNC window. sinteractive --ntasks = 1 --cpus-per-task = 4 --mem-per-cpu = 4096 --time = 08 :00:00 --partition = medium --job-name = JOB_NAME Please note, sinteractive starts your shell in a screen session. Screen is a terminal emulator that is designed to make it possible to detach and reattach a session. This feature can mostly be ignored. If you application uses ctrl-a as a special command sequence (e.g. Emacs), however, you may find the application doesn't receive this special character. When using screen, you need to type ctrl-a a (ctrl-a followed by a single \"a\" key press) to send a ctrl-a to your application. Screen uses ctrl-a as it's own command character, so this special sequence issues the command to screen to \"send ctrl-a to my app\". Learn more about screen from it's documentation . Storage \u00b6 Privacy \u00b6 Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s). File and Directory Permissions \u00b6 The default permissions for all user data storage locations described below are as follows. In these descriptions, the \"$USER\" variable should be replaced with the user's account name string: /home/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /data/user/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /scratch/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /data/projects/ - a PI can request project space for their lab or specific collaborations. The project directory is created with the PI/requestor as the user-owner and a dedicated collaboration group as the group-owner. The PI and all members of the dedicated collaboration group have can read, write/delete, and list files. No privileges are granted to other users of the system. Additional controls can be implemented via access control lists (ACLs). The PI/requestor can modify the ACLs to allow additional access to specific users. These permissions are the default configuration. While it is possible to modify these permissions or change the group owner of a file to any group to which a user belongs, users are encouraged to work within the default configuration and contact support@listserv.uab.edu if the default permissions are not adequate. Setting up a collaboration group and associated project directory can address most collaboration need while keep data access restricted to the minimum necessary users for the collaboration. Additional background on Linux file system permissions can be found here: https://its.unc.edu/research-computing/techdocs/how-to-use-unix-and-linux-file-permissions/ https://www.rc.fas.harvard.edu/resources/documentation/linux/unix-permissions/ https://hpc.nih.gov/storage/permissions.html No Automatic Backups \u00b6 There is no automatic back up of any user data on the cluster in home, data, or scratch. At this time, all user data back up processes are defined and managed by each user and/or lab. Given that data backup demands vary widely between different users, groups, and research domains, this approach enables those who are most familiar with the data to make appropriate decisions based on their specific needs. For example, if a group is working with a large shared data set that is a local copy of a data set maintained authoritatively at a national data bank, maintaining a local backup is unlikely to be a productive use of limited storage resources, since this data could potentially be restored from the authoritative source. If, however, you are maintaining a unique source of data of which yours is the only copy, then maintaining a backup is critical if you value that data set. It's worth noting that while this \"uniqueness\" criteria may not apply to the data you analyze, it may readily apply to the codes that define your analysis pipelines. An often recommended backup policy is the 3-2-1 rule: maintain three copies of data, on two different media, with one copy off-site. You can read more about the 3-2-1 rule here . In the case of your application codes, using revision control tools during development provides an easy way to maintain a second copy, makes for a good software development process, and can help achieve reproducible research goals. Please review the data storage options provided by UAB IT for maintaining copies of your data. In choosing among these options, you should also be aware of UAB's data classification rules and requirements for security requirements for sensitive and restricted data storage. Given the importance of backup, Research Computing continues to explore options to facilitate data backup workflows from the cluster. Please contact us if you have questions or would like to discuss specific data backup scenarios. A good guide for thinking about your backup strategy might be: \"If you aren't managing a data back up process, then you have no backup data.\" Home directories \u00b6 Your home directory on Cheaha is NFS-mounted to the compute nodes as /home/$USER or $HOME. It is acceptable to use your home directory as a location to store job scripts and custom code. You are responsible for keeping your home directory under 10GB in size! The home directory must not be used to store large amounts of data. Please use $USER_SCRATCH for actively used data sets and $USER_DATA for storage of non scratch data. Scratch \u00b6 Research Computing policy requires that all bulky input and output must be located on the scratch space. The home directory is intended to store your job scripts, log files, libraries and other supporting files. Important Information: Scratch space (network and local) is not backed up . Research Computing expects each user to keep their scratch areas clean. The cluster scratch area are not to be used for archiving data. Cheaha has two types of scratch space, network mounted and local. Network scratch ($USER_SCRATCH) is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 4.7PB of usable storage. This should be your jobs primary working directory, unless the job would benefit from local scratch (see below). Local scratch is physically located on each compute node and is not accessible to the other nodes (including the login node). This space is useful if the job performs a lot of file I/O. Most of the jobs that run on our clusters do not fall into this category. Because the local scratch is inaccessible outside the job, it is important to note that you must move any data between local scratch to your network accessible scratch within your job. For example, step 1 in the job could be to copy the input from $USER_SCRATCH to ${USER_SCRATCH}, step 2 code execution, step 3 move the data back to $USER_SCRATCH. Network Scratch \u00b6 Network scratch is available using the environment variable $USER_SCRATCH or directly by /data/scratch/$USER It is advisable to use the environment variable whenever possible rather than the hard coded path. Local Scratch \u00b6 Each compute node has a local scratch directory that is accessible via the variable $LOCAL_SCRATCH . If your job performs a lot of file I/O, the job should use $LOCAL_SCRATCH rather than $USER_SCRATCH to prevent bogging down the network scratch file system. The amount of scratch space available is approximately 800GB. The $LOCAL_SCRATCH is a special temporary directory and it's important to note that this directory is deleted when the job completes, so the job script has to move the results to $USER_SCRATCH or other location prior to the job exiting. Note that $LOCAL_SCRATCH is only useful for jobs in which all processes run on the same compute node, so MPI jobs are not candidates for this solution. The following is an array job example that uses $LOCAL_SCRATCH by transferring the inputs into $LOCAL_SCRATCH at the beginning of the script and the result out of $LOCAL_SCRATCH at the end of the script. #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes and the appropriate partition # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 echo \"TMPDIR: $LOCAL_SCRATCH \" cd $LOCAL_SCRATCH # Create a working directory under the special scheduler local scratch directory # using the array job's taskID mdkir $SLURM_ARRAY_TASK_ID cd $SLURM_ARRAY_TASK_ID # Next copy the input data to the local scratch echo \"Copying input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) # The input data in this case has a numerical file extension that # matches $SLURM_ARRAY_TASK_ID cp -a $USER_SCRATCH /GeneData/INP*. $SLURM_ARRAY_TASK_ID ./ echo \" copied input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) someapp -S 1 -D 10 -i INP*. $SLURM_ARRAY_TASK_ID -o geneapp.out. $SLURM_ARRAY_TASK_ID # Lastly copy the results back to network scratch echo \"Copying results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) cp -a geneapp.out. $SLURM_ARRAY_TASK_ID $USER_SCRATCH /GeneData/ echo \" Copied results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) Project Storage \u00b6 Cheaha has a location where shared data can be stored called $SHARE_PROJECT. As with user scratch, this area is not backed up ! This is helpful if a team of researchers must access the same data. Please open a help desk ticket to request a project directory under $SHARE_PROJECT. Uploading Data \u00b6 Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s). Data can be moved onto the cluster (pushed) from a remote client (ie. you desktop) via SCP or SFTP. Data can also be downloaded to the cluster (pulled) by issuing transfer commands once you are logged into the cluster. Common transfer methods are wget <URL> , FTP, or SCP, and depend on how the data is made available from the data provider. Large data sets should be staged directly to your $USER_SCRATCH directory so as not to fill up $HOME. If you are working on a data set shared with multiple users, it's preferable to request space in $SHARE_PROJECT rather than duplicating the data for each user. Environment Modules \u00b6 Environment Modules is installed on Cheaha and should be used when constructing your job scripts if an applicable module file exists. Using the module command you can easily configure your environment for specific software packages without having to know the specific environment variables and values to set. Modules allows you to dynamically configure your environment without having to logout / login for the changes to take affect. If you find that specific software does not have a module, please submit a helpdesk ticket to request the module. Cheaha supports bash completion for the module command. For example, type 'module' and press the TAB key twice to see a list of options: module TAB TAB add display initlist keyword refresh switch use apropos help initprepend list rm unload whatis avail initadd initrm load show unuse clear initclear initswitch purge swap update To see the list of available modulefiles on the cluster, run the module avail command (note the example list below may not be complete!) or module load followed by two tab key presses: module avail ----------------------------------------------------------------------------------------- /cm/shared/modulefiles ----------------------------------------------------------------------------------------- acml/gcc/64/5.3.1 acml/open64-int64/mp/fma4/5.3.1 fftw2/openmpi/gcc/64/float/2.1.5 intel-cluster-runtime/ia32/3.8 netcdf/gcc/64/4.3.3.1 acml/gcc/fma4/5.3.1 blacs/openmpi/gcc/64/1.1patch03 fftw2/openmpi/open64/64/double/2.1.5 intel-cluster-runtime/intel64/3.8 netcdf/open64/64/4.3.3.1 acml/gcc/mp/64/5.3.1 blacs/openmpi/open64/64/1.1patch03 fftw2/openmpi/open64/64/float/2.1.5 intel-cluster-runtime/mic/3.8 netperf/2.7.0 acml/gcc/mp/fma4/5.3.1 blas/gcc/64/3.6.0 fftw3/openmpi/gcc/64/3.3.4 intel-tbb-oss/ia32/44_20160526oss open64/4.5.2.1 acml/gcc-int64/64/5.3.1 blas/open64/64/3.6.0 fftw3/openmpi/open64/64/3.3.4 intel-tbb-oss/intel64/44_20160526oss openblas/dynamic/0.2.15 acml/gcc-int64/fma4/5.3.1 bonnie++/1.97.1 gdb/7.9 iozone/3_434 openmpi/gcc/64/1.10.1 acml/gcc-int64/mp/64/5.3.1 cmgui/7.2 globalarrays/openmpi/gcc/64/5.4 lapack/gcc/64/3.6.0 openmpi/open64/64/1.10.1 acml/gcc-int64/mp/fma4/5.3.1 cuda75/blas/7.5.18 globalarrays/openmpi/open64/64/5.4 lapack/open64/64/3.6.0 pbspro/13.0.2.153173 acml/open64/64/5.3.1 cuda75/fft/7.5.18 hdf5/1.6.10 mpich/ge/gcc/64/3.2 puppet/3.8.4 acml/open64/fma4/5.3.1 cuda75/gdk/352.79 hdf5_18/1.8.16 mpich/ge/open64/64/3.2 rc-base acml/open64/mp/64/5.3.1 cuda75/nsight/7.5.18 hpl/2.1 mpiexec/0.84_432 scalapack/mvapich2/gcc/64/2.0.2 acml/open64/mp/fma4/5.3.1 cuda75/profiler/7.5.18 hwloc/1.10.1 mvapich/gcc/64/1.2rc1 scalapack/openmpi/gcc/64/2.0.2 acml/open64-int64/64/5.3.1 cuda75/toolkit/7.5.18 intel/compiler/32/15.0/2015.5.223 mvapich/open64/64/1.2rc1 sge/2011.11p1 acml/open64-int64/fma4/5.3.1 default-environment intel/compiler/64/15.0/2015.5.223 mvapich2/gcc/64/2.2b slurm/15.08.6 acml/open64-int64/mp/64/5.3.1 fftw2/openmpi/gcc/64/double/2.1.5 intel-cluster-checker/2.2.2 mvapich2/open64/64/2.2b torque/6.0.0.1 ---------------------------------------------------------------------------------------- /share/apps/modulefiles ----------------------------------------------------------------------------------------- rc/BrainSuite/15b rc/freesurfer/freesurfer-5.3.0 rc/intel/compiler/64/ps_2016/2016.0.047 rc/matlab/R2015a rc/SAS/v9.4 rc/cmg/2012.116.G rc/gromacs-intel/5.1.1 rc/Mathematica/10.3 rc/matlab/R2015b rc/dsistudio/dsistudio-20151020 rc/gtool/0.7.5 rc/matlab/R2012a rc/MRIConvert/2.0.8 --------------------------------------------------------------------------------------- /share/apps/rc/modules/all --------------------------------------------------------------------------------------- AFNI/linux_openmp_64-goolf-1.7.20-20160616 gperf/3.0.4-intel-2016a MVAPICH2/2.2b-GCC-4.9.3-2.25 Amber/14-intel-2016a-AmberTools-15-patchlevel-13-13 grep/2.15-goolf-1.4.10 NASM/2.11.06-goolf-1.7.20 annovar/2016Feb01-foss-2015b-Perl-5.22.1 GROMACS/5.0.5-intel-2015b-hybrid NASM/2.11.08-foss-2015b ant/1.9.6-Java-1.7.0_80 GSL/1.16-goolf-1.7.20 NASM/2.11.08-intel-2016a APBS/1.4-linux-static-x86_64 GSL/1.16-intel-2015b NASM/2.12.02-foss-2016a ASHS/rev103_20140612 GSL/2.1-foss-2015b NASM/2.12.02-intel-2015b Aspera-Connect/3.6.1 gtool/0.7.5_linux_x86_64 NASM/2.12.02-intel-2016a ATLAS/3.10.1-gompi-1.5.12-LAPACK-3.4.2 guile/1.8.8-GNU-4.9.3-2.25 ncurses/5.9-foss-2015b Autoconf/2.69-foss-2016a HAPGEN2/2.2.0 ncurses/5.9-GCC-4.8.4 Autoconf/2.69-GCC-4.8.4 HarfBuzz/1.2.7-intel-2016a ncurses/5.9-GNU-4.9.3-2.25 Autoconf/2.69-GNU-4.9.3-2.25 HDF5/1.8.15-patch1-intel-2015b ncurses/5.9-goolf-1.4.10 . . . . Some software packages have multiple module files, for example: GCC/4.7.2 GCC/4.8.1 GCC/4.8.2 GCC/4.8.4 GCC/4.9.2 GCC/4.9.3 GCC/4.9.3-2.25 In this case, the GCC module will always load the latest version, so loading this module is equivalent to loading GCC/4.9.3-2.25. If you always want to use the latest version, use this approach. If you want use a specific version, use the module file containing the appropriate version number. Some modules, when loaded, will actually load other modules. For example, the GROMACS/5.0.5-intel-2015b-hybrid module will also load intel/2015b and other related tools. To load a module, ex: for a GROMACS job, use the following module load command in your job script: module load GROMACS/5.0.5-intel-2015b-hybrid To see a list of the modules that you currently have loaded use the module list command module list Currently Loaded Modulefiles: 1 ) slurm/15.08.6 9 ) impi/5.0.3.048-iccifort-2015.3.187-GNU-4.9.3-2.25 17 ) Tcl/8.6.3-intel-2015b 2 ) rc-base 10 ) iimpi/7.3.5-GNU-4.9.3-2.25 18 ) SQLite/3.8.8.1-intel-2015b 3 ) GCC/4.9.3-binutils-2.25 11 ) imkl/11.2.3.187-iimpi-7.3.5-GNU-4.9.3-2.25 19 ) Tk/8.6.3-intel-2015b-no-X11 4 ) binutils/2.25-GCC-4.9.3-binutils-2.25 12 ) intel/2015b 20 ) Python/2.7.9-intel-2015b 5 ) GNU/4.9.3-2.25 13 ) bzip2/1.0.6-intel-2015b 21 ) Boost/1.58.0-intel-2015b-Python-2.7.9 6 ) icc/2015.3.187-GNU-4.9.3-2.25 14 ) zlib/1.2.8-intel-2015b 22 ) GROMACS/5.0.5-intel-2015b-hybrid 7 ) ifort/2015.3.187-GNU-4.9.3-2.25 15 ) ncurses/5.9-intel-2015b 8 ) iccifort/2015.3.187-GNU-4.9.3-2.25 16 ) libreadline/6.3-intel-2015b A module can be removed from your environment by using the module unload command: module unload GROMACS/5.0.5-intel-2015b-hybrid The definition of a module can also be viewed using the module show command, revealing what a specific module will do to your environment: module show GROMACS/5.0.5-intel-2015b-hybrid ------------------------------------------------------------------- /share/apps/rc/modules/all/GROMACS/5.0.5-intel-2015b-hybrid: module-whatis GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. - Homepage: http://www.gromacs.org conflict GROMACS prepend-path CPATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/include prepend-path LD_LIBRARY_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64 prepend-path LIBRARY_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64 prepend-path MANPATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/share/man prepend-path PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/bin prepend-path PKG_CONFIG_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64/pkgconfig setenv EBROOTGROMACS /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid setenv EBVERSIONGROMACS 5 .0.5 setenv EBDEVELGROMACS /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/easybuild/GROMACS-5.0.5-intel-2015b-hybrid-easybuild-devel ------------------------------------------------------------------- Error Using Modules from a Job Script \u00b6 If you are using modules and the command your job executes runs fine from the command line but fails when you run it from the job, you may be having an issue with the script initialization. If you see this error in your job error output file -bash: module: line 1 : syntax error: unexpected end of file -bash: error importing function definition for ` BASH_FUNC_module ' Add the command unset module before calling your module files. The -V job argument will cause a conflict with the module function used in your script. Sample Job Scripts \u00b6 The following are sample job scripts, please be careful to edit these for your environment (i.e. replace YOUR_EMAIL_ADDRESS with your real email address), set the h_rt to an appropriate runtime limit and modify the job name and any other parameters. Hello World is the classic example used throughout programming. We don't want to buck the system, so we'll use it as well to demonstrate jobs submission with one minor variation: our hello world will send us a greeting using the name of whatever machine it runs on. For example, when run on the Cheaha login node, it would print \"Hello from login001\". Hello World (serial) \u00b6 A serial job is one that can run independently of other commands, ie. it doesn't depend on the data from other jobs running simultaneously. You can run many serial jobs in any order. This is a common solution to processing lots of data when each command works on a single piece of data. For example, running the same conversion on 100s of images. Here we show how to create job script for one simple command. Running more than one command just requires submitting more jobs. Create your hello world application. Run this command to create a script, turn it into to a command, and run the command (just copy and past the following on to the command line). Create the file: vim helloworld.sh Write into \"helloworld.sh\" file (To write in vim editor: press shift + I ) #!/bin/bash echo Hello from ` hostname ` Save the file by pressing the esc key, type the following :wq Need to give permission the \"helloworld.sh\" file chmod +x helloworld.sh Create the Slurm job script that will request 256 MB RAM and a maximum runtime of 10 minutes. Create the JOB file: vim helloworld.job Write into \"helloworld.job\" file (To write in vim editor: press shift + I ) #!/bin/bash #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=helloworld # # Set your error and output files # #SBATCH --error=helloworld.err #SBATCH --output=helloworld.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu ./helloworld.sh Save the file by pressing the esc key, type the following :wq Submit the job to Slurm scheduler and check the status using squeue $ sbatch helloworld.job Submitted batch job 52888 When the job completes, you should have output files named helloworld.out and helloworld.err $ cat helloworld.out Hello from c0003 Hello World (parallel with MPI) \u00b6 MPI is used to coordinate the activity of many computations occurring in parallel. It is commonly used in simulation software for molecular dynamics, fluid dynamics, and similar domains where there is significant communication (data) exchanged between cooperating process. Here is a simple parallel Slurm job script for running commands the rely on MPI. This example also includes the example of compiling the code and submitting the job script to the Slurm scheduler. First, create a directory for the Hello World jobs mkdir -p ~/jobs/helloworld cd ~/jobs/helloworld Create the Hello World code written in C (this example of MPI enabled Hello World includes a 3 minute sleep to ensure the job runs for several minutes, a normal hello world example would run in a matter of seconds). $ vi helloworld-mpi.c #include <stdio.h> #include <mpi.h> main ( int argc, char **argv ) { int rank, size ; int i, j ; float f ; MPI_Init ( & argc, & argv ) ; MPI_Comm_rank ( MPI_COMM_WORLD, & rank ) ; MPI_Comm_size ( MPI_COMM_WORLD, & size ) ; printf ( \"Hello World from process %d of %d.\\n\" , rank, size ) ; sleep ( 180 ) ; for ( j = 0 ; j< = 100000 ; j++ ) for ( i = 0 ; i< = 100000 ; i++ ) f = i*2.718281828*i+i+i*3.141592654 ; MPI_Finalize () ; } Compile the code, first purging any modules you may have loaded followed by loading the module for OpenMPI GNU. The mpicc command will compile the code and produce a binary named helloworld_gnu_openmpi module purge module load DefaultModules module load OpenMPI/4.0.1-GCC-8.3.0-2.32 mpicc helloworld-mpi.c -o helloworld_gnu_openmpi Create the Slurm job script that will request 8 cpu slots and a maximum runtime of 10 minutes $ vi helloworld.job #!/bin/bash #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=helloworld_mpi # # Set your error and output files # #SBATCH --error=helloworld_mpi.err #SBATCH --output=helloworld_mpi.out #SBATCH --ntasks=8 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load OpenMPI/1.8.8-GNU-4.9.3-2.25 mpirun -np $SLURM_NTASKS helloworld_gnu_openmpi Submit the job to Slurm scheduler and check the status using squeue -u $USER $ sbatch helloworld.job Submitted batch job 52893 $ squeue -u BLAZERID JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 52893 express hellowor BLAZERID R 2 :07 2 c [ 0005 -0006 ] When the job completes, you should have output files named helloworld_mpi.out and helloworld_mpi.err $ cat helloworld_mpi.out Hello World from process 1 of 8 . Hello World from process 3 of 8 . Hello World from process 4 of 8 . Hello World from process 7 of 8 . Hello World from process 5 of 8 . Hello World from process 6 of 8 . Hello World from process 0 of 8 . Hello World from process 2 of 8 . Hello World (serial) -- revisited \u00b6 The job submit scripts (sbatch scripts) are actually bash shell scripts in their own right. The reason for using the funky #SBATCH prefix in the scripts is so that bash interprets any such line as a comment and won't execute it. Because the # character starts a comment in bash, we can weave the Slurm scheduler directives (the #SBATCH lines) into standard bash scripts. This lets us build scripts that we can execute locally and then easily run the same script to on a cluster node by calling it with sbatch. This can be used to our advantage to create a more fluid experience in moving between development and production job runs. The following example is a simple variation on the serial job above. All we will do is convert our Slurm job script into a command called helloworld that calls the helloworld.sh command. If the first line of a file is #!/bin/bash and that file is executable, the shell will automatically run the command as if were any other system command, eg. ls. That is, the \".sh\" extension on our HelloWorld.sh script is completely optional and is only meaningful to the user. Copy the serial helloworld.job script to a new file, add a the special #!/bin/bash as the first line, and make it executable with the following command (note: those are single quotes in the echo command): echo '#!/bin/bash' | cat helloworld.job > helloworld ; chmod +x helloworld Our sbatch script has now become a regular command. We can now execute the command with the simple prefix \"./helloworld\", which means \"execute this file in the current directory\": ./helloworld Hello from login001 Or if we want to run the command on a compute node, replace the \"./\" prefix with \"sbatch \": $ sbatch helloworld Submitted batch job 53001 And when the cluster run is complete you can look at the content of the output: $ $ cat helloworld.out Hello from c0003 You can use this approach of treating you sbatch files as command wrappers to build a collection of commands that can be executed locally or via sbatch. The other examples can be restructured similarly. To avoid having to use the \"./\" prefix, just add the current directory to your PATH. Also, if you plan to do heavy development using this feature on the cluster, please be sure to run sinteractive first so you don't load the login node with your development work. Gromacs \u00b6 #!/bin/bash #SBATCH --partition=short # # Name your job to make it easier for you to track # #SBATCH --job-name=test_gromacs # # Set your error and output files # #SBATCH --error=test_gromacs.err #SBATCH --output=test_gromacs.out #SBATCH --ntasks=8 # # Tell the scheduler only need 10 minutes # #SBATCH --time=10:00:00 #SBATCH --mem-per-cpu=2048 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load OpenMPI/1.8.8-GNU-4.9.3-2.25 module load GROMACS/5.0.5-intel-2015b-hybrid # Change directory to the job working directory if not already there cd ${ USER_SCRATCH } /jobs/gromacs # Single precision MDRUN = mdrun_mpi # Enter your tpr file over here export MYFILE = example.tpr mpirun -np SLURM_NTASKS $MDRUN -v -s $MYFILE -o $MYFILE -c $MYFILE -x $MYFILE -e $MYFILE -g ${ MYFILE } .log R (array job) \u00b6 The following is an example job script that will use an array of 10 tasks (--array=1-10), each task has a max runtime of 2 hours and will use no more than 256 MB of RAM per task. Array's of tasks are useful when you have lots of simple jobs that work on their own separate files or a sub-set of the problem that can be selected by the array task index. For a more comprehensive introduction please see this tutorial . Create a working directory and the job submission script $ mkdir -p ~/jobs/ArrayExample $ cd ~/jobs/ArrayExample $ vi R-example-array.job #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 cd ~/jobs/ArrayExample/rep $SLURM_ARRAY_TASK_ID srun R CMD BATCH rscript.R Submit the job to the Slurm scheduler and check the status of the job using the squeue command sbatch R-example-array.job squeue -u $USER Array Job Parameterization \u00b6 Suppose you need to submit thousands of jobs. While you could do this in a for loop, the global limit on jobs in the SLURM queue is 10,000. The limit is in place for performance reasons and the jobs may be rejected with the following error message and an incomplete set of tasks. sbatch: error: Slurm temporarily unable to accept job, sleeping and retrying The preferred way to handle this scenario is to allow SLURM to schedule the jobs for you using the array flag in an sbatch script. This allows many jobs to be submitted as a single entry in the queue, letting SLURM handle the for loop and queueing. It is possible to reference the current loop index, or task id, as $SLURM_ARRAY_TASK_ID. An example using $SLURM_ARRAY_TASK_ID to load input files and create output files is shown below. Suppose you have a short script called my_processing_script that needs to be run on 20,000 separate files. Suppose each instance only needs 1 cpu and 2 GB of RAM and finishes in 5 minutes. Submitting these files all at once won't work and at least half of them will be rejected by SLURM. Instead we can use the sbatch array flag. Note that some other useful flags have been omitted for brevity. #! /bin/bash #SBATCH --partition=express #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --array=1-20000%100 # This will run tasks 1 through 20000, with up to 100 at a time. # It is possible to provide any comma-separated list of intervals. # An example of a valid subset is --array=1,2,5-1000,3777,4995-5000%100 INPUT_FILE = $USER_DATA /input/file_ $SLURM_ARRAY_TASK_ID .txt OUTPUT_FILE = $USER_DATA /output/file_ $SLURM_ARRAY_TASK_ID .txt my_processing_script --input = \" $INPUT_FILE \" --output = \" $OUTPUT_FILE \" GPU JOB \u00b6 A Graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Create a math.sh file as: $vim math.sh #!/bin/bash ( e = 5 ) echo $e (( e = e + 3 )) echo $e (( e = e+4 )) # -- spaces or no spaces, it doesn't matter echo $e Give File permissions for script as follows: $chmod +x math.sh Create Job submission script file: $vi math.job #!/bin/bash #SBATCH --share #SBATCH --partition=pascalnodes #SBATCH --gres=gpu:1 # Name your job to make it easier for you to track # #SBATCH --job-name=math # # Set your error and output files # #SBATCH --error=math.err #SBATCH --output=math.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu ./math.sh Submitting batch script to Slurm scheduler $sbatch math.job We can also request GPU's on cluster as: $sinteractive --ntasks = 1 --time = 00 :10:00 --exclusive --partition = pascalnodes -N2 --gres = gpu:2 GPU Job (with MPI) \u00b6 As mentioned above, MPI is used to coordinate the activity of many computations occurring in parallel. It is commonly used in simulation software for molecular dynamics, fluid dynamics, and similar domains where there is significant communication (data) exchanged between cooperating process. An example of an GPU job with MPI can be found by visiting this link . Be sure to request the appropiate amount of gpu resources for your job: sinteractive --ntasks = 8 --time = 08 :00:00 --exclusive --partition = pascalnodes -N2 --gres = gpu:4 Singularity Container \u00b6 Singularity is designed so that you can use it within SLURM jobs and it does not violate security constraints on the cluster. Singularity was built keeping HPC in mind, i.e a shared environment. Using Singularity container with SLURM job script is very easy, as the containers run as a process on the host machine, just like any other command in a batch script. You just need to load Singularity in your job script and run the command via a singularity process. Here's an example job script below: #!/bin/bash # #SBATCH --job-name=test-singularity #SBATCH --output=res.out #SBATCH --error=res.err # # Number of tasks needed for this job. Generally, used with MPI jobs #SBATCH --ntasks=1 #SBATCH --partition=express # # Time format = HH:MM:SS, DD-HH:MM:SS #SBATCH --time=10:00 # # Number of CPUs allocated to each task. #SBATCH --cpus-per-task=1 # # Mimimum memory required per allocated CPU in MegaBytes. #SBATCH --mem-per-cpu=100 # # Send mail to the email address when the job fails #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu #Set your environment here module load Singularity/2.5.2-GCC-5.4.0-2.26 #Run your singularity or any other commands here singularity exec -B /data/user/ $USER /data/user/ $USER /rc-training-sessions/neurodebian-neurodebian-master-latest.simg dcm2nii PATH_TO_YOUR_DICOM_FILES For a more comprehensive introduction please see this tutorial . Installed Software \u00b6 A partial list of installed software with additional instructions for their use is available on the Cheaha Software page.","title":"Getting Started"},{"location":"cheaha/uabgrid_getting_started/#getting-started","text":"Cheaha is a cluster computing environment for UAB researchers. Information about the history and future plans for Cheaha is available on the Cheaha page.","title":"Getting Started"},{"location":"cheaha/uabgrid_getting_started/#access-cluster-account-request","text":"To get started using Cheaha , simply visit our Open OnDemand portal at https://rc.uab.edu . This is the primary entry point for Cheaha and provides access to all cluster services directly from your web browser, including graphical desktops, Jupyter Notebooks, and even the traditional command-line. If you don't already have an account, you will be prompted to create one the first time you log into the portal. If you are creating an account, please share some of your interests in using Cheaha as this help us understand the science interests of our users. Please note : Usage of Cheaha is governed by UAB's Acceptable Use Policy (AUP) for computer resources.","title":"Access (Cluster Account Request)"},{"location":"cheaha/uabgrid_getting_started/#external-collaborator","text":"To request an account for an external collaborator, please follow the steps here.","title":"External Collaborator"},{"location":"cheaha/uabgrid_getting_started/#login","text":"","title":"Login"},{"location":"cheaha/uabgrid_getting_started/#overview","text":"Once your account has been created, you'll receive an email containing your user ID, generally your Blazer ID. You can log into Cheaha via your web browser using the new web-based HPC experience. You can also log into Cheaha via a traditional SSH client. Most UAB Windows workstations already have an SSH client installed, possibly named SSH Secure Shell Client or PuTTY . Linux and Mac OS X systems should have an SSH client installed by default. Usage of Cheaha is governed by UAB's Acceptable Use Policy (AUP) for computer and network resources.","title":"Overview"},{"location":"cheaha/uabgrid_getting_started/#client-configuration","text":"This section will cover steps to configure Windows, Linux and Mac OS X clients to connect to Cheaha. The official DNS name of Cheaha's frontend machine is cheaha.rc.uab.edu . If you want to refer to the machine as cheaha , you'll have to either add the \"rc.uab.edu\" to you computer's DNS search path. On Unix-derived systems (Linux, Mac) you can edit your computers /etc/resolv.conf as follows (you'll need administrator access to edit this file) search rc.uab.edu Or you can customize your SSH configuration to use the short name \"cheaha\" as a connection name. On systems using OpenSSH you can add the following to your ~/.ssh/config file Host cheaha Hostname cheaha.rc.uab.edu","title":"Client Configuration"},{"location":"cheaha/uabgrid_getting_started/#linux","text":"Linux systems, regardless of the flavor (RedHat, SuSE, Ubuntu, etc...), should already have an SSH client on the system as part of the default install. Start a terminal (on RedHat click Applications -> Accessories -> Terminal, on Ubuntu Ctrl+Alt+T) At the prompt, enter the following command to connect to Cheaha ( Replace blazerid with your Cheaha userid ) ssh blazerid@cheaha.rc.uab.edu","title":"Linux"},{"location":"cheaha/uabgrid_getting_started/#mac-os-x","text":"Mac OS X is a Unix operating system (BSD) and has a built in ssh client. Start a terminal (click Finder, type Terminal and double click on Terminal under the Applications category) At the prompt, enter the following command to connect to Cheaha ( Replace blazerid with your Cheaha userid ) ssh blazerid@cheaha.rc.uab.edu","title":"Mac OS X"},{"location":"cheaha/uabgrid_getting_started/#windows","text":"There are many SSH clients available for Windows, some commercial and some that are free (GPL). This section will cover two clients that are commonly found on UAB Windows systems.","title":"Windows"},{"location":"cheaha/uabgrid_getting_started/#mobaxterm","text":"MobaXterm is a free (also available for a price in a Profession version) suite of SSH tools. Of the Windows clients we've used, MobaXterm is the easiest to use and feature complete. Features include (but not limited to): SSH client (in a handy web browser like tabbed interface) Embedded Cygwin (which allows Windows users to run many Linux commands like grep, rsync, sed) Remote file system browser (graphical SFTP) X11 forwarding for remotely displaying graphical content from Cheaha Installs without requiring Windows Administrator rights Start MobaXterm and click the Session toolbar button (top left). Click SSH for the session type, enter the following information and click OK. Once finished, double click cheaha.rc.uab.edu in the list of Saved sessions under PuTTY sessions: Field Cheaha Settings Remote host cheaha.rc.uab.edu Port 22","title":"MobaXterm"},{"location":"cheaha/uabgrid_getting_started/#putty","text":"PuTTY is a free suite of SSH and telnet tools written and maintained by Simon Tatham . PuTTY supports SSH, secure FTP (SFTP), and X forwarding (XTERM) among other tools. Start PuTTY (Click START -> All Programs -> PuTTY -> PuTTY). The 'PuTTY Configuration' window will open Use these settings for each of the clusters that you would like to configure Field Cheaha Settings Host Name (or IP address) cheaha.rc.uab.edu Port 22 Protocol SSH Saved Sessions cheaha.rc.uab.edu Click Save to save the configuration, repeat the previous steps for the other clusters The next time you start PuTTY, simply double click on the cluster name under the 'Saved Sessions' list","title":"PuTTY"},{"location":"cheaha/uabgrid_getting_started/#ssh-secure-shell-client","text":"SSH Secure Shell is a commercial application that is installed on many Windows workstations on campus and can be configured as follows: Start the program (Click START -> All Programs -> SSH Secure Shell -> Secure Shell Client). The 'default - SSH Secure Shell' window will open Click File -> Profiles -> Add Profile to open the 'Add Profile' window Type in the name of the cluster (for example: cheaha) in the field and click 'Add to Profiles' Click File -> Profiles -> Edit Profiles to open the 'Profiles' window Single click on your new profile name Use these settings for the clusters Field Cheaha Settings Host name cheaha.rc.uab.edu User name blazerid (insert your blazerid here) Port 22 Protocol SSH Encryption algorithm MAC algorithm Compression Terminal answerback vt100 Leave 'Connect through firewall' and 'Request tunnels only' unchecked Click OK to save the configuration, repeat the previous steps for the other clusters The next time you start SSH Secure Shell, click 'Profiles' and click the cluster name","title":"SSH Secure Shell Client"},{"location":"cheaha/uabgrid_getting_started/#logging-in-to-cheaha","text":"No matter which client you use to connect to the Cheaha, the first time you connect, the SSH client should display a message asking if you would like to import the hosts public key. Answer Yes to this question. Connect to Cheaha using one of the methods listed above Answer Yes to import the cluster's public key Enter your BlazerID password After successfully logging in for the first time, You may see the following message just press ENTER for the next three prompts, don't type any passphrases! It doesn't appear that you have set up your ssh key. This process will make the files: /home/joeuser/.ssh/id_rsa.pub /home/joeuser/.ssh/id_rsa /home/joeuser/.ssh/authorized_keys Generating public/private rsa key pair. Enter file in which to save the key (/home/joeuser/.ssh/id_rsa): Enter file in which to save the key (/home/joeuser/.ssh/id_rsa): Press Enter Enter passphrase (empty for no passphrase): Press Enter Enter same passphrase again: Press Enter Your identification has been saved in /home/joeuser/.ssh/id_rsa. Your public key has been saved in /home/joeuser/.ssh/id_rsa.pub. The key fingerprint is: f6:xx:xx:xx:xx:dd:9a:79:7b:83:xx:f9:d7:a7:d6:27 joeuser@cheaha.rc.uab.edu","title":"Logging in to Cheaha"},{"location":"cheaha/uabgrid_getting_started/#users-without-a-blazerid-collaborators-from-other-universities","text":"If you were issued a temporary password, enter it (Passwords are CaSE SensitivE!!!) You should see a message similar to this You are required to change your password immediately ( password aged ) WARNING: Your password has expired. You must change your password now and login again! Changing password for user joeuser. Changing password for joeuser ( current ) UNIX password: (current) UNIX password: Enter your temporary password at this prompt and press enter New UNIX password: Enter your new strong password and press enter Retype new UNIX password: Enter your new strong password again and press enter After you enter your new password for the second time and press enter, the shell may exit automatically. If it doesn't, type exit and press enter Log in again, this time use your new password Congratulations, you should now have a command prompt and be ready to start submitting jobs !!!","title":"Users without a blazerid (collaborators from other universities)"},{"location":"cheaha/uabgrid_getting_started/#hardware","text":"See Hardware for more information.","title":"Hardware"},{"location":"cheaha/uabgrid_getting_started/#cluster-software","text":"BrightCM 7.2 CentOS 7.2 x86_64 Slurm 15.08","title":"Cluster Software"},{"location":"cheaha/uabgrid_getting_started/#queuing-system","text":"All work on Cheaha must be submitted to our queuing system ( Slurm ) . A common mistake made by new users is to run 'jobs' on the login node. This section gives a basic overview of what a queuing system is and why we use it.","title":"Queuing System"},{"location":"cheaha/uabgrid_getting_started/#what-is-a-queuing-system","text":"Software that gives users fair allocation of the cluster's resources Schedules jobs based using resource requests (the following are commonly requested resources, there are many more that are available) Number of processors (often referred to as \"slots\") Maximum memory (RAM) required per slot Maximum run time Common queuing systems: Slurm Sun Grid Engine (Also know as SGE, OGE, GE) OpenPBS Torque LSF (load sharing facility) Slurm is a queue management system and stands for Simple Linux Utility for Resource Management. Slurm was developed at the Lawrence Livermore National Lab and currently runs some of the largest compute clusters in the world. Slurm is now the primary job manager on Cheaha, it replaces SUN Grid Engine ([ SGE ]) the job manager used earlier. Instructions of using SLURM and writing SLURM scripts for jobs submission on Cheaha can be found here .","title":"What is a queuing system?"},{"location":"cheaha/uabgrid_getting_started/#typical-workflow","text":"Stage data to $USER_SCRATCH (your scratch directory) Research how to run your code in \"batch\" mode. Batch mode typically means the ability to run it from the command line without requiring any interaction from the user. Identify the appropriate resources needed to run the job. The following are mandatory resource requests for all jobs on Cheaha Maximum memory (RAM) required per slot Maximum runtime Write a job script specifying queuing system parameters, resource requests and commands to run program Submit script to queuing system (sbatch script.job) Monitor job (squeue) Review the results and resubmit as necessary Clean up the scratch directory by moving or deleting the data off of the cluster","title":"Typical Workflow"},{"location":"cheaha/uabgrid_getting_started/#resource-requests","text":"Accurate resource requests are extremely important to the health of the over all cluster. In order for Cheaha to operate properly, the queing system must know how much runtime and RAM each job will need.","title":"Resource Requests"},{"location":"cheaha/uabgrid_getting_started/#mandatory-resource-requests","text":"-t, --time= Set a limit on the total run time of the job allocation. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). For Array jobs, this represents the maximum run time for each task For serial or parallel jobs, this represents the maximum run time for the entire job --mem-per-cpu= Mimimum memory required per allocated CPU in MegaBytes.","title":"Mandatory Resource Requests"},{"location":"cheaha/uabgrid_getting_started/#other-common-resource-requests","text":"-N, --nodes= Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. -n, --ntasks= sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This option advises the Slurm controller that job steps run within the allocation will launch a maximum of number tasks and to provide for sufficient resources. The default is one task per node --mem= Specify the real memory required per node in MegaBytes. -c, --cpus-per-task= Advise the Slurm controller that ensuing job steps will require ncpus number of processors per task. Without this option, the controller will just try to allocate one processor per task. -p, --partition= Request a specific partition for the resource allocation. Available partitions are: express(max 2 hrs), short(max 12 hrs), medium(max 50 hrs), long(max 150 hrs), sinteractive(0-2 hrs)","title":"Other Common Resource Requests"},{"location":"cheaha/uabgrid_getting_started/#submitting-jobs","text":"Batch Jobs are submitted on Cheaha by using the \"sbatch\" command. The full manual for sbtach is available by running the following command man sbatch","title":"Submitting Jobs"},{"location":"cheaha/uabgrid_getting_started/#job-script-file-format","text":"To submit a job to the queuing systems, you will first define your job in a script (a text file) and then submit that script to the queuing system. The script file needs to be formatted as a UNIX file , not a Windows or Mac text file. In geek speak, this means that the end of line (EOL) character should be a line feed (LF) rather than a carriage return line feed (CRLF) for Windows or carriage return (CR) for Mac. If you submit a job script formatted as a Windows or Mac text file, your job will likely fail with misleading messages, for example that the path specified does not exist. Windows Notepad does not have the ability to save files using the UNIX file format. Do NOT use Notepad to create files intended for use on the clusters. Instead use one of the alternative text editors listed in the following section.","title":"Job Script File Format"},{"location":"cheaha/uabgrid_getting_started/#converting-files-to-unix-format","text":"","title":"Converting Files to UNIX Format"},{"location":"cheaha/uabgrid_getting_started/#dos2unix-method","text":"The lines below that begin with $ are commands, the $ represents the command prompt and should not be typed! The dos2unix program can be used to convert Windows text files to UNIX files with a simple command. After you have copied the file to your home directory on the cluster, you can identify that the file is a Windows file by executing the following (Windows uses CR LF as the line terminator, where UNIX uses only LF and Mac uses only CR): $ file testfile.txt testfile.txt: ASCII text, with CRLF line terminators Now, convert the file to UNIX $ dos2unix testfile.txt dos2unix: converting file testfile.txt to UNIX format ... Verify the conversion using the file command $ file testfile.txt testfile.txt: ASCII text","title":"Dos2Unix Method"},{"location":"cheaha/uabgrid_getting_started/#alternative-windows-text-editors","text":"There are many good text editors available for Windows that have the capability to save files using the UNIX file format. Here are a few: [ Geany ] is an excellent free text editor for Windows and Linux that supports Windows, UNIX and Mac file formats, syntax highlighting and many programming features. To convert from Windows to UNIX click Document click Set Line Endings and then Convert and Set to LF (Unix) [ Notepad++ ] is a great free Windows text editor that supports Windows, UNIX and Mac file formats, syntax highlighting and many programming features. To convert from Windows to UNIX click Format and then click Convert to UNIX Format [ TextPad ] is another excellent Windows text editor. TextPad is not free, however.","title":"Alternative Windows Text Editors"},{"location":"cheaha/uabgrid_getting_started/#example-batch-job-script","text":"A shared cluster environment like Cheaha uses a job scheduler to run tasks on the cluster to provide optimal resource sharing among users. Cheaha uses a job scheduling system call Slurm to schedule and manage jobs. A user needs to tell Slurm about resource requirements (e.g. CPU, memory) so that it can schedule jobs effectively. These resource requirements along with actual application code can be specified in a single file commonly referred as 'Job Script/File'. Following is a simple job script that prints job number and hostname. Note: Jobs must request the appropriate partition (ex: --partition=short ) to satisfy the jobs resource request (maximum runtime, number of compute nodes, etc...) #!/bin/bash # #SBATCH --job-name=test #SBATCH --output=res.txt #SBATCH --ntasks=1 #SBATCH --partition=express #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS srun hostname srun sleep 60 Lines starting with '#SBATCH' have a special meaning in the Slurm world. Slurm specific configuration options are specified after the '#SBATCH' characters. Above configuration options are useful for most job scripts and for additional configuration options refer to Slurm commands manual. A job script is submitted to the cluster using Slurm specific commands. There are many commands available, but following three commands are the most common: sbatch - to submit job scancel - to delete job squeue - to view job status We can submit above job script using sbatch command: $ sbatch HelloCheaha.sh Submitted batch job 52707 When the job script is submitted, Slurm queues it up and assigns it a job number (e.g. 52707 in above example). The job number is available inside job script using environment variable $JOB_ID. This variable can be used inside job script to create job related directory structure or file names.","title":"Example Batch Job Script"},{"location":"cheaha/uabgrid_getting_started/#interactive-resources","text":"Login Node (the host that you connected to when you setup the SSH connection to Cheaha) is supposed to be used for submitting jobs and/or lighter prep work required for the job scripts. Do not run heavy computations on the login node . If you have a heavier workload to prepare for a batch job (eg. compiling code or other manipulations of data) or your compute application requires interactive control, you should request a dedicated interactive node for this work. Interactive resources are requested by submitting an \"interactive\" job to the scheduler. Interactive jobs will provide you a command line on a compute resource that you can use just like you would the command line on the login node. The difference is that the scheduler has dedicated the requested resources to your job and you can run your interactive commands without having to worry about impacting other users on the login node. Interactive jobs, that can be run on command line, are requested with the srun command. srun --ntasks = 1 --cpus-per-task = 4 --mem-per-cpu = 4096 --time = 08 :00:00 --partition = medium --job-name = JOB_NAME --pty /bin/bash This command requests for 4 cores (--cpus-per-task) for a single task (--ntasks) with each cpu requesting size 4GB of RAM (--mem-per-cpu) for 8 hrs (--time). More advanced interactive scenarios to support graphical applications are available using VNC or X11 tunneling X-Win32 2014 for Windows Interactive jobs that requires running a graphical application, are requested with the sinteractive command, via Terminal on your VNC window. sinteractive --ntasks = 1 --cpus-per-task = 4 --mem-per-cpu = 4096 --time = 08 :00:00 --partition = medium --job-name = JOB_NAME Please note, sinteractive starts your shell in a screen session. Screen is a terminal emulator that is designed to make it possible to detach and reattach a session. This feature can mostly be ignored. If you application uses ctrl-a as a special command sequence (e.g. Emacs), however, you may find the application doesn't receive this special character. When using screen, you need to type ctrl-a a (ctrl-a followed by a single \"a\" key press) to send a ctrl-a to your application. Screen uses ctrl-a as it's own command character, so this special sequence issues the command to screen to \"send ctrl-a to my app\". Learn more about screen from it's documentation .","title":"Interactive Resources"},{"location":"cheaha/uabgrid_getting_started/#storage","text":"","title":"Storage"},{"location":"cheaha/uabgrid_getting_started/#privacy","text":"Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s).","title":"Privacy"},{"location":"cheaha/uabgrid_getting_started/#file-and-directory-permissions","text":"The default permissions for all user data storage locations described below are as follows. In these descriptions, the \"$USER\" variable should be replaced with the user's account name string: /home/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /data/user/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /scratch/$USER - the owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. /data/projects/ - a PI can request project space for their lab or specific collaborations. The project directory is created with the PI/requestor as the user-owner and a dedicated collaboration group as the group-owner. The PI and all members of the dedicated collaboration group have can read, write/delete, and list files. No privileges are granted to other users of the system. Additional controls can be implemented via access control lists (ACLs). The PI/requestor can modify the ACLs to allow additional access to specific users. These permissions are the default configuration. While it is possible to modify these permissions or change the group owner of a file to any group to which a user belongs, users are encouraged to work within the default configuration and contact support@listserv.uab.edu if the default permissions are not adequate. Setting up a collaboration group and associated project directory can address most collaboration need while keep data access restricted to the minimum necessary users for the collaboration. Additional background on Linux file system permissions can be found here: https://its.unc.edu/research-computing/techdocs/how-to-use-unix-and-linux-file-permissions/ https://www.rc.fas.harvard.edu/resources/documentation/linux/unix-permissions/ https://hpc.nih.gov/storage/permissions.html","title":"File and Directory Permissions"},{"location":"cheaha/uabgrid_getting_started/#no-automatic-backups","text":"There is no automatic back up of any user data on the cluster in home, data, or scratch. At this time, all user data back up processes are defined and managed by each user and/or lab. Given that data backup demands vary widely between different users, groups, and research domains, this approach enables those who are most familiar with the data to make appropriate decisions based on their specific needs. For example, if a group is working with a large shared data set that is a local copy of a data set maintained authoritatively at a national data bank, maintaining a local backup is unlikely to be a productive use of limited storage resources, since this data could potentially be restored from the authoritative source. If, however, you are maintaining a unique source of data of which yours is the only copy, then maintaining a backup is critical if you value that data set. It's worth noting that while this \"uniqueness\" criteria may not apply to the data you analyze, it may readily apply to the codes that define your analysis pipelines. An often recommended backup policy is the 3-2-1 rule: maintain three copies of data, on two different media, with one copy off-site. You can read more about the 3-2-1 rule here . In the case of your application codes, using revision control tools during development provides an easy way to maintain a second copy, makes for a good software development process, and can help achieve reproducible research goals. Please review the data storage options provided by UAB IT for maintaining copies of your data. In choosing among these options, you should also be aware of UAB's data classification rules and requirements for security requirements for sensitive and restricted data storage. Given the importance of backup, Research Computing continues to explore options to facilitate data backup workflows from the cluster. Please contact us if you have questions or would like to discuss specific data backup scenarios. A good guide for thinking about your backup strategy might be: \"If you aren't managing a data back up process, then you have no backup data.\"","title":"No Automatic Backups"},{"location":"cheaha/uabgrid_getting_started/#home-directories","text":"Your home directory on Cheaha is NFS-mounted to the compute nodes as /home/$USER or $HOME. It is acceptable to use your home directory as a location to store job scripts and custom code. You are responsible for keeping your home directory under 10GB in size! The home directory must not be used to store large amounts of data. Please use $USER_SCRATCH for actively used data sets and $USER_DATA for storage of non scratch data.","title":"Home directories"},{"location":"cheaha/uabgrid_getting_started/#scratch","text":"Research Computing policy requires that all bulky input and output must be located on the scratch space. The home directory is intended to store your job scripts, log files, libraries and other supporting files. Important Information: Scratch space (network and local) is not backed up . Research Computing expects each user to keep their scratch areas clean. The cluster scratch area are not to be used for archiving data. Cheaha has two types of scratch space, network mounted and local. Network scratch ($USER_SCRATCH) is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 4.7PB of usable storage. This should be your jobs primary working directory, unless the job would benefit from local scratch (see below). Local scratch is physically located on each compute node and is not accessible to the other nodes (including the login node). This space is useful if the job performs a lot of file I/O. Most of the jobs that run on our clusters do not fall into this category. Because the local scratch is inaccessible outside the job, it is important to note that you must move any data between local scratch to your network accessible scratch within your job. For example, step 1 in the job could be to copy the input from $USER_SCRATCH to ${USER_SCRATCH}, step 2 code execution, step 3 move the data back to $USER_SCRATCH.","title":"Scratch"},{"location":"cheaha/uabgrid_getting_started/#network-scratch","text":"Network scratch is available using the environment variable $USER_SCRATCH or directly by /data/scratch/$USER It is advisable to use the environment variable whenever possible rather than the hard coded path.","title":"Network Scratch"},{"location":"cheaha/uabgrid_getting_started/#local-scratch","text":"Each compute node has a local scratch directory that is accessible via the variable $LOCAL_SCRATCH . If your job performs a lot of file I/O, the job should use $LOCAL_SCRATCH rather than $USER_SCRATCH to prevent bogging down the network scratch file system. The amount of scratch space available is approximately 800GB. The $LOCAL_SCRATCH is a special temporary directory and it's important to note that this directory is deleted when the job completes, so the job script has to move the results to $USER_SCRATCH or other location prior to the job exiting. Note that $LOCAL_SCRATCH is only useful for jobs in which all processes run on the same compute node, so MPI jobs are not candidates for this solution. The following is an array job example that uses $LOCAL_SCRATCH by transferring the inputs into $LOCAL_SCRATCH at the beginning of the script and the result out of $LOCAL_SCRATCH at the end of the script. #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes and the appropriate partition # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 echo \"TMPDIR: $LOCAL_SCRATCH \" cd $LOCAL_SCRATCH # Create a working directory under the special scheduler local scratch directory # using the array job's taskID mdkir $SLURM_ARRAY_TASK_ID cd $SLURM_ARRAY_TASK_ID # Next copy the input data to the local scratch echo \"Copying input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) # The input data in this case has a numerical file extension that # matches $SLURM_ARRAY_TASK_ID cp -a $USER_SCRATCH /GeneData/INP*. $SLURM_ARRAY_TASK_ID ./ echo \" copied input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) someapp -S 1 -D 10 -i INP*. $SLURM_ARRAY_TASK_ID -o geneapp.out. $SLURM_ARRAY_TASK_ID # Lastly copy the results back to network scratch echo \"Copying results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) cp -a geneapp.out. $SLURM_ARRAY_TASK_ID $USER_SCRATCH /GeneData/ echo \" Copied results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date )","title":"Local Scratch"},{"location":"cheaha/uabgrid_getting_started/#project-storage","text":"Cheaha has a location where shared data can be stored called $SHARE_PROJECT. As with user scratch, this area is not backed up ! This is helpful if a team of researchers must access the same data. Please open a help desk ticket to request a project directory under $SHARE_PROJECT.","title":"Project Storage"},{"location":"cheaha/uabgrid_getting_started/#uploading-data","text":"Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s). Data can be moved onto the cluster (pushed) from a remote client (ie. you desktop) via SCP or SFTP. Data can also be downloaded to the cluster (pulled) by issuing transfer commands once you are logged into the cluster. Common transfer methods are wget <URL> , FTP, or SCP, and depend on how the data is made available from the data provider. Large data sets should be staged directly to your $USER_SCRATCH directory so as not to fill up $HOME. If you are working on a data set shared with multiple users, it's preferable to request space in $SHARE_PROJECT rather than duplicating the data for each user.","title":"Uploading Data"},{"location":"cheaha/uabgrid_getting_started/#environment-modules","text":"Environment Modules is installed on Cheaha and should be used when constructing your job scripts if an applicable module file exists. Using the module command you can easily configure your environment for specific software packages without having to know the specific environment variables and values to set. Modules allows you to dynamically configure your environment without having to logout / login for the changes to take affect. If you find that specific software does not have a module, please submit a helpdesk ticket to request the module. Cheaha supports bash completion for the module command. For example, type 'module' and press the TAB key twice to see a list of options: module TAB TAB add display initlist keyword refresh switch use apropos help initprepend list rm unload whatis avail initadd initrm load show unuse clear initclear initswitch purge swap update To see the list of available modulefiles on the cluster, run the module avail command (note the example list below may not be complete!) or module load followed by two tab key presses: module avail ----------------------------------------------------------------------------------------- /cm/shared/modulefiles ----------------------------------------------------------------------------------------- acml/gcc/64/5.3.1 acml/open64-int64/mp/fma4/5.3.1 fftw2/openmpi/gcc/64/float/2.1.5 intel-cluster-runtime/ia32/3.8 netcdf/gcc/64/4.3.3.1 acml/gcc/fma4/5.3.1 blacs/openmpi/gcc/64/1.1patch03 fftw2/openmpi/open64/64/double/2.1.5 intel-cluster-runtime/intel64/3.8 netcdf/open64/64/4.3.3.1 acml/gcc/mp/64/5.3.1 blacs/openmpi/open64/64/1.1patch03 fftw2/openmpi/open64/64/float/2.1.5 intel-cluster-runtime/mic/3.8 netperf/2.7.0 acml/gcc/mp/fma4/5.3.1 blas/gcc/64/3.6.0 fftw3/openmpi/gcc/64/3.3.4 intel-tbb-oss/ia32/44_20160526oss open64/4.5.2.1 acml/gcc-int64/64/5.3.1 blas/open64/64/3.6.0 fftw3/openmpi/open64/64/3.3.4 intel-tbb-oss/intel64/44_20160526oss openblas/dynamic/0.2.15 acml/gcc-int64/fma4/5.3.1 bonnie++/1.97.1 gdb/7.9 iozone/3_434 openmpi/gcc/64/1.10.1 acml/gcc-int64/mp/64/5.3.1 cmgui/7.2 globalarrays/openmpi/gcc/64/5.4 lapack/gcc/64/3.6.0 openmpi/open64/64/1.10.1 acml/gcc-int64/mp/fma4/5.3.1 cuda75/blas/7.5.18 globalarrays/openmpi/open64/64/5.4 lapack/open64/64/3.6.0 pbspro/13.0.2.153173 acml/open64/64/5.3.1 cuda75/fft/7.5.18 hdf5/1.6.10 mpich/ge/gcc/64/3.2 puppet/3.8.4 acml/open64/fma4/5.3.1 cuda75/gdk/352.79 hdf5_18/1.8.16 mpich/ge/open64/64/3.2 rc-base acml/open64/mp/64/5.3.1 cuda75/nsight/7.5.18 hpl/2.1 mpiexec/0.84_432 scalapack/mvapich2/gcc/64/2.0.2 acml/open64/mp/fma4/5.3.1 cuda75/profiler/7.5.18 hwloc/1.10.1 mvapich/gcc/64/1.2rc1 scalapack/openmpi/gcc/64/2.0.2 acml/open64-int64/64/5.3.1 cuda75/toolkit/7.5.18 intel/compiler/32/15.0/2015.5.223 mvapich/open64/64/1.2rc1 sge/2011.11p1 acml/open64-int64/fma4/5.3.1 default-environment intel/compiler/64/15.0/2015.5.223 mvapich2/gcc/64/2.2b slurm/15.08.6 acml/open64-int64/mp/64/5.3.1 fftw2/openmpi/gcc/64/double/2.1.5 intel-cluster-checker/2.2.2 mvapich2/open64/64/2.2b torque/6.0.0.1 ---------------------------------------------------------------------------------------- /share/apps/modulefiles ----------------------------------------------------------------------------------------- rc/BrainSuite/15b rc/freesurfer/freesurfer-5.3.0 rc/intel/compiler/64/ps_2016/2016.0.047 rc/matlab/R2015a rc/SAS/v9.4 rc/cmg/2012.116.G rc/gromacs-intel/5.1.1 rc/Mathematica/10.3 rc/matlab/R2015b rc/dsistudio/dsistudio-20151020 rc/gtool/0.7.5 rc/matlab/R2012a rc/MRIConvert/2.0.8 --------------------------------------------------------------------------------------- /share/apps/rc/modules/all --------------------------------------------------------------------------------------- AFNI/linux_openmp_64-goolf-1.7.20-20160616 gperf/3.0.4-intel-2016a MVAPICH2/2.2b-GCC-4.9.3-2.25 Amber/14-intel-2016a-AmberTools-15-patchlevel-13-13 grep/2.15-goolf-1.4.10 NASM/2.11.06-goolf-1.7.20 annovar/2016Feb01-foss-2015b-Perl-5.22.1 GROMACS/5.0.5-intel-2015b-hybrid NASM/2.11.08-foss-2015b ant/1.9.6-Java-1.7.0_80 GSL/1.16-goolf-1.7.20 NASM/2.11.08-intel-2016a APBS/1.4-linux-static-x86_64 GSL/1.16-intel-2015b NASM/2.12.02-foss-2016a ASHS/rev103_20140612 GSL/2.1-foss-2015b NASM/2.12.02-intel-2015b Aspera-Connect/3.6.1 gtool/0.7.5_linux_x86_64 NASM/2.12.02-intel-2016a ATLAS/3.10.1-gompi-1.5.12-LAPACK-3.4.2 guile/1.8.8-GNU-4.9.3-2.25 ncurses/5.9-foss-2015b Autoconf/2.69-foss-2016a HAPGEN2/2.2.0 ncurses/5.9-GCC-4.8.4 Autoconf/2.69-GCC-4.8.4 HarfBuzz/1.2.7-intel-2016a ncurses/5.9-GNU-4.9.3-2.25 Autoconf/2.69-GNU-4.9.3-2.25 HDF5/1.8.15-patch1-intel-2015b ncurses/5.9-goolf-1.4.10 . . . . Some software packages have multiple module files, for example: GCC/4.7.2 GCC/4.8.1 GCC/4.8.2 GCC/4.8.4 GCC/4.9.2 GCC/4.9.3 GCC/4.9.3-2.25 In this case, the GCC module will always load the latest version, so loading this module is equivalent to loading GCC/4.9.3-2.25. If you always want to use the latest version, use this approach. If you want use a specific version, use the module file containing the appropriate version number. Some modules, when loaded, will actually load other modules. For example, the GROMACS/5.0.5-intel-2015b-hybrid module will also load intel/2015b and other related tools. To load a module, ex: for a GROMACS job, use the following module load command in your job script: module load GROMACS/5.0.5-intel-2015b-hybrid To see a list of the modules that you currently have loaded use the module list command module list Currently Loaded Modulefiles: 1 ) slurm/15.08.6 9 ) impi/5.0.3.048-iccifort-2015.3.187-GNU-4.9.3-2.25 17 ) Tcl/8.6.3-intel-2015b 2 ) rc-base 10 ) iimpi/7.3.5-GNU-4.9.3-2.25 18 ) SQLite/3.8.8.1-intel-2015b 3 ) GCC/4.9.3-binutils-2.25 11 ) imkl/11.2.3.187-iimpi-7.3.5-GNU-4.9.3-2.25 19 ) Tk/8.6.3-intel-2015b-no-X11 4 ) binutils/2.25-GCC-4.9.3-binutils-2.25 12 ) intel/2015b 20 ) Python/2.7.9-intel-2015b 5 ) GNU/4.9.3-2.25 13 ) bzip2/1.0.6-intel-2015b 21 ) Boost/1.58.0-intel-2015b-Python-2.7.9 6 ) icc/2015.3.187-GNU-4.9.3-2.25 14 ) zlib/1.2.8-intel-2015b 22 ) GROMACS/5.0.5-intel-2015b-hybrid 7 ) ifort/2015.3.187-GNU-4.9.3-2.25 15 ) ncurses/5.9-intel-2015b 8 ) iccifort/2015.3.187-GNU-4.9.3-2.25 16 ) libreadline/6.3-intel-2015b A module can be removed from your environment by using the module unload command: module unload GROMACS/5.0.5-intel-2015b-hybrid The definition of a module can also be viewed using the module show command, revealing what a specific module will do to your environment: module show GROMACS/5.0.5-intel-2015b-hybrid ------------------------------------------------------------------- /share/apps/rc/modules/all/GROMACS/5.0.5-intel-2015b-hybrid: module-whatis GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. - Homepage: http://www.gromacs.org conflict GROMACS prepend-path CPATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/include prepend-path LD_LIBRARY_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64 prepend-path LIBRARY_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64 prepend-path MANPATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/share/man prepend-path PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/bin prepend-path PKG_CONFIG_PATH /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/lib64/pkgconfig setenv EBROOTGROMACS /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid setenv EBVERSIONGROMACS 5 .0.5 setenv EBDEVELGROMACS /share/apps/rc/software/GROMACS/5.0.5-intel-2015b-hybrid/easybuild/GROMACS-5.0.5-intel-2015b-hybrid-easybuild-devel -------------------------------------------------------------------","title":"Environment Modules"},{"location":"cheaha/uabgrid_getting_started/#error-using-modules-from-a-job-script","text":"If you are using modules and the command your job executes runs fine from the command line but fails when you run it from the job, you may be having an issue with the script initialization. If you see this error in your job error output file -bash: module: line 1 : syntax error: unexpected end of file -bash: error importing function definition for ` BASH_FUNC_module ' Add the command unset module before calling your module files. The -V job argument will cause a conflict with the module function used in your script.","title":"Error Using Modules from a Job Script"},{"location":"cheaha/uabgrid_getting_started/#sample-job-scripts","text":"The following are sample job scripts, please be careful to edit these for your environment (i.e. replace YOUR_EMAIL_ADDRESS with your real email address), set the h_rt to an appropriate runtime limit and modify the job name and any other parameters. Hello World is the classic example used throughout programming. We don't want to buck the system, so we'll use it as well to demonstrate jobs submission with one minor variation: our hello world will send us a greeting using the name of whatever machine it runs on. For example, when run on the Cheaha login node, it would print \"Hello from login001\".","title":"Sample Job Scripts"},{"location":"cheaha/uabgrid_getting_started/#hello-world-serial","text":"A serial job is one that can run independently of other commands, ie. it doesn't depend on the data from other jobs running simultaneously. You can run many serial jobs in any order. This is a common solution to processing lots of data when each command works on a single piece of data. For example, running the same conversion on 100s of images. Here we show how to create job script for one simple command. Running more than one command just requires submitting more jobs. Create your hello world application. Run this command to create a script, turn it into to a command, and run the command (just copy and past the following on to the command line). Create the file: vim helloworld.sh Write into \"helloworld.sh\" file (To write in vim editor: press shift + I ) #!/bin/bash echo Hello from ` hostname ` Save the file by pressing the esc key, type the following :wq Need to give permission the \"helloworld.sh\" file chmod +x helloworld.sh Create the Slurm job script that will request 256 MB RAM and a maximum runtime of 10 minutes. Create the JOB file: vim helloworld.job Write into \"helloworld.job\" file (To write in vim editor: press shift + I ) #!/bin/bash #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=helloworld # # Set your error and output files # #SBATCH --error=helloworld.err #SBATCH --output=helloworld.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu ./helloworld.sh Save the file by pressing the esc key, type the following :wq Submit the job to Slurm scheduler and check the status using squeue $ sbatch helloworld.job Submitted batch job 52888 When the job completes, you should have output files named helloworld.out and helloworld.err $ cat helloworld.out Hello from c0003","title":"Hello World (serial)"},{"location":"cheaha/uabgrid_getting_started/#hello-world-parallel-with-mpi","text":"MPI is used to coordinate the activity of many computations occurring in parallel. It is commonly used in simulation software for molecular dynamics, fluid dynamics, and similar domains where there is significant communication (data) exchanged between cooperating process. Here is a simple parallel Slurm job script for running commands the rely on MPI. This example also includes the example of compiling the code and submitting the job script to the Slurm scheduler. First, create a directory for the Hello World jobs mkdir -p ~/jobs/helloworld cd ~/jobs/helloworld Create the Hello World code written in C (this example of MPI enabled Hello World includes a 3 minute sleep to ensure the job runs for several minutes, a normal hello world example would run in a matter of seconds). $ vi helloworld-mpi.c #include <stdio.h> #include <mpi.h> main ( int argc, char **argv ) { int rank, size ; int i, j ; float f ; MPI_Init ( & argc, & argv ) ; MPI_Comm_rank ( MPI_COMM_WORLD, & rank ) ; MPI_Comm_size ( MPI_COMM_WORLD, & size ) ; printf ( \"Hello World from process %d of %d.\\n\" , rank, size ) ; sleep ( 180 ) ; for ( j = 0 ; j< = 100000 ; j++ ) for ( i = 0 ; i< = 100000 ; i++ ) f = i*2.718281828*i+i+i*3.141592654 ; MPI_Finalize () ; } Compile the code, first purging any modules you may have loaded followed by loading the module for OpenMPI GNU. The mpicc command will compile the code and produce a binary named helloworld_gnu_openmpi module purge module load DefaultModules module load OpenMPI/4.0.1-GCC-8.3.0-2.32 mpicc helloworld-mpi.c -o helloworld_gnu_openmpi Create the Slurm job script that will request 8 cpu slots and a maximum runtime of 10 minutes $ vi helloworld.job #!/bin/bash #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=helloworld_mpi # # Set your error and output files # #SBATCH --error=helloworld_mpi.err #SBATCH --output=helloworld_mpi.out #SBATCH --ntasks=8 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load OpenMPI/1.8.8-GNU-4.9.3-2.25 mpirun -np $SLURM_NTASKS helloworld_gnu_openmpi Submit the job to Slurm scheduler and check the status using squeue -u $USER $ sbatch helloworld.job Submitted batch job 52893 $ squeue -u BLAZERID JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 52893 express hellowor BLAZERID R 2 :07 2 c [ 0005 -0006 ] When the job completes, you should have output files named helloworld_mpi.out and helloworld_mpi.err $ cat helloworld_mpi.out Hello World from process 1 of 8 . Hello World from process 3 of 8 . Hello World from process 4 of 8 . Hello World from process 7 of 8 . Hello World from process 5 of 8 . Hello World from process 6 of 8 . Hello World from process 0 of 8 . Hello World from process 2 of 8 .","title":"Hello World (parallel with MPI)"},{"location":"cheaha/uabgrid_getting_started/#hello-world-serial-revisited","text":"The job submit scripts (sbatch scripts) are actually bash shell scripts in their own right. The reason for using the funky #SBATCH prefix in the scripts is so that bash interprets any such line as a comment and won't execute it. Because the # character starts a comment in bash, we can weave the Slurm scheduler directives (the #SBATCH lines) into standard bash scripts. This lets us build scripts that we can execute locally and then easily run the same script to on a cluster node by calling it with sbatch. This can be used to our advantage to create a more fluid experience in moving between development and production job runs. The following example is a simple variation on the serial job above. All we will do is convert our Slurm job script into a command called helloworld that calls the helloworld.sh command. If the first line of a file is #!/bin/bash and that file is executable, the shell will automatically run the command as if were any other system command, eg. ls. That is, the \".sh\" extension on our HelloWorld.sh script is completely optional and is only meaningful to the user. Copy the serial helloworld.job script to a new file, add a the special #!/bin/bash as the first line, and make it executable with the following command (note: those are single quotes in the echo command): echo '#!/bin/bash' | cat helloworld.job > helloworld ; chmod +x helloworld Our sbatch script has now become a regular command. We can now execute the command with the simple prefix \"./helloworld\", which means \"execute this file in the current directory\": ./helloworld Hello from login001 Or if we want to run the command on a compute node, replace the \"./\" prefix with \"sbatch \": $ sbatch helloworld Submitted batch job 53001 And when the cluster run is complete you can look at the content of the output: $ $ cat helloworld.out Hello from c0003 You can use this approach of treating you sbatch files as command wrappers to build a collection of commands that can be executed locally or via sbatch. The other examples can be restructured similarly. To avoid having to use the \"./\" prefix, just add the current directory to your PATH. Also, if you plan to do heavy development using this feature on the cluster, please be sure to run sinteractive first so you don't load the login node with your development work.","title":"Hello World (serial) -- revisited"},{"location":"cheaha/uabgrid_getting_started/#gromacs","text":"#!/bin/bash #SBATCH --partition=short # # Name your job to make it easier for you to track # #SBATCH --job-name=test_gromacs # # Set your error and output files # #SBATCH --error=test_gromacs.err #SBATCH --output=test_gromacs.out #SBATCH --ntasks=8 # # Tell the scheduler only need 10 minutes # #SBATCH --time=10:00:00 #SBATCH --mem-per-cpu=2048 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load OpenMPI/1.8.8-GNU-4.9.3-2.25 module load GROMACS/5.0.5-intel-2015b-hybrid # Change directory to the job working directory if not already there cd ${ USER_SCRATCH } /jobs/gromacs # Single precision MDRUN = mdrun_mpi # Enter your tpr file over here export MYFILE = example.tpr mpirun -np SLURM_NTASKS $MDRUN -v -s $MYFILE -o $MYFILE -c $MYFILE -x $MYFILE -e $MYFILE -g ${ MYFILE } .log","title":"Gromacs"},{"location":"cheaha/uabgrid_getting_started/#r-array-job","text":"The following is an example job script that will use an array of 10 tasks (--array=1-10), each task has a max runtime of 2 hours and will use no more than 256 MB of RAM per task. Array's of tasks are useful when you have lots of simple jobs that work on their own separate files or a sub-set of the problem that can be selected by the array task index. For a more comprehensive introduction please see this tutorial . Create a working directory and the job submission script $ mkdir -p ~/jobs/ArrayExample $ cd ~/jobs/ArrayExample $ vi R-example-array.job #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 cd ~/jobs/ArrayExample/rep $SLURM_ARRAY_TASK_ID srun R CMD BATCH rscript.R Submit the job to the Slurm scheduler and check the status of the job using the squeue command sbatch R-example-array.job squeue -u $USER","title":"R (array job)"},{"location":"cheaha/uabgrid_getting_started/#array-job-parameterization","text":"Suppose you need to submit thousands of jobs. While you could do this in a for loop, the global limit on jobs in the SLURM queue is 10,000. The limit is in place for performance reasons and the jobs may be rejected with the following error message and an incomplete set of tasks. sbatch: error: Slurm temporarily unable to accept job, sleeping and retrying The preferred way to handle this scenario is to allow SLURM to schedule the jobs for you using the array flag in an sbatch script. This allows many jobs to be submitted as a single entry in the queue, letting SLURM handle the for loop and queueing. It is possible to reference the current loop index, or task id, as $SLURM_ARRAY_TASK_ID. An example using $SLURM_ARRAY_TASK_ID to load input files and create output files is shown below. Suppose you have a short script called my_processing_script that needs to be run on 20,000 separate files. Suppose each instance only needs 1 cpu and 2 GB of RAM and finishes in 5 minutes. Submitting these files all at once won't work and at least half of them will be rejected by SLURM. Instead we can use the sbatch array flag. Note that some other useful flags have been omitted for brevity. #! /bin/bash #SBATCH --partition=express #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --array=1-20000%100 # This will run tasks 1 through 20000, with up to 100 at a time. # It is possible to provide any comma-separated list of intervals. # An example of a valid subset is --array=1,2,5-1000,3777,4995-5000%100 INPUT_FILE = $USER_DATA /input/file_ $SLURM_ARRAY_TASK_ID .txt OUTPUT_FILE = $USER_DATA /output/file_ $SLURM_ARRAY_TASK_ID .txt my_processing_script --input = \" $INPUT_FILE \" --output = \" $OUTPUT_FILE \"","title":"Array Job Parameterization"},{"location":"cheaha/uabgrid_getting_started/#gpu-job","text":"A Graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Create a math.sh file as: $vim math.sh #!/bin/bash ( e = 5 ) echo $e (( e = e + 3 )) echo $e (( e = e+4 )) # -- spaces or no spaces, it doesn't matter echo $e Give File permissions for script as follows: $chmod +x math.sh Create Job submission script file: $vi math.job #!/bin/bash #SBATCH --share #SBATCH --partition=pascalnodes #SBATCH --gres=gpu:1 # Name your job to make it easier for you to track # #SBATCH --job-name=math # # Set your error and output files # #SBATCH --error=math.err #SBATCH --output=math.out #SBATCH --ntasks=1 # # Tell the scheduler only need 10 minutes # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu ./math.sh Submitting batch script to Slurm scheduler $sbatch math.job We can also request GPU's on cluster as: $sinteractive --ntasks = 1 --time = 00 :10:00 --exclusive --partition = pascalnodes -N2 --gres = gpu:2","title":"GPU JOB"},{"location":"cheaha/uabgrid_getting_started/#gpu-job-with-mpi","text":"As mentioned above, MPI is used to coordinate the activity of many computations occurring in parallel. It is commonly used in simulation software for molecular dynamics, fluid dynamics, and similar domains where there is significant communication (data) exchanged between cooperating process. An example of an GPU job with MPI can be found by visiting this link . Be sure to request the appropiate amount of gpu resources for your job: sinteractive --ntasks = 8 --time = 08 :00:00 --exclusive --partition = pascalnodes -N2 --gres = gpu:4","title":"GPU Job (with MPI)"},{"location":"cheaha/uabgrid_getting_started/#singularity-container","text":"Singularity is designed so that you can use it within SLURM jobs and it does not violate security constraints on the cluster. Singularity was built keeping HPC in mind, i.e a shared environment. Using Singularity container with SLURM job script is very easy, as the containers run as a process on the host machine, just like any other command in a batch script. You just need to load Singularity in your job script and run the command via a singularity process. Here's an example job script below: #!/bin/bash # #SBATCH --job-name=test-singularity #SBATCH --output=res.out #SBATCH --error=res.err # # Number of tasks needed for this job. Generally, used with MPI jobs #SBATCH --ntasks=1 #SBATCH --partition=express # # Time format = HH:MM:SS, DD-HH:MM:SS #SBATCH --time=10:00 # # Number of CPUs allocated to each task. #SBATCH --cpus-per-task=1 # # Mimimum memory required per allocated CPU in MegaBytes. #SBATCH --mem-per-cpu=100 # # Send mail to the email address when the job fails #SBATCH --mail-type=FAIL #SBATCH --mail-user=$USER@uab.edu #Set your environment here module load Singularity/2.5.2-GCC-5.4.0-2.26 #Run your singularity or any other commands here singularity exec -B /data/user/ $USER /data/user/ $USER /rc-training-sessions/neurodebian-neurodebian-master-latest.simg dcm2nii PATH_TO_YOUR_DICOM_FILES For a more comprehensive introduction please see this tutorial .","title":"Singularity Container"},{"location":"cheaha/uabgrid_getting_started/#installed-software","text":"A partial list of installed software with additional instructions for their use is available on the Cheaha Software page.","title":"Installed Software"},{"location":"cheaha/open_ondemand/ood_files/","text":"File Browser \u00b6 OOD provides an web-based file browser for your files on Cheaha. In order to access it, in the toolbar click Files --> <dir> , where <dir> is one of the choices of your $HOME , $USER_SCRATCH , or $USER_DATA directories. This will open up the following page: This page has a few parts to it: Home directory (red): A link to your $HOME directory and all of its subdirectories for easier navigation. This is always $HOME even if you chose $USER_DATA or $USER_SCRATCH to open. Working directory (green): The absolute path for the current directory you are in along with hyperlinks to the parent directories for easier navigation. File List (black): A list of all file and folders in the working directory along with select information. Hidden files and file permissions can be shown using the radio button in Command Bar at the top right (orange). File Command Menu (blue): A list of commands to perform to a file or directory. Danger Be careful deleting files here. They will be gone forever! OOD Command Menu (orange): A list of commands for navigating in the file browser, file or folder creation, and opening a terminal. OOD Command Menu \u00b6 Uploading Data \u00b6 Data can be uploaded from your local machine using this interface. Use the Upload button in the OOD Command Menu at the top right to select files from your local browser. Additionally, you can drag and drop files from your machine into the File List window as well. This should be limited to small files only. For large files or datasets, please use Globus instead. Opening a Terminal \u00b6 You can also open a bash terminal in the current directory using the >_Open in Terminal command. This should only be used for small tasks when fine-grain researcher control is necessary because the terminal is running on the login node. For compute-intensive tasks, either request an interactive session in the terminal or request an HPC Desktop session through the Interactive Apps and use the terminal there.","title":"File Browser"},{"location":"cheaha/open_ondemand/ood_files/#file-browser","text":"OOD provides an web-based file browser for your files on Cheaha. In order to access it, in the toolbar click Files --> <dir> , where <dir> is one of the choices of your $HOME , $USER_SCRATCH , or $USER_DATA directories. This will open up the following page: This page has a few parts to it: Home directory (red): A link to your $HOME directory and all of its subdirectories for easier navigation. This is always $HOME even if you chose $USER_DATA or $USER_SCRATCH to open. Working directory (green): The absolute path for the current directory you are in along with hyperlinks to the parent directories for easier navigation. File List (black): A list of all file and folders in the working directory along with select information. Hidden files and file permissions can be shown using the radio button in Command Bar at the top right (orange). File Command Menu (blue): A list of commands to perform to a file or directory. Danger Be careful deleting files here. They will be gone forever! OOD Command Menu (orange): A list of commands for navigating in the file browser, file or folder creation, and opening a terminal.","title":"File Browser"},{"location":"cheaha/open_ondemand/ood_files/#ood-command-menu","text":"","title":"OOD Command Menu"},{"location":"cheaha/open_ondemand/ood_files/#uploading-data","text":"Data can be uploaded from your local machine using this interface. Use the Upload button in the OOD Command Menu at the top right to select files from your local browser. Additionally, you can drag and drop files from your machine into the File List window as well. This should be limited to small files only. For large files or datasets, please use Globus instead.","title":"Uploading Data"},{"location":"cheaha/open_ondemand/ood_files/#opening-a-terminal","text":"You can also open a bash terminal in the current directory using the >_Open in Terminal command. This should only be used for small tasks when fine-grain researcher control is necessary because the terminal is running on the login node. For compute-intensive tasks, either request an interactive session in the terminal or request an HPC Desktop session through the Interactive Apps and use the terminal there.","title":"Opening a Terminal"},{"location":"cheaha/open_ondemand/ood_interactive/","text":"Interactive Apps \u00b6 The Interactive Apps dropdown from the toolbar will list a few standalone programs you are able to launch directly from the browser as well as an HPC Desktop that will allow you access all of the other software on Cheaha. Currently, the available standalone programs are IGV, Matlab, RStudio, SAS, and Jupyter. All of the interactive apps have similar setup pages. For instance, if we click HPC Desktop, the following screen will appear: This will allow to choose the number of hours, partition, number of cpus, and memory per cpu needed for the job. These fields are common to all interactive apps and are required. Not all partitions are available when creating an interactive job in OOD. For instance, if you need to use the largemem partition, request those resources in a terminal session for an interactive job or submit a batch job. Tip You can decrease wait time in the queue by choosing resources carefully. The closer your request is to actual usage, the more optimal your wait time will be. Please see our section on Job Efficiency for more information. Once you've selected the compute resources you need, Launch the job. This will bring you to the My Interactive Sessions page, and read on. My Interactive Sessions \u00b6 The My Interactive Sessions page looks like: For each job running via Open OnDemand, there will be a card listed on this page. Each card has basic information about the number of cores, nodes, and time remaining for the job. Also shown are the Job ID in the top-left part of the job card, and a Session ID link near the center of the card. The Job ID and Session ID are important for diagnosing issues you may encounter on Cheaha while using Open OnDemand. The Session ID link points to a folder in your home directory with information we may ask for. Click the Launch Desktop in new tab button to open your interactive VNC session. You may have to wait for the card to change from blue to green to launch the job. Bug If your job fails to launch, please see our FAQ for possible solutions, or contact us . Note For HPC Desktop, you do not need to request resources after you open the Desktop. You are already on a compute node. Any tasks you run will use the resources you requested when initializing the job. Note You can request another interactive session in a terminal in HPC Desktop. Only the terminal you requested the other interactive session in will have access to the new resources. Everything else in the HPC Desktop will run with the resources you requested when creating the initial job. These interactive jobs can be stopped early by clicking Delete on the right side of the job card. Standalone Programs \u00b6 As shown earlier, some software can be run outside of the VNC session. Setup for most of these follow the same rules as creation of an HPC Desktop job in terms of requesting resources. You will also need to select the version of software to use for the job. Note Versions shown in the OOD form may not line up with versions available in modules. If you need a version not available in OOD, please feel free to send in a ticket . RStudio Server \u00b6 RStudio is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. You'll also need to select both the version of RStudio you wish to use, and the version of R you wish to use. The job creation form is shown below. To adjust the environment, please use the Environment Setup box to load modules or Anaconda environments. A common strategy when there is difficulty building R packages is to search for it on Anaconda , create an Anaconda environment with that package already built, and load the environment in the Environment Setup box. An example is shown below. Important We have recently changed the way we deploy RStudio on OOD. There are now two versions available, and one is marked with the text (deprecated) . The version without the deprecation notice is the new deployment method and is preferred for use. It is more flexible and more robust, and will allow us to support you more quickly and easily. The version with the deprecation notice is the previous containerized version. Because it is deprecated, we will not provide additional support for that version. Please move your workflows to the newer version. Jupyter Notebook \u00b6 Jupyter Notebooks are available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. To adjust the environment, please use the Environment Setup box to load modules. For GPU applications it is generally necessary to load one of our cuda##.#/toolkit modules, and possibly a cuDNN module. These are required for tensorflow , keras and pytorch . Use module spider cuda and module spider cudnn to view the list of appropriate modules. An example is shown below. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. Tip You do not need module load Anaconda3 in the Environment Setup field, it is loaded automatically. Warning Having conda activate statements in the Environment Setup field can cause unexpected and silent job failure. Please do not activate conda environments in the Environment Setup field. Extra Jupyter Arguments \u00b6 The Extra Jupyter Arguments field allows you to pass additional arguments to the Jupyter Server as it is being started. It can be helpful to point the server to the folder containing your notebook. To do this, assuming your notebooks are stored in /data/user/$USER , also known as $USER_DATA , put --notebook-dir=$USER_DATA in this field. You will be able to navigate to the notebook if it is in a subdirectory of notebook-dir , but you won't be able to navigate to any other directories. An example is shown below. Submitting the Jupyter Notebook Job \u00b6 Submitting the job will bring you to the My Interactive Jobs window while the Jupyter job is initialized. Click Connect to Jupyter to open the Jupyter Home Page. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. The Jupyter Server Home Page \u00b6 The Jupyter Server Home Page will look like the following From here, you can navigate to and select an existing notebook, or you can create a new one using one of your existing virtual environments or the base environment. Once inside a Jupyter notebook, you can use the Kernel --> Change kernel menu to select your preferred Anaconda environment. Important See Anaconda Environments for information on Jupyter related packages. Warning Files deleted within using the Jupyter server interface are not truly deleted. Instead they are moved to $HOME/.local/share/Trash . This may cause inflation of personal storage usage on Cheaha. Python Libraries and Virtual Environments \u00b6 To run Jupyter with specific libraries and packages outside of the base install, you will need to create a virtual environment first. You can do this either in an HPC Desktop job or in the Conda tab of the Jupyter homepage. The Conda has the following layout: Current environments (red): a listing of the current existing environments in your $HOME/.conda/envs folder. Available packages (green): a list of all packages available to install from conda sources. Installed packages (blue): a list of the packages installed in the currently selected environment. To create a new environment, click the + button at the top of the Current environments pane and enter the name of the environment. After it has been created, you can select packages to install by searching for the package name at the top right of the Available packages pane. After selecting the package, click the -> button, and the package and all its dependencies will be installed. Note If a package is not available using the conda command directly, it will not be listed as an available package. Use a terminal window to install the package as necessary. Note In order to use an environment with Jupyter, the ipykernel library is necessary. Creating an environment in the Conda tab will autoinstall this library. If using the terminal, use conda install ipykernel to install it. After successfully creating your environment, navigate to the Files tab. You can create a new notebook using the New dropdown menu in the top right. Select your virtual environment of choice, and a notebook will be created and opened.","title":"Interactive Apps"},{"location":"cheaha/open_ondemand/ood_interactive/#interactive-apps","text":"The Interactive Apps dropdown from the toolbar will list a few standalone programs you are able to launch directly from the browser as well as an HPC Desktop that will allow you access all of the other software on Cheaha. Currently, the available standalone programs are IGV, Matlab, RStudio, SAS, and Jupyter. All of the interactive apps have similar setup pages. For instance, if we click HPC Desktop, the following screen will appear: This will allow to choose the number of hours, partition, number of cpus, and memory per cpu needed for the job. These fields are common to all interactive apps and are required. Not all partitions are available when creating an interactive job in OOD. For instance, if you need to use the largemem partition, request those resources in a terminal session for an interactive job or submit a batch job. Tip You can decrease wait time in the queue by choosing resources carefully. The closer your request is to actual usage, the more optimal your wait time will be. Please see our section on Job Efficiency for more information. Once you've selected the compute resources you need, Launch the job. This will bring you to the My Interactive Sessions page, and read on.","title":"Interactive Apps"},{"location":"cheaha/open_ondemand/ood_interactive/#my-interactive-sessions","text":"The My Interactive Sessions page looks like: For each job running via Open OnDemand, there will be a card listed on this page. Each card has basic information about the number of cores, nodes, and time remaining for the job. Also shown are the Job ID in the top-left part of the job card, and a Session ID link near the center of the card. The Job ID and Session ID are important for diagnosing issues you may encounter on Cheaha while using Open OnDemand. The Session ID link points to a folder in your home directory with information we may ask for. Click the Launch Desktop in new tab button to open your interactive VNC session. You may have to wait for the card to change from blue to green to launch the job. Bug If your job fails to launch, please see our FAQ for possible solutions, or contact us . Note For HPC Desktop, you do not need to request resources after you open the Desktop. You are already on a compute node. Any tasks you run will use the resources you requested when initializing the job. Note You can request another interactive session in a terminal in HPC Desktop. Only the terminal you requested the other interactive session in will have access to the new resources. Everything else in the HPC Desktop will run with the resources you requested when creating the initial job. These interactive jobs can be stopped early by clicking Delete on the right side of the job card.","title":"My Interactive Sessions"},{"location":"cheaha/open_ondemand/ood_interactive/#standalone-programs","text":"As shown earlier, some software can be run outside of the VNC session. Setup for most of these follow the same rules as creation of an HPC Desktop job in terms of requesting resources. You will also need to select the version of software to use for the job. Note Versions shown in the OOD form may not line up with versions available in modules. If you need a version not available in OOD, please feel free to send in a ticket .","title":"Standalone Programs"},{"location":"cheaha/open_ondemand/ood_interactive/#rstudio-server","text":"RStudio is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. You'll also need to select both the version of RStudio you wish to use, and the version of R you wish to use. The job creation form is shown below. To adjust the environment, please use the Environment Setup box to load modules or Anaconda environments. A common strategy when there is difficulty building R packages is to search for it on Anaconda , create an Anaconda environment with that package already built, and load the environment in the Environment Setup box. An example is shown below. Important We have recently changed the way we deploy RStudio on OOD. There are now two versions available, and one is marked with the text (deprecated) . The version without the deprecation notice is the new deployment method and is preferred for use. It is more flexible and more robust, and will allow us to support you more quickly and easily. The version with the deprecation notice is the previous containerized version. Because it is deprecated, we will not provide additional support for that version. Please move your workflows to the newer version.","title":"RStudio Server"},{"location":"cheaha/open_ondemand/ood_interactive/#jupyter-notebook","text":"Jupyter Notebooks are available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. To adjust the environment, please use the Environment Setup box to load modules. For GPU applications it is generally necessary to load one of our cuda##.#/toolkit modules, and possibly a cuDNN module. These are required for tensorflow , keras and pytorch . Use module spider cuda and module spider cudnn to view the list of appropriate modules. An example is shown below. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. Tip You do not need module load Anaconda3 in the Environment Setup field, it is loaded automatically. Warning Having conda activate statements in the Environment Setup field can cause unexpected and silent job failure. Please do not activate conda environments in the Environment Setup field.","title":"Jupyter Notebook"},{"location":"cheaha/open_ondemand/ood_interactive/#extra-jupyter-arguments","text":"The Extra Jupyter Arguments field allows you to pass additional arguments to the Jupyter Server as it is being started. It can be helpful to point the server to the folder containing your notebook. To do this, assuming your notebooks are stored in /data/user/$USER , also known as $USER_DATA , put --notebook-dir=$USER_DATA in this field. You will be able to navigate to the notebook if it is in a subdirectory of notebook-dir , but you won't be able to navigate to any other directories. An example is shown below.","title":"Extra Jupyter Arguments"},{"location":"cheaha/open_ondemand/ood_interactive/#submitting-the-jupyter-notebook-job","text":"Submitting the job will bring you to the My Interactive Jobs window while the Jupyter job is initialized. Click Connect to Jupyter to open the Jupyter Home Page. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running.","title":"Submitting the Jupyter Notebook Job"},{"location":"cheaha/open_ondemand/ood_interactive/#the-jupyter-server-home-page","text":"The Jupyter Server Home Page will look like the following From here, you can navigate to and select an existing notebook, or you can create a new one using one of your existing virtual environments or the base environment. Once inside a Jupyter notebook, you can use the Kernel --> Change kernel menu to select your preferred Anaconda environment. Important See Anaconda Environments for information on Jupyter related packages. Warning Files deleted within using the Jupyter server interface are not truly deleted. Instead they are moved to $HOME/.local/share/Trash . This may cause inflation of personal storage usage on Cheaha.","title":"The Jupyter Server Home Page"},{"location":"cheaha/open_ondemand/ood_interactive/#python-libraries-and-virtual-environments","text":"To run Jupyter with specific libraries and packages outside of the base install, you will need to create a virtual environment first. You can do this either in an HPC Desktop job or in the Conda tab of the Jupyter homepage. The Conda has the following layout: Current environments (red): a listing of the current existing environments in your $HOME/.conda/envs folder. Available packages (green): a list of all packages available to install from conda sources. Installed packages (blue): a list of the packages installed in the currently selected environment. To create a new environment, click the + button at the top of the Current environments pane and enter the name of the environment. After it has been created, you can select packages to install by searching for the package name at the top right of the Available packages pane. After selecting the package, click the -> button, and the package and all its dependencies will be installed. Note If a package is not available using the conda command directly, it will not be listed as an available package. Use a terminal window to install the package as necessary. Note In order to use an environment with Jupyter, the ipykernel library is necessary. Creating an environment in the Conda tab will autoinstall this library. If using the terminal, use conda install ipykernel to install it. After successfully creating your environment, navigate to the Files tab. You can create a new notebook using the New dropdown menu in the top right. Select your virtual environment of choice, and a notebook will be created and opened.","title":"Python Libraries and Virtual Environments"},{"location":"cheaha/open_ondemand/ood_jobs/","text":"Job Viewer and Composer \u00b6 Using the Jobs dropdown menu in the toolbar, you can view the status of your current submitted jobs and how long they have been running as well as submit new jobs via the job composer. View Current Jobs \u00b6 Click Jobs > Active Jobs . This will open a new window with you current active jobs that looks like: There are fields such as job ID, job name, time spent active, and the queue or partition. You can sort your job list by any of these fields and can filter for specific jobs using the Filter option at the top right. Additionally, you can view more detailed information about a job by clicking the arrow to the left of the job ID. The resulting table will look like: This table shows extra information such as the total number of nodes, CPUs, time limit, and memory requested for the job. The path to the output file for the job created by SLURM will also be listed. You can open the file location in a file browser or in a terminal here. You can end a currently running job by pressing Delete at the bottom right of these expanded job details. Job Composer \u00b6 The job composer allows you to create SLURM jobs directly in the web interface without having to create a VNC session. Clicking Jobs > Job Composer will bring up a new window: To create a job: Select New Job > From Default Template . This will bring up the job details window pane. Click Open Editor at the bottom of the Job Details. This will open an editable document in another browser window. Paste or type in your script directives including SBATCH options. Once done, click Save in the top left of the editor and close the tab. In the main job window, click Submit , and your job will be submitted to the scheduler. View its status on the Active Jobs page.","title":"Job Composer"},{"location":"cheaha/open_ondemand/ood_jobs/#job-viewer-and-composer","text":"Using the Jobs dropdown menu in the toolbar, you can view the status of your current submitted jobs and how long they have been running as well as submit new jobs via the job composer.","title":"Job Viewer and Composer"},{"location":"cheaha/open_ondemand/ood_jobs/#view-current-jobs","text":"Click Jobs > Active Jobs . This will open a new window with you current active jobs that looks like: There are fields such as job ID, job name, time spent active, and the queue or partition. You can sort your job list by any of these fields and can filter for specific jobs using the Filter option at the top right. Additionally, you can view more detailed information about a job by clicking the arrow to the left of the job ID. The resulting table will look like: This table shows extra information such as the total number of nodes, CPUs, time limit, and memory requested for the job. The path to the output file for the job created by SLURM will also be listed. You can open the file location in a file browser or in a terminal here. You can end a currently running job by pressing Delete at the bottom right of these expanded job details.","title":"View Current Jobs"},{"location":"cheaha/open_ondemand/ood_jobs/#job-composer","text":"The job composer allows you to create SLURM jobs directly in the web interface without having to create a VNC session. Clicking Jobs > Job Composer will bring up a new window: To create a job: Select New Job > From Default Template . This will bring up the job details window pane. Click Open Editor at the bottom of the Job Details. This will open an editable document in another browser window. Paste or type in your script directives including SBATCH options. Once done, click Save in the top left of the editor and close the tab. In the main job window, click Submit , and your job will be submitted to the scheduler. View its status on the Active Jobs page.","title":"Job Composer"},{"location":"cheaha/open_ondemand/ood_main/","text":"Homepage \u00b6 Navigating to rc.uab.edu will take you to the OOD homepage: You will find system-wide messages from admins at the tope of the page (red outline). These will always include links to the Research Computing Office Hours on Zoom. This will also be the place to see information about ongoing maintenance. In the middle of the page (green outline), you will see a Message of the Day containing the email address for support if you are having any issues with Cheaha. There are also links to our Acceptable Use Policy as well as links to our documentation. Lastly, there is a table with a list of available SLURM partitions on Cheaha with their max runtime and number of compute nodes per job as well as their priority. Use this table to plan job requests based on your needed computational resources. Toolbar \u00b6 To access all of the features OOD has to offer, use the toolbar at the top of the page that looks like: In it, you will find options to: Directly access your files on Cheaha View currently running jobs Interface with Cheaha via a shell terminal Request interactive sessions To use a shell terminal in Cheaha through OOD, click Clusters >> >_Cheaha Shell Access . You can use this exactly like a standard ssh tunnel. Warning Using the shell terminal in this way puts you on the login node. Do not run any compute tasks on the login node. Request a compute node first!","title":"Homepage"},{"location":"cheaha/open_ondemand/ood_main/#homepage","text":"Navigating to rc.uab.edu will take you to the OOD homepage: You will find system-wide messages from admins at the tope of the page (red outline). These will always include links to the Research Computing Office Hours on Zoom. This will also be the place to see information about ongoing maintenance. In the middle of the page (green outline), you will see a Message of the Day containing the email address for support if you are having any issues with Cheaha. There are also links to our Acceptable Use Policy as well as links to our documentation. Lastly, there is a table with a list of available SLURM partitions on Cheaha with their max runtime and number of compute nodes per job as well as their priority. Use this table to plan job requests based on your needed computational resources.","title":"Homepage"},{"location":"cheaha/open_ondemand/ood_main/#toolbar","text":"To access all of the features OOD has to offer, use the toolbar at the top of the page that looks like: In it, you will find options to: Directly access your files on Cheaha View currently running jobs Interface with Cheaha via a shell terminal Request interactive sessions To use a shell terminal in Cheaha through OOD, click Clusters >> >_Cheaha Shell Access . You can use this exactly like a standard ssh tunnel. Warning Using the shell terminal in this way puts you on the login node. Do not run any compute tasks on the login node. Request a compute node first!","title":"Toolbar"},{"location":"cheaha/slurm/gpu/","text":"GPUs \u00b6 Available Devices \u00b6 Currently, the Cheaha cluster has 18 nodes dedicated to GPU use under the pascalnodes partition family. Each node contains 4 individual NVIDIA P100 GPUs. These GPUs have the following specifications: GPU Architecture NVIDIA Pascal NVIDIA CUDA Cores 3584 GPU Memory 16GB CoWoS HBM2 at 732 GB/s Double-Precision Performance 4.7 TeraFLOPS Single-Precision Performance 9.3 TeraFLOPS Compute APIs CUDA, DirectCompute, OpenCL, OpenACC For more information on these nodes, see Detailed Hardware Information . Scheduling GPUs \u00b6 To successfully request access to GPUs, you will need to set the partition to one of the pascalnodes family of partitions depending on how much time you need for the job. Partition Time Limit pascalnodes 12 hours pascalnodes-medium 50 hours Additionally, when requesting a job using sbatch , you will need to include a SLURM directive --gres=gpu:# where # is the number of GPUs you need. Note It is suggested that at least 2 CPUs are requested for every GPU to begin with. The user should monitor and adjust the number of cores on subsequent job submissions if necessary. Look at Managing Jobs for more information. Open OnDemand \u00b6 When requesting an interactive job through Open OnDemand , selecting the pascalnodes partitions will automatically request access to one GPU as well. There is currently no way to change the number of GPUs for OOD interactive jobs. CUDA Toolkit \u00b6 You will need to load a CUDA toolkit module for relevant commands to access the GPUs. Depending on which version of tensorflow, pytorch, or other similar software you are using, a different version of the CUDA toolkit may be required. For instance, tensorflow version 2.5.0 requires CUDA toolkit version 11.2. Several CUDA toolkit versions have been installed as modules on Cheaha. To see which CUDA toolkits are available, use: module -r spider 'cuda.*toolkit' If a specific version of the CUDA toolkit is needed but not installed, send an install request to [support@listserv.uab.edu].","title":"GPUs"},{"location":"cheaha/slurm/gpu/#gpus","text":"","title":"GPUs"},{"location":"cheaha/slurm/gpu/#available-devices","text":"Currently, the Cheaha cluster has 18 nodes dedicated to GPU use under the pascalnodes partition family. Each node contains 4 individual NVIDIA P100 GPUs. These GPUs have the following specifications: GPU Architecture NVIDIA Pascal NVIDIA CUDA Cores 3584 GPU Memory 16GB CoWoS HBM2 at 732 GB/s Double-Precision Performance 4.7 TeraFLOPS Single-Precision Performance 9.3 TeraFLOPS Compute APIs CUDA, DirectCompute, OpenCL, OpenACC For more information on these nodes, see Detailed Hardware Information .","title":"Available Devices"},{"location":"cheaha/slurm/gpu/#scheduling-gpus","text":"To successfully request access to GPUs, you will need to set the partition to one of the pascalnodes family of partitions depending on how much time you need for the job. Partition Time Limit pascalnodes 12 hours pascalnodes-medium 50 hours Additionally, when requesting a job using sbatch , you will need to include a SLURM directive --gres=gpu:# where # is the number of GPUs you need. Note It is suggested that at least 2 CPUs are requested for every GPU to begin with. The user should monitor and adjust the number of cores on subsequent job submissions if necessary. Look at Managing Jobs for more information.","title":"Scheduling GPUs"},{"location":"cheaha/slurm/gpu/#open-ondemand","text":"When requesting an interactive job through Open OnDemand , selecting the pascalnodes partitions will automatically request access to one GPU as well. There is currently no way to change the number of GPUs for OOD interactive jobs.","title":"Open OnDemand"},{"location":"cheaha/slurm/gpu/#cuda-toolkit","text":"You will need to load a CUDA toolkit module for relevant commands to access the GPUs. Depending on which version of tensorflow, pytorch, or other similar software you are using, a different version of the CUDA toolkit may be required. For instance, tensorflow version 2.5.0 requires CUDA toolkit version 11.2. Several CUDA toolkit versions have been installed as modules on Cheaha. To see which CUDA toolkits are available, use: module -r spider 'cuda.*toolkit' If a specific version of the CUDA toolkit is needed but not installed, send an install request to [support@listserv.uab.edu].","title":"CUDA Toolkit"},{"location":"cheaha/slurm/introduction/","text":"Introduction to SLURM \u00b6 All work on Cheaha must be submitted to the queueing system, Slurm. This doc gives a basic overview of Slurm and how to use it. Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more. The main Slurm documentation can be found at the Slurm site . The Slurm Quickstart can also be helpful for orienting researchers new to queueing systems on the cluster. Batch Job Workflow \u00b6 Stage data to $USER_DATA , $USER_SCRATCH , or a project directory. Research how to run your directives in 'batch' mode. In other words, how to run your analysis pipeline from the command line, with no GUIs or researcher input. Identify the appropriate resources necessary to run the jobs (CPUs, time, memory, etc) Write a job script specifying these parameters using Slurm directives. Submit the job ( sbatch ) Monitor the job ( squeue ) Review the results, and modify/rerun if necessary ( sacct and seff ) Remove data from $USER_SCRATCH","title":"Introduction"},{"location":"cheaha/slurm/introduction/#introduction-to-slurm","text":"All work on Cheaha must be submitted to the queueing system, Slurm. This doc gives a basic overview of Slurm and how to use it. Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more. The main Slurm documentation can be found at the Slurm site . The Slurm Quickstart can also be helpful for orienting researchers new to queueing systems on the cluster.","title":"Introduction to SLURM"},{"location":"cheaha/slurm/introduction/#batch-job-workflow","text":"Stage data to $USER_DATA , $USER_SCRATCH , or a project directory. Research how to run your directives in 'batch' mode. In other words, how to run your analysis pipeline from the command line, with no GUIs or researcher input. Identify the appropriate resources necessary to run the jobs (CPUs, time, memory, etc) Write a job script specifying these parameters using Slurm directives. Submit the job ( sbatch ) Monitor the job ( squeue ) Review the results, and modify/rerun if necessary ( sacct and seff ) Remove data from $USER_SCRATCH","title":"Batch Job Workflow"},{"location":"cheaha/slurm/job_management/","text":"Managing Jobs \u00b6 When jobs are submitted, researchers can monitor their status using Slurm commands. Additionally, researchers can get information about completed jobs regarding their CPU and memory usage during execution for planning future jobs. Both of these cases should be a regular part of using Cheaha for researchers. In case jobs were submitted by accident or the code was written incorrectly, they can also be cancelled. Monitoring Jobs \u00b6 Currently running jobs can be monitored using the squeue command. The basic command to list all jobs for a specific researcher is: squeue -u $USER The output of squeue will look like: By default the fields displayed are jobid , partition , jobname as name , blazerid as user , job state as st , total run time as time , number of nodes as node , and the list of nodes as nodelist , used for each job a researcher has submitted. For array jobs, the job id will be formatted as jobid_arrayid . Further information about filtering by job name or partition, including information about memory or number of CPUs, and info regarding messages specific to a job's status can be seen using man squeue . Cancelling Jobs \u00b6 Cancelling queued and currently running jobs can be done using the scancel command. Importantly, this will only cancel jobs that were initiated by the researcher running the command. scancel is very flexible in how it behaves: # cancel a single job or an entire job array scancel <jobid> # cancel specific job array IDs, specified as single number or a range scancel <jobid_arrayid> # cancel all jobs on a partition for the user scancel -p <partition> # cancel all jobs for a researcher scancel -u $USER Warning Cancelling all jobs will also cancel the interactive jobs created on the Open OnDemand portal. More information on options to cancel jobs can be seen using man scancel . Reviewing Past Jobs \u00b6 If you are planning a new set of jobs and are estimating resource requests, it is useful to review similar jobs that have already completed. To list past jobs for a researcher, use the sacct command. Tip To minimize queue wait times and make best use of resources, please review job efficiency using seff . See our Job Efficiency page for more information. Review With Job ID \u00b6 The basic form is to use -j along with a job ID to list information about that job. sacct -j [ jobid ] This command will output basic information such as the ID, Name, Partition, Allocated CPUs, and State for the given job ID. Jobs can have matching extern and/or batch job entries as well. These are not especially helpful for most researchers. You can remove these entries using: sacct -j [ jobid ] | grep -wv -e extern -e batch Review Jobs Submitted Between Specific Timepoints \u00b6 If you do not remember the job ID, you can use the -S and -E flags to retrieve jobs submitted between the given start datetime and end datetime. Valid start/end time formats are: HH:MM[:SS] [AM|PM] MMDD[YY] or MM/DD[/YY] or MM.DD[.YY] MM/DD[/YY]-HH:MM[:SS] YYYY-MM-DD[THH:MM[:SS]] Notes: Anything in [] is optional Times can be specified in either 12-hour with AM/PM or 24-hour For the last specification, the T itself is inserted, it is not replaced with any value. For example, requesting jobs starting after 12:30 PM on October 5, 2021, the form would be 2021-10-05T12:30 . For example, to retrieve jobs submitted during the month of July 2021, the command could be: sacct -S 070121 -E 073121 sacct -S 07 /01/21 -E 07 /31/21 sacct -S 2021 -07-01 -E 2021 -07-31 Customizing the Output \u00b6 You can add -o with a list of output fields to customize the information you see. sacct -j [ jobid ] -o jobid,start,end,state,alloccpu,reqmem This command will output the job ID, the start time, end time, the state, the number of allocated CPUs, and the requested memory for the specified job. All potential output fields can be seen using sacct --helpformat . Their descriptions can be found on the sacct documentation under Job Accounting Fields.","title":"Managing Jobs"},{"location":"cheaha/slurm/job_management/#managing-jobs","text":"When jobs are submitted, researchers can monitor their status using Slurm commands. Additionally, researchers can get information about completed jobs regarding their CPU and memory usage during execution for planning future jobs. Both of these cases should be a regular part of using Cheaha for researchers. In case jobs were submitted by accident or the code was written incorrectly, they can also be cancelled.","title":"Managing Jobs"},{"location":"cheaha/slurm/job_management/#monitoring-jobs","text":"Currently running jobs can be monitored using the squeue command. The basic command to list all jobs for a specific researcher is: squeue -u $USER The output of squeue will look like: By default the fields displayed are jobid , partition , jobname as name , blazerid as user , job state as st , total run time as time , number of nodes as node , and the list of nodes as nodelist , used for each job a researcher has submitted. For array jobs, the job id will be formatted as jobid_arrayid . Further information about filtering by job name or partition, including information about memory or number of CPUs, and info regarding messages specific to a job's status can be seen using man squeue .","title":"Monitoring Jobs"},{"location":"cheaha/slurm/job_management/#cancelling-jobs","text":"Cancelling queued and currently running jobs can be done using the scancel command. Importantly, this will only cancel jobs that were initiated by the researcher running the command. scancel is very flexible in how it behaves: # cancel a single job or an entire job array scancel <jobid> # cancel specific job array IDs, specified as single number or a range scancel <jobid_arrayid> # cancel all jobs on a partition for the user scancel -p <partition> # cancel all jobs for a researcher scancel -u $USER Warning Cancelling all jobs will also cancel the interactive jobs created on the Open OnDemand portal. More information on options to cancel jobs can be seen using man scancel .","title":"Cancelling Jobs"},{"location":"cheaha/slurm/job_management/#reviewing-past-jobs","text":"If you are planning a new set of jobs and are estimating resource requests, it is useful to review similar jobs that have already completed. To list past jobs for a researcher, use the sacct command. Tip To minimize queue wait times and make best use of resources, please review job efficiency using seff . See our Job Efficiency page for more information.","title":"Reviewing Past Jobs"},{"location":"cheaha/slurm/job_management/#review-with-job-id","text":"The basic form is to use -j along with a job ID to list information about that job. sacct -j [ jobid ] This command will output basic information such as the ID, Name, Partition, Allocated CPUs, and State for the given job ID. Jobs can have matching extern and/or batch job entries as well. These are not especially helpful for most researchers. You can remove these entries using: sacct -j [ jobid ] | grep -wv -e extern -e batch","title":"Review With Job ID"},{"location":"cheaha/slurm/job_management/#review-jobs-submitted-between-specific-timepoints","text":"If you do not remember the job ID, you can use the -S and -E flags to retrieve jobs submitted between the given start datetime and end datetime. Valid start/end time formats are: HH:MM[:SS] [AM|PM] MMDD[YY] or MM/DD[/YY] or MM.DD[.YY] MM/DD[/YY]-HH:MM[:SS] YYYY-MM-DD[THH:MM[:SS]] Notes: Anything in [] is optional Times can be specified in either 12-hour with AM/PM or 24-hour For the last specification, the T itself is inserted, it is not replaced with any value. For example, requesting jobs starting after 12:30 PM on October 5, 2021, the form would be 2021-10-05T12:30 . For example, to retrieve jobs submitted during the month of July 2021, the command could be: sacct -S 070121 -E 073121 sacct -S 07 /01/21 -E 07 /31/21 sacct -S 2021 -07-01 -E 2021 -07-31","title":"Review Jobs Submitted Between Specific Timepoints"},{"location":"cheaha/slurm/job_management/#customizing-the-output","text":"You can add -o with a list of output fields to customize the information you see. sacct -j [ jobid ] -o jobid,start,end,state,alloccpu,reqmem This command will output the job ID, the start time, end time, the state, the number of allocated CPUs, and the requested memory for the specified job. All potential output fields can be seen using sacct --helpformat . Their descriptions can be found on the sacct documentation under Job Accounting Fields.","title":"Customizing the Output"},{"location":"cheaha/slurm/submitting_jobs/","text":"Submitting Jobs with Slurm \u00b6 Slurm is simple to use to submit batch jobs. Scripts should be written in an available shell language on Cheaha, typically bash, and should include the appropriate Slurm directives at the top of the script telling the scheduler the requested resources. Common Slurm directives can be seen below along with simple examples for both single batch jobs and array batch jobs. Tip Please see our page on Job Efficiency for more information on making the best use of cluster resources and minimizing queue wait times. Common Slurm Terminology \u00b6 Node: A self-contained computing devices, forming the basic unit of the cluster. A node has multiple CPUs, memory, and some have GPUs. Jobs requiring multiple nodes must use a protocol such as MPI to communicate between them. Login nodes: Gateway for reseacher access to computing resources, shared among all users. DO NOT run research computation tasks on the login node. Compute nodes: Dedicated nodes for running research computation tasks. Core: A single unit of computational processing, not to be confused with a CPU, which may have many cores. Partition: A logical subset of nodes sharing computational features. Different partitions have different resource limits, priorities, and hardware. Job: A collection of commands that require computational resources to perform. Can be interactive with srun or submitted to the scheduler with srun or sbatch . Batch Job: An array of jobs which all have the same plan for execution, but may vary in terms of input and output. Only available in non-interactive batch mode via sbatch Job ID: The unique number representing the job, returned by srun and sbatch . Stored in $SLURM_JOB_ID within a job. Job Index Number: For array jobs, the index of the currently running job within the array. Stored in $SLURM_ARRAY_TASK_ID within a job. Slurm Flags \u00b6 Slurm has many flags a researcher can use when creating a job, but a short list of the most important ones are listed here. It is highly recommended to explicitly use all of these flags in every job you submit. --job-name : The name of your job to be stored in job accounting records and visible in squeue . --nodes : The number of nodes a job needs. If your job does not require MPI, set this to 1 . --ntasks : The number of tasks you plan to execute on each node, used mostly for bookkeeping and computing total cpus for each node. If unsure, set to 1 . --cpus-per-task : The number of cores to request for each task. Total cpus fore each node equals ntasks times cpus-per-task . --partition : The partition to submit the job to. Partition details can be seen below. --time : Amount of time the job is estimated to run for. Acceptable time formats include mm , mm:ss , hh:mm:ss , d-hh , d-hh:mm and d-hh:mm:ss . --mem : Amount of RAM in MB needed per node. Can specify 16 GB with either 16000 or 16G. --output : Path to a file storing the text output of the job commands. --error : Path to an output file if the script errors. --array : A comma-separated list of array tasks to run. We will explain in more detail further down. For batch jobs, directives are typically included as comments at the top of the script. See examples below. All batch jobs should be submitted using the sbatch command. All flags and more information on how to submit jobs can be seen using man sbatch . For a complete list, see Slurm sbatch Documentation . The --output and --error flags can use other information as part of the name: %j is the Job ID, equal to $SLURM_JOB_ID . %A is the main Array Job ID, equal to $SLURM_ARRAY_JOB_ID . %a is the Array job index number, equal to $SLURM_ARRAY_TASK_ID . %x is the --job-name , equal to $SLURM_JOB_NAME . Available Partitions for --partition \u00b6 Please see Partitions page for more information. Remember, the smaller your resource request, the sooner your job will get through the queue. A Batch Job with sbatch \u00b6 Below is an example batch job script. To test it, copy and paste it into a plain text file testjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testjob.sh . Momentarily, two text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%j.out #SBATCH --error=%x_%j.err echo \"Hello World\" echo \"Hello Error\" 1 > & 2 There is a lot going on in the above script, so let's break it down. There are three main chunks of this script: Line 1 is the interpreter directive: #!/bin/bash . This tells the shell what application to use to execute this script. All sbatch scripts on Cheaha should start with this line. Lines 3-11 are the sbatch flags which tell the scheduler what resources you need and how to manage your job. Line 3: The job name is test . Lines 4-7: The job will have 1 node, with 1 core and 1 GB of memory. Line 8: The job will be on the express partition. Line 9: The job will be no longer than 10 minutes, and will be terminated if it runs over. Line 10: Any standard output ( stdout ) will be written to the file test_$SLURM_JOB_ID.out in the same directory as the script, whatever the $SLURM_JOB_ID happens to be when the job is submitted. The name comes from %x equal to test , the --job-name , and %j equal to the Job ID. Line 11: Any error output ( stderr ) will be written to a different file test_$SLURM_JOB_ID.err in the same directory. Lines 13 and 14 are the payload, or tasks to be run. They will be executed in order from top to bottom just like any shell script. In this case, it is simply writing \"Hello World\" to the --output file and \"Hello Error\" to the --error file. The 1>&2 Means redirect a copy ( >& ) of stdout to stderr . An Array Job with sbatch \u00b6 Building on the job script above, below is an array job. Array jobs are useful when you need to perform the same analysis on slightly different inputs with no interaction between those analyses. We call this situation \"pleasingly parallel\". We can take advantage of an array job using the variable $SLURM_ARRAY_TASK_ID , which will have an integer in the set of values we give to the --array flag. To test the script below, copy and paste it into a plain text file testarrayjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testarrayjob.sh . Momentarily, 16 text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%A_%a.out #SBATCH --error=%x_%A_%a.err #SBATCH --array=0-5,7,9 echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID This script is very similar to the one above, but will submit 8 jobs to the scheduler that all do slightly different things. Each of the 8 jobs will have the same amount and type of resources allocated, and can run in parallel. The 8 jobs come from --array=0-7 . The output of each job will be one of the numbers in the set {0, 1, 2, 3, 4, 5, 6, 7} , depending on which job is running. The output files will look like test_$(SLURM_ARRAY_JOB_ID)_$(SLURM_ARRAY_TASK_ID).out or .err . The value of $(SLURM_ARRAY_JOB_ID) is the main Job ID given to the entire array submission. Scripts can be written to take advantage of the $SLURM_ARRAY_TASK_ID variable indexing variable. For example, a project could have a list of participants that should be processed in the same way, and the analysis script uses the array task ID as an index to pull out one entry from that list for each job. Many common programming languages can interact with shell variables like $SLURM_ARRAY_TASK_ID , or the values can be passed to a program as an argument. You can override the --array flag stored in the script when you call sbatch . To do so, pass another --array flag along with the script name like below. This allows you to rerun only subsets of your array script. # submit jobs with index 0, 3, and 7 sbatch --array = 0 ,3,7 array.sh # submit jobs with index 0, 2, 4, and 6 sbatch --array = 0 -6:2 array.sh For more details on using sbatch please see the official documentation . Note If you are using bash or shell arrays, it is crucial to note they use 0-based indexing. Plan your --array flag indices accordingly. Interactive Jobs with srun at the Terminal \u00b6 To interact with a job at the terminal, use the following srun command with the --pty /bin/bash flag. The other flags should be substituted in place of $FLAGS . srun $FLAGS --pty /bin/bash For more details on using srun please see the official documentation . Graphical Interactive Jobs \u00b6 It is highly recommended to use the Open OnDemand web portal for interactive apps . Interactive sessions for certain software such as MATLAB and RStudio can be created directly from the browser while an HPC Desktop is available to access all of the other software on Cheaha. A terminal is also available through Open OnDemand. It is possible to use other remote desktop software, such as VNC, to start and interact with jobs. These methods are not officially supported and we do not have the capacity to help with remote desktop connections. Instead, please consider switching your workflow to use the Open OnDemand HPC Desktop . If you are unable to use this method, please contact Support . Estimating Compute Resources \u00b6 Being able to estimate how many resources a job will need is critical. Requesting many more resources than necessary bottlenecks the cluster by reserving unused resources for an inefficient job preventing other jobs from using them. However, requesting too few resources will slow down the job or cause it to error. Questions to ask yourself when requesting job resources: Can my scripts take advantage of multiple CPUs? For instance, RStudio generally works on a single thread. Requesting more than 1 CPU here would not improve performance. How large is the data I'm working with? Do my pipelines keep large amounts of data in memory? How long should my job take? For example, do not request 50 hours time for a 15 hour process. Have a reasonable buffer included to account for unexpected processing delays, but do not request the maximum time on a partition if that's unnecessary. Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. To get the most out of your Cheaha experience and ensure your jobs get through the queue as fast as possible, please read about Job Efficiency .","title":"Submitting Jobs"},{"location":"cheaha/slurm/submitting_jobs/#submitting-jobs-with-slurm","text":"Slurm is simple to use to submit batch jobs. Scripts should be written in an available shell language on Cheaha, typically bash, and should include the appropriate Slurm directives at the top of the script telling the scheduler the requested resources. Common Slurm directives can be seen below along with simple examples for both single batch jobs and array batch jobs. Tip Please see our page on Job Efficiency for more information on making the best use of cluster resources and minimizing queue wait times.","title":"Submitting Jobs with Slurm"},{"location":"cheaha/slurm/submitting_jobs/#common-slurm-terminology","text":"Node: A self-contained computing devices, forming the basic unit of the cluster. A node has multiple CPUs, memory, and some have GPUs. Jobs requiring multiple nodes must use a protocol such as MPI to communicate between them. Login nodes: Gateway for reseacher access to computing resources, shared among all users. DO NOT run research computation tasks on the login node. Compute nodes: Dedicated nodes for running research computation tasks. Core: A single unit of computational processing, not to be confused with a CPU, which may have many cores. Partition: A logical subset of nodes sharing computational features. Different partitions have different resource limits, priorities, and hardware. Job: A collection of commands that require computational resources to perform. Can be interactive with srun or submitted to the scheduler with srun or sbatch . Batch Job: An array of jobs which all have the same plan for execution, but may vary in terms of input and output. Only available in non-interactive batch mode via sbatch Job ID: The unique number representing the job, returned by srun and sbatch . Stored in $SLURM_JOB_ID within a job. Job Index Number: For array jobs, the index of the currently running job within the array. Stored in $SLURM_ARRAY_TASK_ID within a job.","title":"Common Slurm Terminology"},{"location":"cheaha/slurm/submitting_jobs/#slurm-flags","text":"Slurm has many flags a researcher can use when creating a job, but a short list of the most important ones are listed here. It is highly recommended to explicitly use all of these flags in every job you submit. --job-name : The name of your job to be stored in job accounting records and visible in squeue . --nodes : The number of nodes a job needs. If your job does not require MPI, set this to 1 . --ntasks : The number of tasks you plan to execute on each node, used mostly for bookkeeping and computing total cpus for each node. If unsure, set to 1 . --cpus-per-task : The number of cores to request for each task. Total cpus fore each node equals ntasks times cpus-per-task . --partition : The partition to submit the job to. Partition details can be seen below. --time : Amount of time the job is estimated to run for. Acceptable time formats include mm , mm:ss , hh:mm:ss , d-hh , d-hh:mm and d-hh:mm:ss . --mem : Amount of RAM in MB needed per node. Can specify 16 GB with either 16000 or 16G. --output : Path to a file storing the text output of the job commands. --error : Path to an output file if the script errors. --array : A comma-separated list of array tasks to run. We will explain in more detail further down. For batch jobs, directives are typically included as comments at the top of the script. See examples below. All batch jobs should be submitted using the sbatch command. All flags and more information on how to submit jobs can be seen using man sbatch . For a complete list, see Slurm sbatch Documentation . The --output and --error flags can use other information as part of the name: %j is the Job ID, equal to $SLURM_JOB_ID . %A is the main Array Job ID, equal to $SLURM_ARRAY_JOB_ID . %a is the Array job index number, equal to $SLURM_ARRAY_TASK_ID . %x is the --job-name , equal to $SLURM_JOB_NAME .","title":"Slurm Flags"},{"location":"cheaha/slurm/submitting_jobs/#available-partitions-for-partition","text":"Please see Partitions page for more information. Remember, the smaller your resource request, the sooner your job will get through the queue.","title":"Available Partitions for --partition"},{"location":"cheaha/slurm/submitting_jobs/#a-batch-job-with-sbatch","text":"Below is an example batch job script. To test it, copy and paste it into a plain text file testjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testjob.sh . Momentarily, two text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%j.out #SBATCH --error=%x_%j.err echo \"Hello World\" echo \"Hello Error\" 1 > & 2 There is a lot going on in the above script, so let's break it down. There are three main chunks of this script: Line 1 is the interpreter directive: #!/bin/bash . This tells the shell what application to use to execute this script. All sbatch scripts on Cheaha should start with this line. Lines 3-11 are the sbatch flags which tell the scheduler what resources you need and how to manage your job. Line 3: The job name is test . Lines 4-7: The job will have 1 node, with 1 core and 1 GB of memory. Line 8: The job will be on the express partition. Line 9: The job will be no longer than 10 minutes, and will be terminated if it runs over. Line 10: Any standard output ( stdout ) will be written to the file test_$SLURM_JOB_ID.out in the same directory as the script, whatever the $SLURM_JOB_ID happens to be when the job is submitted. The name comes from %x equal to test , the --job-name , and %j equal to the Job ID. Line 11: Any error output ( stderr ) will be written to a different file test_$SLURM_JOB_ID.err in the same directory. Lines 13 and 14 are the payload, or tasks to be run. They will be executed in order from top to bottom just like any shell script. In this case, it is simply writing \"Hello World\" to the --output file and \"Hello Error\" to the --error file. The 1>&2 Means redirect a copy ( >& ) of stdout to stderr .","title":"A Batch Job with sbatch"},{"location":"cheaha/slurm/submitting_jobs/#an-array-job-with-sbatch","text":"Building on the job script above, below is an array job. Array jobs are useful when you need to perform the same analysis on slightly different inputs with no interaction between those analyses. We call this situation \"pleasingly parallel\". We can take advantage of an array job using the variable $SLURM_ARRAY_TASK_ID , which will have an integer in the set of values we give to the --array flag. To test the script below, copy and paste it into a plain text file testarrayjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testarrayjob.sh . Momentarily, 16 text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%A_%a.out #SBATCH --error=%x_%A_%a.err #SBATCH --array=0-5,7,9 echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID This script is very similar to the one above, but will submit 8 jobs to the scheduler that all do slightly different things. Each of the 8 jobs will have the same amount and type of resources allocated, and can run in parallel. The 8 jobs come from --array=0-7 . The output of each job will be one of the numbers in the set {0, 1, 2, 3, 4, 5, 6, 7} , depending on which job is running. The output files will look like test_$(SLURM_ARRAY_JOB_ID)_$(SLURM_ARRAY_TASK_ID).out or .err . The value of $(SLURM_ARRAY_JOB_ID) is the main Job ID given to the entire array submission. Scripts can be written to take advantage of the $SLURM_ARRAY_TASK_ID variable indexing variable. For example, a project could have a list of participants that should be processed in the same way, and the analysis script uses the array task ID as an index to pull out one entry from that list for each job. Many common programming languages can interact with shell variables like $SLURM_ARRAY_TASK_ID , or the values can be passed to a program as an argument. You can override the --array flag stored in the script when you call sbatch . To do so, pass another --array flag along with the script name like below. This allows you to rerun only subsets of your array script. # submit jobs with index 0, 3, and 7 sbatch --array = 0 ,3,7 array.sh # submit jobs with index 0, 2, 4, and 6 sbatch --array = 0 -6:2 array.sh For more details on using sbatch please see the official documentation . Note If you are using bash or shell arrays, it is crucial to note they use 0-based indexing. Plan your --array flag indices accordingly.","title":"An Array Job with sbatch"},{"location":"cheaha/slurm/submitting_jobs/#interactive-jobs-with-srun-at-the-terminal","text":"To interact with a job at the terminal, use the following srun command with the --pty /bin/bash flag. The other flags should be substituted in place of $FLAGS . srun $FLAGS --pty /bin/bash For more details on using srun please see the official documentation .","title":"Interactive Jobs with srun at the Terminal"},{"location":"cheaha/slurm/submitting_jobs/#graphical-interactive-jobs","text":"It is highly recommended to use the Open OnDemand web portal for interactive apps . Interactive sessions for certain software such as MATLAB and RStudio can be created directly from the browser while an HPC Desktop is available to access all of the other software on Cheaha. A terminal is also available through Open OnDemand. It is possible to use other remote desktop software, such as VNC, to start and interact with jobs. These methods are not officially supported and we do not have the capacity to help with remote desktop connections. Instead, please consider switching your workflow to use the Open OnDemand HPC Desktop . If you are unable to use this method, please contact Support .","title":"Graphical Interactive Jobs"},{"location":"cheaha/slurm/submitting_jobs/#estimating-compute-resources","text":"Being able to estimate how many resources a job will need is critical. Requesting many more resources than necessary bottlenecks the cluster by reserving unused resources for an inefficient job preventing other jobs from using them. However, requesting too few resources will slow down the job or cause it to error. Questions to ask yourself when requesting job resources: Can my scripts take advantage of multiple CPUs? For instance, RStudio generally works on a single thread. Requesting more than 1 CPU here would not improve performance. How large is the data I'm working with? Do my pipelines keep large amounts of data in memory? How long should my job take? For example, do not request 50 hours time for a 15 hour process. Have a reasonable buffer included to account for unexpected processing delays, but do not request the maximum time on a partition if that's unnecessary. Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. To get the most out of your Cheaha experience and ensure your jobs get through the queue as fast as possible, please read about Job Efficiency .","title":"Estimating Compute Resources"},{"location":"cheaha/software/modules/","text":"Modules and Applications \u00b6 Most software available on Cheaha is installed as modules, managed by the Lmod system. This document will provide a basic rundown of using Lmod commands to customize a software environment. module is the main command used to interface with module files in Lmod. Listing and Searching Modules \u00b6 To begin, all module commands are run from the terminal. To know what software is installed on Cheaha, use the avail command. module avail If you need to know what software is already loaded in your environment, run: module list If there is specific software you want to search for, you can use the spider subcommand, and provide a string or regular expression to match against. All modules containing the string (case-insensitive) or matching the regular expression will be returned along with their installed versions. # list modules containing string module spider <string> # list modules matching a regular expression module -r spider <regex> Loading Modules \u00b6 To load modules, run: module load module1 module2 ... Note If you only specify a module name without an accompanying version tag, the most recently installed version will be loaded into the workspace. If your scripts depend on specific versions of software being used, explicitly load the module version you need. To unload packages, run: module unload package1 package2 ... If you want to revert to the default modules, you can use: module reset Licensed and Commercial Software Restrictions \u00b6 The following software have license restrictions that may preclude some researchers or collaborators depending on their departmental or group affiliations. In the table, \"affiliated\" means employed by, or a student of, unless otherwise noted. External collaborators are not considered affiliated with UAB for the purposes of software licensing and access unless otherwise noted. These software packages may be commercial paid software. If you believe you should have access to software that you do not have access to, please contact Support . Software Restrictions License Holder Ansys School of Engineering affiliated School of Engineering Gurobi One individual user per license Individuals LS-Dyna School of Engineering affiliated School of Engineering MATLAB UAB affiliated UAB Campus Parabricks 2 GPUs for researchers using RC systems Research Computing SAS UAB affiliated UAB Campus SPSS UAB affiliated UAB Campus Use of these software packages without authorization may be a violation of the UAB IT Acceptable Use Policy . Security Issues \u00b6 IGV \u00b6 Danger Versions of IGV prior to 2.11.9 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of IGV >= 2.11.9 . GSEA \u00b6 Danger Versions of GSEA prior to 4.2.3 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of GSEA >= 4.2.3 .","title":"Preinstalled Modules"},{"location":"cheaha/software/modules/#modules-and-applications","text":"Most software available on Cheaha is installed as modules, managed by the Lmod system. This document will provide a basic rundown of using Lmod commands to customize a software environment. module is the main command used to interface with module files in Lmod.","title":"Modules and Applications"},{"location":"cheaha/software/modules/#listing-and-searching-modules","text":"To begin, all module commands are run from the terminal. To know what software is installed on Cheaha, use the avail command. module avail If you need to know what software is already loaded in your environment, run: module list If there is specific software you want to search for, you can use the spider subcommand, and provide a string or regular expression to match against. All modules containing the string (case-insensitive) or matching the regular expression will be returned along with their installed versions. # list modules containing string module spider <string> # list modules matching a regular expression module -r spider <regex>","title":"Listing and Searching Modules"},{"location":"cheaha/software/modules/#loading-modules","text":"To load modules, run: module load module1 module2 ... Note If you only specify a module name without an accompanying version tag, the most recently installed version will be loaded into the workspace. If your scripts depend on specific versions of software being used, explicitly load the module version you need. To unload packages, run: module unload package1 package2 ... If you want to revert to the default modules, you can use: module reset","title":"Loading Modules"},{"location":"cheaha/software/modules/#licensed-and-commercial-software-restrictions","text":"The following software have license restrictions that may preclude some researchers or collaborators depending on their departmental or group affiliations. In the table, \"affiliated\" means employed by, or a student of, unless otherwise noted. External collaborators are not considered affiliated with UAB for the purposes of software licensing and access unless otherwise noted. These software packages may be commercial paid software. If you believe you should have access to software that you do not have access to, please contact Support . Software Restrictions License Holder Ansys School of Engineering affiliated School of Engineering Gurobi One individual user per license Individuals LS-Dyna School of Engineering affiliated School of Engineering MATLAB UAB affiliated UAB Campus Parabricks 2 GPUs for researchers using RC systems Research Computing SAS UAB affiliated UAB Campus SPSS UAB affiliated UAB Campus Use of these software packages without authorization may be a violation of the UAB IT Acceptable Use Policy .","title":"Licensed and Commercial Software Restrictions"},{"location":"cheaha/software/modules/#security-issues","text":"","title":"Security Issues"},{"location":"cheaha/software/modules/#igv","text":"Danger Versions of IGV prior to 2.11.9 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of IGV >= 2.11.9 .","title":"IGV"},{"location":"cheaha/software/modules/#gsea","text":"Danger Versions of GSEA prior to 4.2.3 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of GSEA >= 4.2.3 .","title":"GSEA"},{"location":"cheaha/software/software/","text":"Software Installation \u00b6 Construction This page is a stub and is under construction. Open On Demand Sandbox App \u00b6","title":"Self-Installed Software"},{"location":"cheaha/software/software/#software-installation","text":"Construction This page is a stub and is under construction.","title":"Software Installation"},{"location":"cheaha/software/software/#open-on-demand-sandbox-app","text":"","title":"Open On Demand Sandbox App"},{"location":"data_management/code_storage/","text":"Code Storage \u00b6 Construction This page is a stub and is under construction. UAB GitLab vs GitHub: which should I use? \u00b6 GitHub \u00b6 Please visit https://github.com . For UAB Research Computing's GitHub repositories, please see our Social Media page. GitLab \u00b6 UAB Research Computing maintains a GitLab instance. To gain access, please visit https://gitlab.rc.uab.edu and create an Account .","title":"Code Storage"},{"location":"data_management/code_storage/#code-storage","text":"Construction This page is a stub and is under construction.","title":"Code Storage"},{"location":"data_management/code_storage/#uab-gitlab-vs-github-which-should-i-use","text":"","title":"UAB GitLab vs GitHub: which should I use?"},{"location":"data_management/code_storage/#github","text":"Please visit https://github.com . For UAB Research Computing's GitHub repositories, please see our Social Media page.","title":"GitHub"},{"location":"data_management/code_storage/#gitlab","text":"UAB Research Computing maintains a GitLab instance. To gain access, please visit https://gitlab.rc.uab.edu and create an Account .","title":"GitLab"},{"location":"data_management/storage/","text":"Storage \u00b6 Warning The information on this page is under construction and some of it may be obsolete. If you need additional clarifications in the meantime, please contact us . Privacy \u00b6 Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s). No Automatic Backups \u00b6 There is no automatic back up of any user data on the cluster in home, data, or scratch. At this time, all user data back up processes are defined and managed by each user and/or lab. Given that data backup demands vary widely between different users, groups, and research domains, this approach enables those who are most familiar with the data to make appropriate decisions based on their specific needs. For example, if a group is working with a large shared data set that is a local copy of a data set maintained authoritatively at a national data bank, maintaining a local backup is unlikely to be a productive use of limited storage resources, since this data could potentially be restored from the authoritative source. If, however, you are maintaining a unique source of data of which yours is the only copy, then maintaining a backup is critical if you value that data set. It's worth noting that while this \"uniqueness\" criteria may not apply to the data you analyze, it may readily apply to the codes that define your analysis pipelines. An often recommended backup policy is the 3-2-1 rule: maintain three copies of data, on two different media, with one copy off-site. You can read more about the 3-2-1 rule here . In the case of your application codes, using revision control tools during development provides an easy way to maintain a second copy, makes for a good software development process, and can help achieve reproducible research goals. Please review the data storage options provided by UAB IT for maintaining copies of your data. In choosing among these options, you should also be aware of UAB's data classification rules and requirements for security requirements for sensitive and restricted data storage. Given the importance of backup, Research Computing continues to explore options to facilitate data backup workflows from the cluster. Please contact us if you have questions or would like to discuss specific data backup scenarios. Directories on Cheaha \u00b6 Users are provided a high performance GPFS file system to store data, toolboxes, and other supporting files. The specific directories a user can access are described below. In these descriptions, the \"$USER\" variable should be replaced with the user's account name string. Home Directory \u00b6 Each user has a personal directory found at /home/$USER (or $HOME ). This is traditionally meant to store scripts and supporting files and toolboxes such as those relating to Anaconda virtual environments or R packages. The owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. A user is limited to 5 TB of data across both their home directory and their user data directory (see below). User Data Directory \u00b6 Each user has another directory found at /data/user/$USER (or $USER_DATA ) that can store datasets and results for a user's projects. The owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. A user is limited to 5 TB of data across both their home directory (see above)and their user data directory. Note The home and user data directories are mirrored across storage locations to allow for emergency backup in case some of the drives fail. This is not meant to be a long-term backup solution as any data deleted by a user is deleted on the main drive and the mirrored drive. Project Directory \u00b6 Shared data can be stored in a /data/project/<project_name> directory. The default storage size for a new project is 50TB. If you need less than 5TB or your need for shared space is short term, please request a Sloss space instead. Project storage can be helpful for teams of researchers who need access to the same data. All project spaces must be owned by a principal investigator (PI) who is an employee of UAB with a legitimate research interest. The PI takes responsibility for data in the space, as well as access control of all files and directories under the parent directory. As with all data on Cheaha, backups and archival services are not provided, and are the responsibility of the respective data owners. The PI and all members with access to the project directory can read, write/delete, and list files within the top-level directory, and all other subdirectories by default. Other people on the system have no ability to access the project space. Access control for directories and files within the project space can be implemented via access control lists. Please see the bash commands setfacl and getfacl ) for more information. Access control within the project directory are the responsibility of the project owner. However, we respect that access control lists can be tricky, so please feel free to contact us for assistance. To create a project directory, or change access to or ownership of a project directory, the PI should follow the instructions at How Do I Request Or Change A Project Space? Sloss \u00b6 A special location under /data/project/sloss to store projects that are at most 5 TB. In keeping with the name Sloss , these spaces are intended as a foundry for experimental or temporary project spaces that have potential to grow. Otherwise, they are treated like any other project space. Scratch \u00b6 Two types of scratch space are provided for analyses currently being ran, network-mounted and local. These are spaces shared across users (though one user still cannot access another user's files without permission) and as such, data should be moved out of scratch when the analysis is finished. Note Scratch space (network and local) is not backed up . Network Scratch \u00b6 All users have access to a large, temporary, work-in-progress directory for storing data, called a scratch directory in /data/scratch/$USER or $USER_SCRATCH . Use this directory to store very large datasets for a short period of time and to run your jobs. The maximum amount of data a single user can store in network scratch is 100 TB at once. Network scratch is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 1 PB of network scratch storage. If using scratch, this should be your jobs' primary working directory, unless the job would benefit from local scratch (see below). Warning Research Computing expects each user to keep their scratch areas clean. The cluster scratch areas are not to be used for archiving data. In order to keep scratch clear and usable for everyone, files older than 28 days will be automatically deleted. Local Scratch \u00b6 Each compute node has a local scratch directory that is accessible via the variable $LOCAL_SCRATCH . If your job performs a lot of file I/O, the job should use $LOCAL_SCRATCH rather than $USER_SCRATCH to prevent bogging down the network scratch file system. It's important to recognize most jobs run on the cluster do not fall under this category. The amount of scratch space available is approximately 800GB. The $LOCAL_SCRATCH is a special temporary directory and it's important to note that this directory is deleted when the job completes, so the job script has to move the results to $USER_SCRATCH or other location prior to the job exiting. Note that $LOCAL_SCRATCH is only useful for jobs in which all processes run on the same compute node, so MPI jobs are not candidates for this solution. Use the #SBATCH --nodes=1 slurm directive to specify that all requested cores are on the same node. The following is an array job example that uses $LOCAL_SCRATCH by transferring the inputs into $LOCAL_SCRATCH at the beginning of the script and the result out of $LOCAL_SCRATCH at the end of the script. #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 #SBATCH --nodes=1 # # Tell the scheduler only need 10 minutes and the appropriate partition # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 echo \"TMPDIR: $LOCAL_SCRATCH \" cd $LOCAL_SCRATCH # Create a working directory under the special scheduler local scratch directory # using the array job's taskID mdkir $SLURM_ARRAY_TASK_ID cd $SLURM_ARRAY_TASK_ID # Next copy the input data to the local scratch echo \"Copying input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) # The input data in this case has a numerical file extension that # matches $SLURM_ARRAY_TASK_ID cp -a $USER_SCRATCH /GeneData/INP*. $SLURM_ARRAY_TASK_ID ./ echo \" copied input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) someapp -S 1 -D 10 -i INP*. $SLURM_ARRAY_TASK_ID -o geneapp.out. $SLURM_ARRAY_TASK_ID # Lastly copy the results back to network scratch echo \"Copying results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) cp -a geneapp.out. $SLURM_ARRAY_TASK_ID $USER_SCRATCH /GeneData/ echo \" Copied results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) Directory Permissions \u00b6 Default file permissions are described for each directory above. Additional background on Linux file system permissions can be found here: https://its.unc.edu/research-computing/techdocs/how-to-use-unix-and-linux-file-permissions/ https://www.rc.fas.harvard.edu/resources/documentation/linux/unix-permissions/ https://hpc.nih.gov/storage/permissions.html","title":"Overview"},{"location":"data_management/storage/#storage","text":"Warning The information on this page is under construction and some of it may be obsolete. If you need additional clarifications in the meantime, please contact us .","title":"Storage"},{"location":"data_management/storage/#privacy","text":"Do not store sensitive information on this filesystem. It is not encrypted. Note that your data will be stored on the cluster filesystem, and while not accessible to ordinary users, it could be accessible to the cluster administrator(s).","title":"Privacy"},{"location":"data_management/storage/#no-automatic-backups","text":"There is no automatic back up of any user data on the cluster in home, data, or scratch. At this time, all user data back up processes are defined and managed by each user and/or lab. Given that data backup demands vary widely between different users, groups, and research domains, this approach enables those who are most familiar with the data to make appropriate decisions based on their specific needs. For example, if a group is working with a large shared data set that is a local copy of a data set maintained authoritatively at a national data bank, maintaining a local backup is unlikely to be a productive use of limited storage resources, since this data could potentially be restored from the authoritative source. If, however, you are maintaining a unique source of data of which yours is the only copy, then maintaining a backup is critical if you value that data set. It's worth noting that while this \"uniqueness\" criteria may not apply to the data you analyze, it may readily apply to the codes that define your analysis pipelines. An often recommended backup policy is the 3-2-1 rule: maintain three copies of data, on two different media, with one copy off-site. You can read more about the 3-2-1 rule here . In the case of your application codes, using revision control tools during development provides an easy way to maintain a second copy, makes for a good software development process, and can help achieve reproducible research goals. Please review the data storage options provided by UAB IT for maintaining copies of your data. In choosing among these options, you should also be aware of UAB's data classification rules and requirements for security requirements for sensitive and restricted data storage. Given the importance of backup, Research Computing continues to explore options to facilitate data backup workflows from the cluster. Please contact us if you have questions or would like to discuss specific data backup scenarios.","title":"No Automatic Backups"},{"location":"data_management/storage/#directories-on-cheaha","text":"Users are provided a high performance GPFS file system to store data, toolboxes, and other supporting files. The specific directories a user can access are described below. In these descriptions, the \"$USER\" variable should be replaced with the user's account name string.","title":"Directories on Cheaha"},{"location":"data_management/storage/#home-directory","text":"Each user has a personal directory found at /home/$USER (or $HOME ). This is traditionally meant to store scripts and supporting files and toolboxes such as those relating to Anaconda virtual environments or R packages. The owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. A user is limited to 5 TB of data across both their home directory and their user data directory (see below).","title":"Home Directory"},{"location":"data_management/storage/#user-data-directory","text":"Each user has another directory found at /data/user/$USER (or $USER_DATA ) that can store datasets and results for a user's projects. The owner ($USER) of the directory can read, write/delete, and list files. No other users or groups have permissions to this directory. A user is limited to 5 TB of data across both their home directory (see above)and their user data directory. Note The home and user data directories are mirrored across storage locations to allow for emergency backup in case some of the drives fail. This is not meant to be a long-term backup solution as any data deleted by a user is deleted on the main drive and the mirrored drive.","title":"User Data Directory"},{"location":"data_management/storage/#project-directory","text":"Shared data can be stored in a /data/project/<project_name> directory. The default storage size for a new project is 50TB. If you need less than 5TB or your need for shared space is short term, please request a Sloss space instead. Project storage can be helpful for teams of researchers who need access to the same data. All project spaces must be owned by a principal investigator (PI) who is an employee of UAB with a legitimate research interest. The PI takes responsibility for data in the space, as well as access control of all files and directories under the parent directory. As with all data on Cheaha, backups and archival services are not provided, and are the responsibility of the respective data owners. The PI and all members with access to the project directory can read, write/delete, and list files within the top-level directory, and all other subdirectories by default. Other people on the system have no ability to access the project space. Access control for directories and files within the project space can be implemented via access control lists. Please see the bash commands setfacl and getfacl ) for more information. Access control within the project directory are the responsibility of the project owner. However, we respect that access control lists can be tricky, so please feel free to contact us for assistance. To create a project directory, or change access to or ownership of a project directory, the PI should follow the instructions at How Do I Request Or Change A Project Space?","title":"Project Directory"},{"location":"data_management/storage/#sloss","text":"A special location under /data/project/sloss to store projects that are at most 5 TB. In keeping with the name Sloss , these spaces are intended as a foundry for experimental or temporary project spaces that have potential to grow. Otherwise, they are treated like any other project space.","title":"Sloss"},{"location":"data_management/storage/#scratch","text":"Two types of scratch space are provided for analyses currently being ran, network-mounted and local. These are spaces shared across users (though one user still cannot access another user's files without permission) and as such, data should be moved out of scratch when the analysis is finished. Note Scratch space (network and local) is not backed up .","title":"Scratch"},{"location":"data_management/storage/#network-scratch","text":"All users have access to a large, temporary, work-in-progress directory for storing data, called a scratch directory in /data/scratch/$USER or $USER_SCRATCH . Use this directory to store very large datasets for a short period of time and to run your jobs. The maximum amount of data a single user can store in network scratch is 100 TB at once. Network scratch is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 1 PB of network scratch storage. If using scratch, this should be your jobs' primary working directory, unless the job would benefit from local scratch (see below). Warning Research Computing expects each user to keep their scratch areas clean. The cluster scratch areas are not to be used for archiving data. In order to keep scratch clear and usable for everyone, files older than 28 days will be automatically deleted.","title":"Network Scratch"},{"location":"data_management/storage/#local-scratch","text":"Each compute node has a local scratch directory that is accessible via the variable $LOCAL_SCRATCH . If your job performs a lot of file I/O, the job should use $LOCAL_SCRATCH rather than $USER_SCRATCH to prevent bogging down the network scratch file system. It's important to recognize most jobs run on the cluster do not fall under this category. The amount of scratch space available is approximately 800GB. The $LOCAL_SCRATCH is a special temporary directory and it's important to note that this directory is deleted when the job completes, so the job script has to move the results to $USER_SCRATCH or other location prior to the job exiting. Note that $LOCAL_SCRATCH is only useful for jobs in which all processes run on the same compute node, so MPI jobs are not candidates for this solution. Use the #SBATCH --nodes=1 slurm directive to specify that all requested cores are on the same node. The following is an array job example that uses $LOCAL_SCRATCH by transferring the inputs into $LOCAL_SCRATCH at the beginning of the script and the result out of $LOCAL_SCRATCH at the end of the script. #!/bin/bash #SBATCH --array=1-10 #SBATCH --share #SBATCH --partition=express # # Name your job to make it easier for you to track # #SBATCH --job-name=R_array_job # # Set your error and output files # #SBATCH --error=R_array_job.err #SBATCH --output=R_array_job.out #SBATCH --ntasks=1 #SBATCH --nodes=1 # # Tell the scheduler only need 10 minutes and the appropriate partition # #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=256 # # Set your email address and request notification when you job is complete or if it fails # #SBATCH --mail-type=FAIL #SBATCH --mail-user=YOUR_EMAIL_ADDRESS module load R/3.2.0-goolf-1.7.20 echo \"TMPDIR: $LOCAL_SCRATCH \" cd $LOCAL_SCRATCH # Create a working directory under the special scheduler local scratch directory # using the array job's taskID mdkir $SLURM_ARRAY_TASK_ID cd $SLURM_ARRAY_TASK_ID # Next copy the input data to the local scratch echo \"Copying input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) # The input data in this case has a numerical file extension that # matches $SLURM_ARRAY_TASK_ID cp -a $USER_SCRATCH /GeneData/INP*. $SLURM_ARRAY_TASK_ID ./ echo \" copied input data from network scratch to $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID - $( date ) someapp -S 1 -D 10 -i INP*. $SLURM_ARRAY_TASK_ID -o geneapp.out. $SLURM_ARRAY_TASK_ID # Lastly copy the results back to network scratch echo \"Copying results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date ) cp -a geneapp.out. $SLURM_ARRAY_TASK_ID $USER_SCRATCH /GeneData/ echo \" Copied results from local $LOCAL_SCRATCH / $SLURM_ARRAY_TASK_ID to network - $( date )","title":"Local Scratch"},{"location":"data_management/storage/#directory-permissions","text":"Default file permissions are described for each directory above. Additional background on Linux file system permissions can be found here: https://its.unc.edu/research-computing/techdocs/how-to-use-unix-and-linux-file-permissions/ https://www.rc.fas.harvard.edu/resources/documentation/linux/unix-permissions/ https://hpc.nih.gov/storage/permissions.html","title":"Directory Permissions"},{"location":"data_management/lts/lts/","text":"Long-Term Storage \u00b6 Important This page is a work in progress. Please be patient as information is added or removed. Thank you. UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments. Important Currently, UAB LTS is only accessible using the UAB Campus Network. If you are off campus and want to access LTS, please use the UAB Campus VPN . If you are accessing LTS through Cheaha, you do not need to use the VPN, even at home. Terminology \u00b6 When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead. In S3, there are no such things as folders and files, just objects. Everything is an object, and there is no standard filesystem where things are physically stored in a heirarchical manner. However, there are certain things that can make it look that way such as prefixes. Prefixes are used in place of a file path to an object, and so can be used to represent a path to an object. This documentation will use the standard file and path terms since that's more easily understood by most users. Just be aware that documentation such as AWS CLI will use terms prefix, object, and others that are not standard in a typical filesystem. Requesting an Account \u00b6 UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to support@listserv.uab.edu . You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS. Windows/Mac \u00b6 To access LTS from Windows and Mac, we suggest using the Cyberduck GUI which is free to download. Once you have it installed and open, Cyberduck will look like this: Creating a Connection \u00b6 First, download the UAB CyberDuck Connection Profile . After it's downloaded, double click the file to open it in Cyberduck. It will open the following connection creation window: Input your Access Key and Secret Access Key sent to you by Research Computing after account creation in their appropriate fields. Once you've entered these keys you can close the connection creation window. This connection with the keys you entered is now saved as a bookmark for easy access in the future. Double click the created bookmark to open the connection to LTS. Creating a Bucket \u00b6 Sets of storage objects are stored in what are called buckets. Buckets are sets of file systems for storing data. You can create different buckets for various purposes, such as separating buckets by dataset or project, or having a single bucket for all data you are moving off of Cheaha project storage. In order to create a bucket, click File > New Folder... and then name the bucket you would like to create. Once the bucket is created, it will appear in the File window. An example could look like: The bucket will have the symbol of a hard disk with an Amazon A brand on it. This is the root of the file system for that bucket. You can then double click into it to open that file system. Important Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. Uploading and Downloading Data \u00b6 Once you're inside the bucket, files can be uploaded easily through dragging and dropping from your local machine into the GUI. You can also use the Upload button in the toolbar to open a file browser and choose what to upload. Downloading files from the bucket can be done by first selecting the file(s)/folder(s) to download and then clicking the Actions button in the toolbar. In that dropdown will be a Download option. You can also get to this dropdown through the File menu or by right-clicking. Note By default, all buckets and files you upload are only available to you.Currently, there is not a known method to add permissions for individual researchers in the Cyberduck interface. If you need to add access to a bucket or set of files within a bucket, please look in the Linux guide below. If you do not have access to a personal Linux machine, Cheaha is available to use for this purpose. Alternative Interfaces \u00b6 In addition to Cyberduck, there are other GUI based programs for interfacing with UAB LTS. S3 Browser is an easy-to-use program for uploading and downloading files. However more sophisticated tools, such as setting file permissions, are hidden behind a paywall. This tool is also only available on Windows platforms. researchers can investigate this tool if desired, however research computing will not provide direct support for this program. Linux/Command Line \u00b6 Linux has very few workable GUIs capable of accessing S3 storage for free available, and so almost all tools for transferring from Cheaha to LTS will be command line interfaces (CLI). The positives for this are that CLIs offer a much broader range of function available to researchers for managing their LTS buckets. There are a few different CLIs available to researchers on Cheaha to use. Current available CLIs on Cheaha are rclone , s3cmd , and the AWS CLI . This documentation will show how to perform each function using all three tools where possible and will give a comparison chart contrasting what each tool is useful for. Both of these are available as modules under the rclone and awscli module names. Note Of note, all of these tools are available for Windows and Mac as well if you are comfortable using command line interfaces on those platforms. There are installation instructions for both of these tools on their respective websites. Configuration \u00b6 In order to access LTS through the command line no matter which CLI you use, you will need to perform some configuration. This will set up the remote connection from your local machine (or you Cheaha profile) to LTS. If you choose to use multiple CLIs, you will need to perform configuration for each one separately. rclone \u00b6 The instructions for setting up a remote connection with rclone can be found in our main rclone documentation . In the following examples, uablts is used as the name of the remote connection. s3cmd \u00b6 s3cmd can be easily installed via an Anaconda environment . Create the environment, activate it, then install using: pip install s3cmd Note Depending on how Anaconda chooses to install the package, the actual s3cmd script may be in your $HOME/.local/bin folder. This folder can be added to your path using PATH=$PATH:$HOME/.local/bin , and you will have access to the s3cmd script after that. Once you have s3cmd downloaded, you can start the configuration process like so: s3cmd --configure [ -c $HOME /profile_name ] You can run the configuration either with or without the [-c] option. If you use it, a file named profile_name will be created in your home directory with your login credentials and other information. If you omit the -c option, a file called $HOME/.s3cfg will be created by default. This can be helpful if you have multiple S3 profiles you are using. If you use UAB LTS as your only S3 storage platform, it's suggested to omit the -c option. Note After configuration, the s3cmd command will default to using the .s3cfg file for credentials if it exists. If you create a separate named profile file, you will need to add that to the s3cmd call each time you run it. During configuration, you will be asked to enter some information. You can follow the example below, inputting your user-specific information where required. Lines requiring user input are highlighted. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key: <access key> Secret Key: <secret key> Default Region [US]: <leave blank> Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [s3.amazonaws.com]: s3.lts.rc.uab.edu Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).s3.lts.rc.uab.edu Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password: <leave blank> Path to GPG program [/usr/bin/gpg]: $HOME/bin/gpg When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer Use HTTPS protocol [Yes]: <leave blank> On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name: <leave blank> New settings: Access Key: <access key> Secret Key: <secret key> Default Region: US S3 Endpoint: s3.lts.rc.uab.edu DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).s3.lts.rc.uab.edu Encryption password: Path to GPG program: $HOME/bin/gpg Use HTTPS protocol: True HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n] Y Please wait, attempting to list all buckets... Success. Your access key and secret key worked fine :-) Now verifying that encryption works... Not configured. Never mind. Save settings? [y/N] y If your test access succeeded, you are now ready to use s3cmd . AWS CLI \u00b6 Setting up a remote connection using AWS CLI is fairly straightforward. After loading the awscli module, run aws configure in the terminal. It will ask you enter four pieces of information, fill them out as so: AWS Access Key [none]: <access key> AWS Secret Access Key [none]: <secret key> Default region name [none]: <Press Enter> Default output format [none]: json Your access key and secret key should have been given to you by research computing when you requested your account. Copy-paste these into the terminal when requested. Important AWS CLI assumes you are using the AWS service for S3 storage. In order to access UAB LTS, for all AWS CLI commands, you will need to add the --endpoint-url https://s3.lts.rc.uab.edu option to the aws function call. General Command Structure \u00b6 RClone : rclone <subcommand> [ options ] <remote>:<bucket> Important For all commands, replace everything in <> with the necessary values. Do not include the <> symbols in your command. To see a list of all subcommands available to rclone, you can use rclone --help . You can also use the --help option with any subcommand to see a detailed explanation of what it does plus any options you may want or need to set when calling it. s3cmd : s3cmd [ -c profile_file ] <command> [ options ] [ -n --dry-run ] As noted previously, the [-c profile_file] is only required if you are NOT using credentials saved in the $HOME/.s3cfg file. Otherwise, you can leave it out. To see a list of commands available, use s3cmd --help . Additionally, if you want to test an action without actually running it (i.e. it prints all actions that would be performed), you can add the -n or --dry-run option. AWS CLI : aws <command> <subcommand> [ options ] The <command> for most commonly used functions will either be s3 or s3api . You can use the help option to view available commands, subcommands, and options for AWS. Additionally, when running basically any AWS CLI command, you can include the --dryrun option to see the exact actions that will be performed without actually performing them. This is useful for things like deleting files and folders to make sure you are not performing an unwanted action. Important If you are wanting to perform actions on a specific directory in S3, it is imperative to add the / at the end of the directory name. For more information on this, see our FAQ Make a Bucket \u00b6 Buckets are essentially the root folder of a filesystem where you are storing your data. You will need to create a bucket before being able to copy data to LTS. Important Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. RClone : rclone mkdir uablts:<bucket> s3cmd : s3cmd mb s3://<bucket> AWS CLI : aws s3api create-bucket --bucket <bucket> --endpoint-url https://s3.lts.rc.uab.edu Listing Buckets and Contents \u00b6 RClone : To list all buckets you have available, use the lsd subcommand with only the remote specified: rclone lsd uablts: To list all contents inside a bucket, use the ls subcommand with the remote and bucket specified. You can also be specific about the path to the directory you want to list. rclone ls uablts:<bucket/path/directory> This outputs all files along with their directory path recursively. So if you only specify the main bucket, it will output every file in the bucket no matter how deep in the directory tree. To only list files and folders in a given directory, you can use the lsf subcommand rclone lsf uablts:<bucket/path/directory/> s3cmd : With s3cmd, you can use the ls command to list both buckets and their contents. s3cmd ls <s3://bucket/path/> You can add the --recursive option to list all files in the given path. By default, it only lists objects or folders at the top level of the path. AWS CLI : aws s3 ls <bucket/path/directory/> --endpoint-url https://s3.lts.rc.uab.edu If you would like to list all objects recursively, you can add the --recursive tag. A couple of other helpful options are --summarize and --human-readable that will give a total number of objects and their size and make the size output more easily readable, respectively. Uploading Files and Folders \u00b6 RClone : Uploading files and folders can be done a couple of ways. The first is by using the copy subcommand. This will add files from the source to the destination. rclone copy <source> uablts:<bucket/destination> The second method is using the sync subcommand. This subcommand makes the destination identical to the source. The -i option can be added to make it interactive, asking you whether to copy or delete each file. rclone sync [ -i ] <source> uablts:<bucket/destination/> Danger Be extremely cautious using sync. If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable. s3cmd : s3cmd disinguishes between moving files between a local source and S3 versus moving files between two S3 locations using 3 different commands. # transfer from local to S3 s3cmd put <source> s3://<bucket/path/destination/> # transfer from S3 to local s3cmd get s3://<bucket/path/source/> <destination> # transfer between two S3 locations s3cmd cp s3://<bucket/path/> s3://<bucket/path/> If you are transferring an entire folder from S3 to either another S3 location or a local destination, you will need to add the --recursive option, otherwise you will get an error. Like rclone and AWS, there is also a sync command here as well. # sync an S3 location to a local source s3cmd sync <source> s3://<bucket/path/destination> # sync a local destination to an S3 location s3cmd sync s3://<bucket/path/source> <destination> AWS CLI : Copying files and directories can be managed using the cp subcommand and has the same behavior as rclone's copy . aws s3 cp <source> s3://<bucket/path/destination> --endpoint-url https://s3.lts.rc.uab.edu [ --recursive ] If you are copying a directory, you will need to add the --recursive option. If you are wanting to copy data down from LTS to your local machine or Cheaha, just reverse the positions of the source and destination in the function call. Like rclone, AWS also has a sync subcommand that performs the same functionality. You can use it like so: aws s3 sync <source> s3://<bucket/path/destination> --endpoint-url https://s3.lts.rc.uab.edu [ --delete ] sync has an added benefit that only files that do not exist in the destination or files that have changed in the source will be transferred whereas cp copies everything no matter if it already exists in the destination. By default, sync DOES NOT cause files in the detination that are not in the source to be deleted like rclone sync does. If you want this functionality, you can add the --delete tag at the end of the function call. Danger Be extremely cautious using --delete . Only use if you are sure any data deleted is not important. Data deleted from LTS is not recoverable. Deleting Files and Directories \u00b6 RClone : File deletion is performed using the delete subcommand. rclone delete uablts:<bucket/path/file> Directory deletion is handled using the purge subcommand. Be very cautious with this, as this deletes all files and subdirectories within the directory as well. rclone purge uablts:<bucket/path/> s3cmd : File and directory deletion is handled by the rm command. s3cmd rm s3://<bucket/path/file> [ --recursive ] If you want to delete a directory, you will need to add the --recursive option. To delete an entire bucket, use the rb command. s3cmd rb s3://<bucket> AWS CLI : The subcommand for deleting files and folders from LTS is rm : aws s3 rm s3://<bucket/path/object> --endpoint-url https://s3.lts.rc.uab.edu [ --recursive ] rm can delete both files and folders. If you are wanting to delete a folder and everything in it, you will need to add the --recursive option. Like with sync be very cautious using rm and make sure you know what you are deleting before you do so. To delete an entire bucket, you will need to use the s3api command paired with the `delete-bucket subcommand. An example of this looks like: aws s3api delete-bucket --bucket <bucket> --endpoint-url https://s3.lts.rc.uab.edu Command Comparison Chart \u00b6 Note For brevity, the chart will exclude the --endpoint-url option from the AWS CLI commands, but it will need to be included if you choose to use that tool. Action rclone s3cmd AWS CLI Make Bucket rclone mkdir uablts:<bucket> s3cmd mb s3://<bucket> aws s3api create-bucket --bucket <bucket> List Buckets rclone lsd uablts: s3cmd ls aws s3 ls List Files rclone lsf uablts:<bucket/path/> s3cmd ls s3://<bucket/path/> aws s3 ls s3://<bucket/path/> Full Upload rclone copy <source> uablts:<bucket/destination> s3cmd put <source> s3://<bucket/destination/> aws s3 cp <source> s3://<bucket/destination> Download rclone copy uablts:<bucket/source/> <destination> s3cmd get s3://<bucket/source/> <destination> aws s3 cp s3://<bucket/source/> <destination> Sync rclone sync [-i] <source> uablts:<bucket/destination/> s3cmd sync <source> s3://<bucket/destination/> aws s3 sync <source> s3://<bucket/destination/> [--delete] Delete File rclone delete uablts:<bucket/path/file> s3cmd rm s3://<bucket/path/file> aws s3 rm s3://<bucket/path/file> Delete Folder rclone purge uablts:<bucket/path/> s3cmd rm s3://<bucket/path/> --recursive aws s3 rm s3://<bucket/path/> --recursive Delete Bucket rclone purge uablts:<bucket> s3cmd rb s3://<bucket> aws s3api delete-bucket --bucket <bucket>","title":"Introduction"},{"location":"data_management/lts/lts/#long-term-storage","text":"Important This page is a work in progress. Please be patient as information is added or removed. Thank you. UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments. Important Currently, UAB LTS is only accessible using the UAB Campus Network. If you are off campus and want to access LTS, please use the UAB Campus VPN . If you are accessing LTS through Cheaha, you do not need to use the VPN, even at home.","title":"Long-Term Storage"},{"location":"data_management/lts/lts/#terminology","text":"When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead. In S3, there are no such things as folders and files, just objects. Everything is an object, and there is no standard filesystem where things are physically stored in a heirarchical manner. However, there are certain things that can make it look that way such as prefixes. Prefixes are used in place of a file path to an object, and so can be used to represent a path to an object. This documentation will use the standard file and path terms since that's more easily understood by most users. Just be aware that documentation such as AWS CLI will use terms prefix, object, and others that are not standard in a typical filesystem.","title":"Terminology"},{"location":"data_management/lts/lts/#requesting-an-account","text":"UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to support@listserv.uab.edu . You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS.","title":"Requesting an Account"},{"location":"data_management/lts/lts/#windowsmac","text":"To access LTS from Windows and Mac, we suggest using the Cyberduck GUI which is free to download. Once you have it installed and open, Cyberduck will look like this:","title":"Windows/Mac"},{"location":"data_management/lts/lts/#creating-a-connection","text":"First, download the UAB CyberDuck Connection Profile . After it's downloaded, double click the file to open it in Cyberduck. It will open the following connection creation window: Input your Access Key and Secret Access Key sent to you by Research Computing after account creation in their appropriate fields. Once you've entered these keys you can close the connection creation window. This connection with the keys you entered is now saved as a bookmark for easy access in the future. Double click the created bookmark to open the connection to LTS.","title":"Creating a Connection"},{"location":"data_management/lts/lts/#creating-a-bucket","text":"Sets of storage objects are stored in what are called buckets. Buckets are sets of file systems for storing data. You can create different buckets for various purposes, such as separating buckets by dataset or project, or having a single bucket for all data you are moving off of Cheaha project storage. In order to create a bucket, click File > New Folder... and then name the bucket you would like to create. Once the bucket is created, it will appear in the File window. An example could look like: The bucket will have the symbol of a hard disk with an Amazon A brand on it. This is the root of the file system for that bucket. You can then double click into it to open that file system. Important Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage.","title":"Creating a Bucket"},{"location":"data_management/lts/lts/#uploading-and-downloading-data","text":"Once you're inside the bucket, files can be uploaded easily through dragging and dropping from your local machine into the GUI. You can also use the Upload button in the toolbar to open a file browser and choose what to upload. Downloading files from the bucket can be done by first selecting the file(s)/folder(s) to download and then clicking the Actions button in the toolbar. In that dropdown will be a Download option. You can also get to this dropdown through the File menu or by right-clicking. Note By default, all buckets and files you upload are only available to you.Currently, there is not a known method to add permissions for individual researchers in the Cyberduck interface. If you need to add access to a bucket or set of files within a bucket, please look in the Linux guide below. If you do not have access to a personal Linux machine, Cheaha is available to use for this purpose.","title":"Uploading and Downloading Data"},{"location":"data_management/lts/lts/#alternative-interfaces","text":"In addition to Cyberduck, there are other GUI based programs for interfacing with UAB LTS. S3 Browser is an easy-to-use program for uploading and downloading files. However more sophisticated tools, such as setting file permissions, are hidden behind a paywall. This tool is also only available on Windows platforms. researchers can investigate this tool if desired, however research computing will not provide direct support for this program.","title":"Alternative Interfaces"},{"location":"data_management/lts/lts/#linuxcommand-line","text":"Linux has very few workable GUIs capable of accessing S3 storage for free available, and so almost all tools for transferring from Cheaha to LTS will be command line interfaces (CLI). The positives for this are that CLIs offer a much broader range of function available to researchers for managing their LTS buckets. There are a few different CLIs available to researchers on Cheaha to use. Current available CLIs on Cheaha are rclone , s3cmd , and the AWS CLI . This documentation will show how to perform each function using all three tools where possible and will give a comparison chart contrasting what each tool is useful for. Both of these are available as modules under the rclone and awscli module names. Note Of note, all of these tools are available for Windows and Mac as well if you are comfortable using command line interfaces on those platforms. There are installation instructions for both of these tools on their respective websites.","title":"Linux/Command Line"},{"location":"data_management/lts/lts/#configuration","text":"In order to access LTS through the command line no matter which CLI you use, you will need to perform some configuration. This will set up the remote connection from your local machine (or you Cheaha profile) to LTS. If you choose to use multiple CLIs, you will need to perform configuration for each one separately.","title":"Configuration"},{"location":"data_management/lts/lts/#rclone","text":"The instructions for setting up a remote connection with rclone can be found in our main rclone documentation . In the following examples, uablts is used as the name of the remote connection.","title":"rclone"},{"location":"data_management/lts/lts/#s3cmd","text":"s3cmd can be easily installed via an Anaconda environment . Create the environment, activate it, then install using: pip install s3cmd Note Depending on how Anaconda chooses to install the package, the actual s3cmd script may be in your $HOME/.local/bin folder. This folder can be added to your path using PATH=$PATH:$HOME/.local/bin , and you will have access to the s3cmd script after that. Once you have s3cmd downloaded, you can start the configuration process like so: s3cmd --configure [ -c $HOME /profile_name ] You can run the configuration either with or without the [-c] option. If you use it, a file named profile_name will be created in your home directory with your login credentials and other information. If you omit the -c option, a file called $HOME/.s3cfg will be created by default. This can be helpful if you have multiple S3 profiles you are using. If you use UAB LTS as your only S3 storage platform, it's suggested to omit the -c option. Note After configuration, the s3cmd command will default to using the .s3cfg file for credentials if it exists. If you create a separate named profile file, you will need to add that to the s3cmd call each time you run it. During configuration, you will be asked to enter some information. You can follow the example below, inputting your user-specific information where required. Lines requiring user input are highlighted. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key: <access key> Secret Key: <secret key> Default Region [US]: <leave blank> Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [s3.amazonaws.com]: s3.lts.rc.uab.edu Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).s3.lts.rc.uab.edu Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password: <leave blank> Path to GPG program [/usr/bin/gpg]: $HOME/bin/gpg When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer Use HTTPS protocol [Yes]: <leave blank> On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name: <leave blank> New settings: Access Key: <access key> Secret Key: <secret key> Default Region: US S3 Endpoint: s3.lts.rc.uab.edu DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).s3.lts.rc.uab.edu Encryption password: Path to GPG program: $HOME/bin/gpg Use HTTPS protocol: True HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n] Y Please wait, attempting to list all buckets... Success. Your access key and secret key worked fine :-) Now verifying that encryption works... Not configured. Never mind. Save settings? [y/N] y If your test access succeeded, you are now ready to use s3cmd .","title":"s3cmd"},{"location":"data_management/lts/lts/#aws-cli","text":"Setting up a remote connection using AWS CLI is fairly straightforward. After loading the awscli module, run aws configure in the terminal. It will ask you enter four pieces of information, fill them out as so: AWS Access Key [none]: <access key> AWS Secret Access Key [none]: <secret key> Default region name [none]: <Press Enter> Default output format [none]: json Your access key and secret key should have been given to you by research computing when you requested your account. Copy-paste these into the terminal when requested. Important AWS CLI assumes you are using the AWS service for S3 storage. In order to access UAB LTS, for all AWS CLI commands, you will need to add the --endpoint-url https://s3.lts.rc.uab.edu option to the aws function call.","title":"AWS CLI"},{"location":"data_management/lts/lts/#general-command-structure","text":"RClone : rclone <subcommand> [ options ] <remote>:<bucket> Important For all commands, replace everything in <> with the necessary values. Do not include the <> symbols in your command. To see a list of all subcommands available to rclone, you can use rclone --help . You can also use the --help option with any subcommand to see a detailed explanation of what it does plus any options you may want or need to set when calling it. s3cmd : s3cmd [ -c profile_file ] <command> [ options ] [ -n --dry-run ] As noted previously, the [-c profile_file] is only required if you are NOT using credentials saved in the $HOME/.s3cfg file. Otherwise, you can leave it out. To see a list of commands available, use s3cmd --help . Additionally, if you want to test an action without actually running it (i.e. it prints all actions that would be performed), you can add the -n or --dry-run option. AWS CLI : aws <command> <subcommand> [ options ] The <command> for most commonly used functions will either be s3 or s3api . You can use the help option to view available commands, subcommands, and options for AWS. Additionally, when running basically any AWS CLI command, you can include the --dryrun option to see the exact actions that will be performed without actually performing them. This is useful for things like deleting files and folders to make sure you are not performing an unwanted action. Important If you are wanting to perform actions on a specific directory in S3, it is imperative to add the / at the end of the directory name. For more information on this, see our FAQ","title":"General Command Structure"},{"location":"data_management/lts/lts/#make-a-bucket","text":"Buckets are essentially the root folder of a filesystem where you are storing your data. You will need to create a bucket before being able to copy data to LTS. Important Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. RClone : rclone mkdir uablts:<bucket> s3cmd : s3cmd mb s3://<bucket> AWS CLI : aws s3api create-bucket --bucket <bucket> --endpoint-url https://s3.lts.rc.uab.edu","title":"Make a Bucket"},{"location":"data_management/lts/lts/#listing-buckets-and-contents","text":"RClone : To list all buckets you have available, use the lsd subcommand with only the remote specified: rclone lsd uablts: To list all contents inside a bucket, use the ls subcommand with the remote and bucket specified. You can also be specific about the path to the directory you want to list. rclone ls uablts:<bucket/path/directory> This outputs all files along with their directory path recursively. So if you only specify the main bucket, it will output every file in the bucket no matter how deep in the directory tree. To only list files and folders in a given directory, you can use the lsf subcommand rclone lsf uablts:<bucket/path/directory/> s3cmd : With s3cmd, you can use the ls command to list both buckets and their contents. s3cmd ls <s3://bucket/path/> You can add the --recursive option to list all files in the given path. By default, it only lists objects or folders at the top level of the path. AWS CLI : aws s3 ls <bucket/path/directory/> --endpoint-url https://s3.lts.rc.uab.edu If you would like to list all objects recursively, you can add the --recursive tag. A couple of other helpful options are --summarize and --human-readable that will give a total number of objects and their size and make the size output more easily readable, respectively.","title":"Listing Buckets and Contents"},{"location":"data_management/lts/lts/#uploading-files-and-folders","text":"RClone : Uploading files and folders can be done a couple of ways. The first is by using the copy subcommand. This will add files from the source to the destination. rclone copy <source> uablts:<bucket/destination> The second method is using the sync subcommand. This subcommand makes the destination identical to the source. The -i option can be added to make it interactive, asking you whether to copy or delete each file. rclone sync [ -i ] <source> uablts:<bucket/destination/> Danger Be extremely cautious using sync. If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable. s3cmd : s3cmd disinguishes between moving files between a local source and S3 versus moving files between two S3 locations using 3 different commands. # transfer from local to S3 s3cmd put <source> s3://<bucket/path/destination/> # transfer from S3 to local s3cmd get s3://<bucket/path/source/> <destination> # transfer between two S3 locations s3cmd cp s3://<bucket/path/> s3://<bucket/path/> If you are transferring an entire folder from S3 to either another S3 location or a local destination, you will need to add the --recursive option, otherwise you will get an error. Like rclone and AWS, there is also a sync command here as well. # sync an S3 location to a local source s3cmd sync <source> s3://<bucket/path/destination> # sync a local destination to an S3 location s3cmd sync s3://<bucket/path/source> <destination> AWS CLI : Copying files and directories can be managed using the cp subcommand and has the same behavior as rclone's copy . aws s3 cp <source> s3://<bucket/path/destination> --endpoint-url https://s3.lts.rc.uab.edu [ --recursive ] If you are copying a directory, you will need to add the --recursive option. If you are wanting to copy data down from LTS to your local machine or Cheaha, just reverse the positions of the source and destination in the function call. Like rclone, AWS also has a sync subcommand that performs the same functionality. You can use it like so: aws s3 sync <source> s3://<bucket/path/destination> --endpoint-url https://s3.lts.rc.uab.edu [ --delete ] sync has an added benefit that only files that do not exist in the destination or files that have changed in the source will be transferred whereas cp copies everything no matter if it already exists in the destination. By default, sync DOES NOT cause files in the detination that are not in the source to be deleted like rclone sync does. If you want this functionality, you can add the --delete tag at the end of the function call. Danger Be extremely cautious using --delete . Only use if you are sure any data deleted is not important. Data deleted from LTS is not recoverable.","title":"Uploading Files and Folders"},{"location":"data_management/lts/lts/#deleting-files-and-directories","text":"RClone : File deletion is performed using the delete subcommand. rclone delete uablts:<bucket/path/file> Directory deletion is handled using the purge subcommand. Be very cautious with this, as this deletes all files and subdirectories within the directory as well. rclone purge uablts:<bucket/path/> s3cmd : File and directory deletion is handled by the rm command. s3cmd rm s3://<bucket/path/file> [ --recursive ] If you want to delete a directory, you will need to add the --recursive option. To delete an entire bucket, use the rb command. s3cmd rb s3://<bucket> AWS CLI : The subcommand for deleting files and folders from LTS is rm : aws s3 rm s3://<bucket/path/object> --endpoint-url https://s3.lts.rc.uab.edu [ --recursive ] rm can delete both files and folders. If you are wanting to delete a folder and everything in it, you will need to add the --recursive option. Like with sync be very cautious using rm and make sure you know what you are deleting before you do so. To delete an entire bucket, you will need to use the s3api command paired with the `delete-bucket subcommand. An example of this looks like: aws s3api delete-bucket --bucket <bucket> --endpoint-url https://s3.lts.rc.uab.edu","title":"Deleting Files and Directories"},{"location":"data_management/lts/lts/#command-comparison-chart","text":"Note For brevity, the chart will exclude the --endpoint-url option from the AWS CLI commands, but it will need to be included if you choose to use that tool. Action rclone s3cmd AWS CLI Make Bucket rclone mkdir uablts:<bucket> s3cmd mb s3://<bucket> aws s3api create-bucket --bucket <bucket> List Buckets rclone lsd uablts: s3cmd ls aws s3 ls List Files rclone lsf uablts:<bucket/path/> s3cmd ls s3://<bucket/path/> aws s3 ls s3://<bucket/path/> Full Upload rclone copy <source> uablts:<bucket/destination> s3cmd put <source> s3://<bucket/destination/> aws s3 cp <source> s3://<bucket/destination> Download rclone copy uablts:<bucket/source/> <destination> s3cmd get s3://<bucket/source/> <destination> aws s3 cp s3://<bucket/source/> <destination> Sync rclone sync [-i] <source> uablts:<bucket/destination/> s3cmd sync <source> s3://<bucket/destination/> aws s3 sync <source> s3://<bucket/destination/> [--delete] Delete File rclone delete uablts:<bucket/path/file> s3cmd rm s3://<bucket/path/file> aws s3 rm s3://<bucket/path/file> Delete Folder rclone purge uablts:<bucket/path/> s3cmd rm s3://<bucket/path/> --recursive aws s3 rm s3://<bucket/path/> --recursive Delete Bucket rclone purge uablts:<bucket> s3cmd rb s3://<bucket> aws s3api delete-bucket --bucket <bucket>","title":"Command Comparison Chart"},{"location":"data_management/lts/sharing/","text":"Sharing Buckets \u00b6 A major use for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created. Instead, sharing buckets must be done through the command line using bucket policies . A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to objects within the bucket. If you have not worked with JSON files before, a brief explantion can be found here . It's important to note that the bucket owner will always retain the ability to perform all actions on a bucket and its contents and so do not need to be explicitly granted permissions. Policy Structure \u00b6 Policies files are essentially built as a series of statements expressly allowing or deny access to functions that interact with objects in S3. A skeleton policy file would look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"description\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : []}, \"Action\" : [], \"Resource\" : [] }] } Each statement is made up of a few fields: Sid: a short decription of what the statement is for (i.e. \"bucket-access\") Effect: \"Allow\" or \"Deny\" permissions based on how you want to alter permissions Principal: Essentially a list of users to change permissions for. Have to formatted like arn:aws:iam:::user/<blazerid> . Action: A list of commands to allow or deny permission for, depending on the Effect value. Resource: The name of the bucket or objects to apply permissions to. Must be formatted like arn:aws:s3:::<bucket[/path/objects]> . It is currently suggested to have at least two statements, one statement allowing access to the bucket itself, and another statement dictating permissions for objects in the bucket. For example, if you wanted to give users bob and jane the ability to list objects in your bucket b1 , the statement would be: { \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] } Critically, this does not allow the given users the ability to read, download, edit, or delete any objects in the bucket. They will be able to list the objects, see the names, sizes, and directory structure, but will not be able to interact with the objects. These permissions should be enumerated in a separate statement like: { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] } This permission set allows bob and jane to download files but not to move, overwrite, delete, or otherwise interact with them. Notice that the Resource value has changed from just b1 to b1/* . We can set this Resource value to different paths and objects to limit the permissions granted. For example: b1/* : Apply permissions to all objects in the entire bucket b1/test_folder/* : Apply permissions to all objects in folder test_folder b1/test_folder/*jpg : Apply read permissions to only JPGs within test_folder . For the last two examples, bob and jane will not have permission to download any files outside of the test_folder folder. All permissions are implicitly denied unless explicitly given in the policy statements. Common Actions \u00b6 Being able to download a file is only one possible action you may want to give permission for. Uploading files as well as altering the policy of the bucket may also be useful to give permissions for. Here is a short list of common actions you may want to give permissions for: s3:ListBucket : access to see but not interact with objects s3:GetObject : download objects s3:PutObject : upload objects s3:DeleteObject : remove objects s3:GetBucketPolicy : view the current bucket policy s3:PutBucketPolicy : change the current bucket policy A full list of Actions for UAB LTS can be seen on the Ceph docs . Example Policies \u00b6 Read-Only for All Files \u00b6 { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] } Read-Write Permissions \u00b6 This will give read, write, and delete permissions to the users so they are able to sync directories between a local source folder and the S3 destination { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" , \"s3.PutObject\" , \"s3.DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] } Tiered Permissions \u00b6 In some instances, the bucket owner (i.e. ideally the PI for the lab if this is a shared lab space) will want to allow certain users to have permissions to alter the policies for new or departing lab members. This example will give standard read-write permissions to both our lab members, but only policy altering permissions to jane . { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"change-policy\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetBucketPolicy\" , \"s3.PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" , \"s3.PutObject\" , \"s3.DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] } Applying a Policy \u00b6 Policies can be applied to a bucket either by the owner or by a user who has been given the s3:PutBucketPolicy permission. You can use either s3cmd or AWS CLI for this. Note As of now, rclone has not implemented a way to alter policies. If you have been using rclone, you will need to configure either s3cmd or AWS CLI and use those for policy management instead Applying a policy is fairly straightforward for both tools. First, you should save the policy as a JSON file. The, you can interact with the policy using these commands: # s3cmd set policy s3cmd setpolicy <policy_file> s3://<bucket> # s3cmd view policy s3cmd getpolicy s3://<bucket> # s3cmd remove policy s3cmd delpolicy s3://<bucket> # AWS CLI set policy aws s3api put-bucket-policy --bucket <bucket> --policy <policy_file> --endpoint https://s3.lts.rc.uab.edu # AWS CLI view policy aws s3api get-bucket-policy --bucket <bucket> --endpoint https://s3.lts.rc.uab.edu # AWS CLI remove policy aws s3api delete-bucket-policy --bucket <bucket> --endpoint https://s3.lts.rc.uab.edu Policy Suggestions \u00b6 Important Policies can be very complicated depending on how many people need access to the bucket and how you want to tier permissions (i.e. which people are read-only, read-write, admin-esq priveleges, etc.). If you need help structuring your policy files please visit us during office hours and we will be happy to help structure your policy file to your needs. Admin-esq Priveleges \u00b6 It is suggested to keep the number of people who have permission to delete data and alter policies to a minimum. Inexperience with policies can result in permissions being granted to incorrect users which can potentially lead to irrecoverable consequences. Syncing data without purposeful thought can result in the undesired loss of data. Bucket Ownership \u00b6 For labs using LTS to store data from their Cheaha project space, it is highly advised that the PI for the lab creates and owns the bucket and then gives policy changing permissions to another researcher for day-to-day maintenance if desired. For instance, if a lab manager creates the bucket and then leaves the university without giving policy permissions to other users, the lab will not be able to change the policies for those data. Sharing Multiple Datasets with Different Groups \u00b6 Some groups on campus may distribute datasets to other research groups using LTS. If you are distributing data to multiple groups, and those groups should not have access to each other's data, it is highly advised to store those datasets in separate buckets as opposed to separate directories in a single bucket. An idiosyncrasy of buckets involves the fact that all objects are stored in the top level of the bucket, and once permissions are given to someone to see the bucket, they will be able to see all objects within the bucket without restrictions even if they are not given download permissions for some objects. If any identifying or priveleged information is given in file names on LTS, it could constitute an IRB violation. Additionally, managing permissions for groups to access data only from specific folders makes the policy file much more complicated and prone to errors. When sharing multiple datasets with multiple different groups, it's advised to keep these data in separate buckets and have individual policy files for each bucket to make policy management simpler and less prone to error.","title":"Sharing"},{"location":"data_management/lts/sharing/#sharing-buckets","text":"A major use for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created. Instead, sharing buckets must be done through the command line using bucket policies . A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to objects within the bucket. If you have not worked with JSON files before, a brief explantion can be found here . It's important to note that the bucket owner will always retain the ability to perform all actions on a bucket and its contents and so do not need to be explicitly granted permissions.","title":"Sharing Buckets"},{"location":"data_management/lts/sharing/#policy-structure","text":"Policies files are essentially built as a series of statements expressly allowing or deny access to functions that interact with objects in S3. A skeleton policy file would look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"description\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : []}, \"Action\" : [], \"Resource\" : [] }] } Each statement is made up of a few fields: Sid: a short decription of what the statement is for (i.e. \"bucket-access\") Effect: \"Allow\" or \"Deny\" permissions based on how you want to alter permissions Principal: Essentially a list of users to change permissions for. Have to formatted like arn:aws:iam:::user/<blazerid> . Action: A list of commands to allow or deny permission for, depending on the Effect value. Resource: The name of the bucket or objects to apply permissions to. Must be formatted like arn:aws:s3:::<bucket[/path/objects]> . It is currently suggested to have at least two statements, one statement allowing access to the bucket itself, and another statement dictating permissions for objects in the bucket. For example, if you wanted to give users bob and jane the ability to list objects in your bucket b1 , the statement would be: { \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] } Critically, this does not allow the given users the ability to read, download, edit, or delete any objects in the bucket. They will be able to list the objects, see the names, sizes, and directory structure, but will not be able to interact with the objects. These permissions should be enumerated in a separate statement like: { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] } This permission set allows bob and jane to download files but not to move, overwrite, delete, or otherwise interact with them. Notice that the Resource value has changed from just b1 to b1/* . We can set this Resource value to different paths and objects to limit the permissions granted. For example: b1/* : Apply permissions to all objects in the entire bucket b1/test_folder/* : Apply permissions to all objects in folder test_folder b1/test_folder/*jpg : Apply read permissions to only JPGs within test_folder . For the last two examples, bob and jane will not have permission to download any files outside of the test_folder folder. All permissions are implicitly denied unless explicitly given in the policy statements.","title":"Policy Structure"},{"location":"data_management/lts/sharing/#common-actions","text":"Being able to download a file is only one possible action you may want to give permission for. Uploading files as well as altering the policy of the bucket may also be useful to give permissions for. Here is a short list of common actions you may want to give permissions for: s3:ListBucket : access to see but not interact with objects s3:GetObject : download objects s3:PutObject : upload objects s3:DeleteObject : remove objects s3:GetBucketPolicy : view the current bucket policy s3:PutBucketPolicy : change the current bucket policy A full list of Actions for UAB LTS can be seen on the Ceph docs .","title":"Common Actions"},{"location":"data_management/lts/sharing/#example-policies","text":"","title":"Example Policies"},{"location":"data_management/lts/sharing/#read-only-for-all-files","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] }","title":"Read-Only for All Files"},{"location":"data_management/lts/sharing/#read-write-permissions","text":"This will give read, write, and delete permissions to the users so they are able to sync directories between a local source folder and the S3 destination { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" , \"s3.PutObject\" , \"s3.DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] }","title":"Read-Write Permissions"},{"location":"data_management/lts/sharing/#tiered-permissions","text":"In some instances, the bucket owner (i.e. ideally the PI for the lab if this is a shared lab space) will want to allow certain users to have permissions to alter the policies for new or departing lab members. This example will give standard read-write permissions to both our lab members, but only policy altering permissions to jane . { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"change-policy\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetBucketPolicy\" , \"s3.PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::b1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane\" ] }, \"Action\" : [ \"s3.GetObject\" , \"s3.PutObject\" , \"s3.DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::b1/*\" ] }] }","title":"Tiered Permissions"},{"location":"data_management/lts/sharing/#applying-a-policy","text":"Policies can be applied to a bucket either by the owner or by a user who has been given the s3:PutBucketPolicy permission. You can use either s3cmd or AWS CLI for this. Note As of now, rclone has not implemented a way to alter policies. If you have been using rclone, you will need to configure either s3cmd or AWS CLI and use those for policy management instead Applying a policy is fairly straightforward for both tools. First, you should save the policy as a JSON file. The, you can interact with the policy using these commands: # s3cmd set policy s3cmd setpolicy <policy_file> s3://<bucket> # s3cmd view policy s3cmd getpolicy s3://<bucket> # s3cmd remove policy s3cmd delpolicy s3://<bucket> # AWS CLI set policy aws s3api put-bucket-policy --bucket <bucket> --policy <policy_file> --endpoint https://s3.lts.rc.uab.edu # AWS CLI view policy aws s3api get-bucket-policy --bucket <bucket> --endpoint https://s3.lts.rc.uab.edu # AWS CLI remove policy aws s3api delete-bucket-policy --bucket <bucket> --endpoint https://s3.lts.rc.uab.edu","title":"Applying a Policy"},{"location":"data_management/lts/sharing/#policy-suggestions","text":"Important Policies can be very complicated depending on how many people need access to the bucket and how you want to tier permissions (i.e. which people are read-only, read-write, admin-esq priveleges, etc.). If you need help structuring your policy files please visit us during office hours and we will be happy to help structure your policy file to your needs.","title":"Policy Suggestions"},{"location":"data_management/lts/sharing/#admin-esq-priveleges","text":"It is suggested to keep the number of people who have permission to delete data and alter policies to a minimum. Inexperience with policies can result in permissions being granted to incorrect users which can potentially lead to irrecoverable consequences. Syncing data without purposeful thought can result in the undesired loss of data.","title":"Admin-esq Priveleges"},{"location":"data_management/lts/sharing/#bucket-ownership","text":"For labs using LTS to store data from their Cheaha project space, it is highly advised that the PI for the lab creates and owns the bucket and then gives policy changing permissions to another researcher for day-to-day maintenance if desired. For instance, if a lab manager creates the bucket and then leaves the university without giving policy permissions to other users, the lab will not be able to change the policies for those data.","title":"Bucket Ownership"},{"location":"data_management/lts/sharing/#sharing-multiple-datasets-with-different-groups","text":"Some groups on campus may distribute datasets to other research groups using LTS. If you are distributing data to multiple groups, and those groups should not have access to each other's data, it is highly advised to store those datasets in separate buckets as opposed to separate directories in a single bucket. An idiosyncrasy of buckets involves the fact that all objects are stored in the top level of the bucket, and once permissions are given to someone to see the bucket, they will be able to see all objects within the bucket without restrictions even if they are not given download permissions for some objects. If any identifying or priveleged information is given in file names on LTS, it could constitute an IRB violation. Additionally, managing permissions for groups to access data only from specific folders makes the policy file much more complicated and prone to errors. When sharing multiple datasets with multiple different groups, it's advised to keep these data in separate buckets and have individual policy files for each bucket to make policy management simpler and less prone to error.","title":"Sharing Multiple Datasets with Different Groups"},{"location":"data_management/transfer/filezilla/","text":"FileZilla \u00b6 Filezilla is a free SFTP platform used to transfer data between a local machine and a remote server, in this case remote storage accessible to Cheaha and UAB Cloud or long-term S3 storage. Filezilla is useful for medium sized data transfers from a local machine as opposed to Globus which can easily connect two different local or remote servers and handle large quantities of data. Installation \u00b6 To download the program, go to the FileZilla site and click Download FileZilla Client. Filezilla is also already installed on Cheaha and can be accessed through the Accessories menu. Using FileZilla \u00b6 Once FileZilla is installed and you open it, you will see the following window A file browser for the local machine FileZilla is running on is on the left while the file system for the remote site will be shown on the right once a remote site has been connected. Creating a Remote Connection \u00b6 You can easily connect to a remote site using the QuickConnect bar near the top of the window. You will need to input the following options: Host: sftp://cheaha.rc.uab.edu Username: your BlazerID or XIAS ID (do NOT include '@uab.edu') Password: your BlazerID or XIAS password Port: 22 Click Quickconnect and you will be connected to the remote storage system. The window should now look like: When connecting in the future, you will be able to select the connection from the dropdown arrow next to the Quickconnect button. In both the local and remote panes, you can navigate to the directories you are transferring from and to. You only have access to directories you can normally access on Cheaha, so your user space as well as any project directories you have been added to. Transferring Data \u00b6 From here you can drag and drop whichever files and directories between the local and remote site windows. This will automatically initiate a file transfer. The directory structure is maintained from the source to the destination and is recursive, so all subdirectories and folders within the main directory will be transferred as well. Transfer status will be logged at the bottom of the window. Once all your files have been transferred, you can close FileZilla which will close the SFTP connection.","title":"FileZilla"},{"location":"data_management/transfer/filezilla/#filezilla","text":"Filezilla is a free SFTP platform used to transfer data between a local machine and a remote server, in this case remote storage accessible to Cheaha and UAB Cloud or long-term S3 storage. Filezilla is useful for medium sized data transfers from a local machine as opposed to Globus which can easily connect two different local or remote servers and handle large quantities of data.","title":"FileZilla"},{"location":"data_management/transfer/filezilla/#installation","text":"To download the program, go to the FileZilla site and click Download FileZilla Client. Filezilla is also already installed on Cheaha and can be accessed through the Accessories menu.","title":"Installation"},{"location":"data_management/transfer/filezilla/#using-filezilla","text":"Once FileZilla is installed and you open it, you will see the following window A file browser for the local machine FileZilla is running on is on the left while the file system for the remote site will be shown on the right once a remote site has been connected.","title":"Using FileZilla"},{"location":"data_management/transfer/filezilla/#creating-a-remote-connection","text":"You can easily connect to a remote site using the QuickConnect bar near the top of the window. You will need to input the following options: Host: sftp://cheaha.rc.uab.edu Username: your BlazerID or XIAS ID (do NOT include '@uab.edu') Password: your BlazerID or XIAS password Port: 22 Click Quickconnect and you will be connected to the remote storage system. The window should now look like: When connecting in the future, you will be able to select the connection from the dropdown arrow next to the Quickconnect button. In both the local and remote panes, you can navigate to the directories you are transferring from and to. You only have access to directories you can normally access on Cheaha, so your user space as well as any project directories you have been added to.","title":"Creating a Remote Connection"},{"location":"data_management/transfer/filezilla/#transferring-data","text":"From here you can drag and drop whichever files and directories between the local and remote site windows. This will automatically initiate a file transfer. The directory structure is maintained from the source to the destination and is recursive, so all subdirectories and folders within the main directory will be transferred as well. Transfer status will be logged at the bottom of the window. Once all your files have been transferred, you can close FileZilla which will close the SFTP connection.","title":"Transferring Data"},{"location":"data_management/transfer/globus/","text":"Globus \u00b6 Setting up Globus Connect Personal \u00b6 Globus Connect Personal is software meant to be installed on local machines such as laptops, desktops, workstations and self-owned, local-scale servers. Globus maintains excellent documentation for installation on MacOS , Linux and Windows . To verify your installation is complete, please visit https://app.globus.org and log in. Click \"Endpoints\" in the left-hand navigation pane and then click the \"Administered By You\" tab. Look in the table for the endpoint you just created. Moving Data Between Endpoints \u00b6 Log in to the Globus App online at https://app.globus.org using UAB Single Sign-On (SSO). Start typing \"University of Alabama at Birmingham\" into the \"Use your existing organizational login\" text box and selected it when it appears in the list. Click File Manager in the left-hand navigation pane. Ensure the center icon for the \"Panels\" selection is picked. Click the \"Search\" icon in the \"Collection\" text box near the top-left or top-right of the page to locate an endpoint. There are multiple ways to find an endpoint. For some endpoints you may be asked to log in, which is true of all UAB endpoints. Some UAB endpoints may also require that you be on the UAB Campus VPN. Begin typing in the box to search for an endpoint. To find UAB-related endpoints, search for \"UAB\". There are two Cheaha endpoints Cheaha cluster on-campus (UAB Science DMZ) for machines that are either on the UAB Campus Network, or connected to the UAB Campus VPN. Cheaha cluster off-campus (UAB Science DMZ) for machines that are not on the UAB Campus Network and not on the UAB Campus VPN. The \"Recent\" tab shows endpoints that have most recently been used. The \"Bookmarks\" tab shows a list of endpoint bookmarks. The \"Your Collections\" tab shows all endpoints owned by you. For most researchers this will be one or more Globus Connect Personal endpoints. The \"Shared With You\" tab shows any private endpoints that have been shared with you by other users, possibly collaborators. The \"More Options\" tab will show a brief text on installing Globus Connect Personal. When an endpoint has been selected you will see a list of folders and files on the default path for that endpoint in the bottom box. You can use the \"Path\" box to type a path to find the files you are looking for. Repeat the process of selecting an endpoint for the other \"Collection\" text box. When both endpoints have been selected and you have chosen the correct paths for each endpoint, select files and/or folders on the side you wish to transfer FROM. We will call this side the source endpoint, and the other side the target endpoint. Selections may be made by clicking the checkboxes that appear when you hover over each file or folder. When all files and folders have been selected from the source endpoint, click the \"Start\" button on the source endpoint side. This will start a transfer process from source to target. The files will be placed in the currently open path on the target endpoint. A green pop-up notification will appear indicating the transfer has started. Click \"View details >\" to be taken to the status of the transfer. You can also check on the status of any transfers by clicking the \"Activity\" button in the left-hand navigation pane. Note File permissions from the source will not be copied to the destination. Please read more about this in our FAQ Transfer and Sync Options \u00b6 Between the two \"Start\" buttons on the \"File Manager\" page is a \"Transfer & Sync Options\" drop down menu. Click that button to change the options. More information on each option. A brief summary of the options are... sync - Sync files only, rather than create new files. delete files - Delete any files on the target that are not on the source. Useful for forcing identical filesystems when syncing. preserve source - Copies file \"modified time\" metadata. verify integrity - Verifies that checksums are identical on source and target after transfer completes. Highly recommended to have this checked. encrypt transfer - Encrypts data before leaving source and decrypts after arriving at destination. Recommended for all transfers, required and enforced for all UAB endpoints. skip files - Skips source files that cause errors during the transfer. Otherwise the entire transfer will stop when an error is encountered. quota fail - Fails instead of retries when the target storage quota is exceeded. Common Errors \u00b6 File Not Found - This may mean that a file was not readable by Globus. Check that the file hasn't moved or changed names during the transfer. It is recommended to not modify files while they are being transferred by Globus. Permission Denied - Globus is not able to access the files because permissions do not allow it. For Globus Connect Personal, be sure the containing folder is on the \"Accessible Folders\" list. Be sure that your Cheaha account has access to read the file. More Information \u00b6 A Globus FAQ is available for additional information on endpoints and transfers. Using Bookmarks \u00b6 To save a bookmark, use the File Manager interface to select an endpoint and navigate to a path on that endpoint. Then click the bookmark button as shown below. To manage bookmarks, click \"Bookmarks\" in the left-hand navigation pane. Click the \"Pencil\" icon to edit a bookmark. Click the \"Trash Bin\" icon to delete a bookmark. Managing Shared Collections from a Globus Connect Personal Endpoint \u00b6 It is NOT RECOMMENDED to make Globus Connect Personal endpoints public as this is insecure. It is more difficult to manage access controls for the entire Globus Connect Personal endpoint than for a shared collection. Shared collections make it simpler to share different data with distinct collaborators, and to manage who has access to what data. Be secure, use shared collections! Creating a Shared Collection \u00b6 Click \"Endpoints\" in the left-hand navigation pane. Click the \"Administered By You\" tab. In the table, find the endpoint you wish to share data from and click its name. You will be taken to the page for that endpoint. Click the \"Collections\" tab. Click the \"Add a Guest Collection\" button. Fill out the form. Manually enter a path or click the Browse button to select a folder. Give a short but memorable name for your shared collection. This information will be useful for your collaborators. Optionally fill in a more detailed description of the shared collection for your records. Optionally fill in searchable keywords. Click \"Create Share\" to move to the next step. You will be taken to the page for the newly created collection, which is now a full-fledged endpoint. Any further references to \"an endpoint\" will be about the newly created, shared collection. Make sure you are on the \"Permissions\" tab. You should see a permissions table with your name in the first row. Click \"Add Permissions -- Share With\" to share your endpoint with other users. Fill out the form. Optionally enter a path within the shared endpoint or use the Browse button. If you leave the path as just a slash, the entire shared endpoint will be shared with these users. Select who to share with. User - One or more users. Group - All members of a group. All Users - All globus users. Danger This will expose information to everyone on Globus! Search for users to add, or a group, depending on your choice above. You should be able to find any globus user using the search box. Warning Be certain of which user you are selecting! Check the email address domain. If adding users, optionally enter a message so they know why they are being added. Select permissions. Read is automatically selected and cannot be changed. Write permissions are optional. Click \"Add Permission\" to add permissions for these users or groups. You will be returned to the page for the shared endpoint and should be on the \"Permissions\" tab and should see the user or group in the table. Deleting a Shared Collection \u00b6 Click \"Endpoints\" in the left-hand navigation pane, then c Click the \"Administered By You\" tab. Click the right caret \">\" icon at the right side of the row with the endpoint you wish to delete. You will be taken to the information page for that endpoint. Click \"X Delete Endpoint\" and a confirmation dialog will open at the top of the page. Respond to the dialog to delete the endpoint, or to cancel. Setting up Globus Connect Server \u00b6 Under construction!","title":"Globus"},{"location":"data_management/transfer/globus/#globus","text":"","title":"Globus"},{"location":"data_management/transfer/globus/#setting-up-globus-connect-personal","text":"Globus Connect Personal is software meant to be installed on local machines such as laptops, desktops, workstations and self-owned, local-scale servers. Globus maintains excellent documentation for installation on MacOS , Linux and Windows . To verify your installation is complete, please visit https://app.globus.org and log in. Click \"Endpoints\" in the left-hand navigation pane and then click the \"Administered By You\" tab. Look in the table for the endpoint you just created.","title":"Setting up Globus Connect Personal"},{"location":"data_management/transfer/globus/#moving-data-between-endpoints","text":"Log in to the Globus App online at https://app.globus.org using UAB Single Sign-On (SSO). Start typing \"University of Alabama at Birmingham\" into the \"Use your existing organizational login\" text box and selected it when it appears in the list. Click File Manager in the left-hand navigation pane. Ensure the center icon for the \"Panels\" selection is picked. Click the \"Search\" icon in the \"Collection\" text box near the top-left or top-right of the page to locate an endpoint. There are multiple ways to find an endpoint. For some endpoints you may be asked to log in, which is true of all UAB endpoints. Some UAB endpoints may also require that you be on the UAB Campus VPN. Begin typing in the box to search for an endpoint. To find UAB-related endpoints, search for \"UAB\". There are two Cheaha endpoints Cheaha cluster on-campus (UAB Science DMZ) for machines that are either on the UAB Campus Network, or connected to the UAB Campus VPN. Cheaha cluster off-campus (UAB Science DMZ) for machines that are not on the UAB Campus Network and not on the UAB Campus VPN. The \"Recent\" tab shows endpoints that have most recently been used. The \"Bookmarks\" tab shows a list of endpoint bookmarks. The \"Your Collections\" tab shows all endpoints owned by you. For most researchers this will be one or more Globus Connect Personal endpoints. The \"Shared With You\" tab shows any private endpoints that have been shared with you by other users, possibly collaborators. The \"More Options\" tab will show a brief text on installing Globus Connect Personal. When an endpoint has been selected you will see a list of folders and files on the default path for that endpoint in the bottom box. You can use the \"Path\" box to type a path to find the files you are looking for. Repeat the process of selecting an endpoint for the other \"Collection\" text box. When both endpoints have been selected and you have chosen the correct paths for each endpoint, select files and/or folders on the side you wish to transfer FROM. We will call this side the source endpoint, and the other side the target endpoint. Selections may be made by clicking the checkboxes that appear when you hover over each file or folder. When all files and folders have been selected from the source endpoint, click the \"Start\" button on the source endpoint side. This will start a transfer process from source to target. The files will be placed in the currently open path on the target endpoint. A green pop-up notification will appear indicating the transfer has started. Click \"View details >\" to be taken to the status of the transfer. You can also check on the status of any transfers by clicking the \"Activity\" button in the left-hand navigation pane. Note File permissions from the source will not be copied to the destination. Please read more about this in our FAQ","title":"Moving Data Between Endpoints"},{"location":"data_management/transfer/globus/#transfer-and-sync-options","text":"Between the two \"Start\" buttons on the \"File Manager\" page is a \"Transfer & Sync Options\" drop down menu. Click that button to change the options. More information on each option. A brief summary of the options are... sync - Sync files only, rather than create new files. delete files - Delete any files on the target that are not on the source. Useful for forcing identical filesystems when syncing. preserve source - Copies file \"modified time\" metadata. verify integrity - Verifies that checksums are identical on source and target after transfer completes. Highly recommended to have this checked. encrypt transfer - Encrypts data before leaving source and decrypts after arriving at destination. Recommended for all transfers, required and enforced for all UAB endpoints. skip files - Skips source files that cause errors during the transfer. Otherwise the entire transfer will stop when an error is encountered. quota fail - Fails instead of retries when the target storage quota is exceeded.","title":"Transfer and Sync Options"},{"location":"data_management/transfer/globus/#common-errors","text":"File Not Found - This may mean that a file was not readable by Globus. Check that the file hasn't moved or changed names during the transfer. It is recommended to not modify files while they are being transferred by Globus. Permission Denied - Globus is not able to access the files because permissions do not allow it. For Globus Connect Personal, be sure the containing folder is on the \"Accessible Folders\" list. Be sure that your Cheaha account has access to read the file.","title":"Common Errors"},{"location":"data_management/transfer/globus/#more-information","text":"A Globus FAQ is available for additional information on endpoints and transfers.","title":"More Information"},{"location":"data_management/transfer/globus/#using-bookmarks","text":"To save a bookmark, use the File Manager interface to select an endpoint and navigate to a path on that endpoint. Then click the bookmark button as shown below. To manage bookmarks, click \"Bookmarks\" in the left-hand navigation pane. Click the \"Pencil\" icon to edit a bookmark. Click the \"Trash Bin\" icon to delete a bookmark.","title":"Using Bookmarks"},{"location":"data_management/transfer/globus/#managing-shared-collections-from-a-globus-connect-personal-endpoint","text":"It is NOT RECOMMENDED to make Globus Connect Personal endpoints public as this is insecure. It is more difficult to manage access controls for the entire Globus Connect Personal endpoint than for a shared collection. Shared collections make it simpler to share different data with distinct collaborators, and to manage who has access to what data. Be secure, use shared collections!","title":"Managing Shared Collections from a Globus Connect Personal Endpoint"},{"location":"data_management/transfer/globus/#creating-a-shared-collection","text":"Click \"Endpoints\" in the left-hand navigation pane. Click the \"Administered By You\" tab. In the table, find the endpoint you wish to share data from and click its name. You will be taken to the page for that endpoint. Click the \"Collections\" tab. Click the \"Add a Guest Collection\" button. Fill out the form. Manually enter a path or click the Browse button to select a folder. Give a short but memorable name for your shared collection. This information will be useful for your collaborators. Optionally fill in a more detailed description of the shared collection for your records. Optionally fill in searchable keywords. Click \"Create Share\" to move to the next step. You will be taken to the page for the newly created collection, which is now a full-fledged endpoint. Any further references to \"an endpoint\" will be about the newly created, shared collection. Make sure you are on the \"Permissions\" tab. You should see a permissions table with your name in the first row. Click \"Add Permissions -- Share With\" to share your endpoint with other users. Fill out the form. Optionally enter a path within the shared endpoint or use the Browse button. If you leave the path as just a slash, the entire shared endpoint will be shared with these users. Select who to share with. User - One or more users. Group - All members of a group. All Users - All globus users. Danger This will expose information to everyone on Globus! Search for users to add, or a group, depending on your choice above. You should be able to find any globus user using the search box. Warning Be certain of which user you are selecting! Check the email address domain. If adding users, optionally enter a message so they know why they are being added. Select permissions. Read is automatically selected and cannot be changed. Write permissions are optional. Click \"Add Permission\" to add permissions for these users or groups. You will be returned to the page for the shared endpoint and should be on the \"Permissions\" tab and should see the user or group in the table.","title":"Creating a Shared Collection"},{"location":"data_management/transfer/globus/#deleting-a-shared-collection","text":"Click \"Endpoints\" in the left-hand navigation pane, then c Click the \"Administered By You\" tab. Click the right caret \">\" icon at the right side of the row with the endpoint you wish to delete. You will be taken to the information page for that endpoint. Click \"X Delete Endpoint\" and a confirmation dialog will open at the top of the page. Respond to the dialog to delete the endpoint, or to cancel.","title":"Deleting a Shared Collection"},{"location":"data_management/transfer/globus/#setting-up-globus-connect-server","text":"Under construction!","title":"Setting up Globus Connect Server"},{"location":"data_management/transfer/rclone/","text":"RClone \u00b6 RClone is a powerful command line tool for transferring and synchronizing files over the internet between various machines, servers and cloud storage services. It is highly recommended for small to moderate amounts of data. For very large amounts of data consider using Globus for increased robustness against failure. Where Globus is not available, rclone is still suitable. RClone requires a modest amount of setup time on local machines, but once setup can be used fairly easily. RClone uses the concepts of \"remotes\", which is an abstract term for any storage service or device that is not physically part of the local machine. Many remotes are offered, including SFTP and various UAB Cloud Storage Solutions . SFTP may be used to access Cheaha, cloud.rc and other laptop and desktop computers. To use RClone effectively, you'll need to setup remotes before using the various commands. Most file manipulation commands on Linux can be found in the RClone commands, but may have slightly different names, e.g. cp is rclone copy . RClone is very powerful and, as such, has a wide variety of configuration options and flags to fine tune behavior. We will only cover the basics needed to install the software, setup remotes relevant to work at UAB, and some basic usage commands. Installing \u00b6 Installing on Cheaha \u00b6 On Cheaha, RClone is already installed as a Module . Use module load rclone to load it. Installing on Linux and cloud.rc \u00b6 See Installing Software for general good practice on installing software, then use the following command. curl https://rclone.org/install.sh | sudo bash Open a new terminal and enter rclone to verify installation. Installing on Windows \u00b6 It is highly recommended to install rclone in Windows Subsystem for Linux (WSL). To instead install natively on Windows, you will need to use the following instructions. Download the appropriate version from the downloads page. Extract rclone.exe into a memorable folder on your system. Do not put it into Program Files . In the Start Menu type env and look for the application \"Edit the system Environment Variables\" to open the System Properties dialog. Click the \"Environment Variables...\" button. Under \"User variables for $USER\" find the variable \"Path\". Click \"Path\" to select it. Click the \"Edit...\" button to open a new dialog. Click the \"New\" button. Type in the folder path to rclone.exe as C:/path/to/rclone_folder , modified appropriately. Click the \"OK\" button to confirm and close the dialog box. Click the \"OK\" button to confirm and close the Environment Variables dialog box. Click the \"OK\" button to confirm and close the System Properties dialog box. Verify the installation by typing \"cmd\" in the Start Menu and opening the Command Prompt application. Type rclone and you should see the rclone help text. MacOS \u00b6 Follow the online instructions for installing with brew . Setting up Remotes \u00b6 RClone is capable of interfacing with many remote cloud services, as well as using sftp for connecting two personal computers or servers. We will only cover those cloud services relevant to UAB use. We will not cover how to connect to any other cloud services using RClone. More detailed information is available at the RClone documentation Important Cloud access tokens are always supplied with an expiration date for security reasons. You will need to repeat the setup process periodically to regain access via RClone. Note RClone has an unusual user interface, using alternating red and green blocks to differentiate list items. The colors do not convey any particular meaning beyond differentiation. Setting up an SFTP Remote \u00b6 RClone connects two personal computers or servers using SFTP which is built on SSH, so a lot of these instructions mirror what would be done with an SSH configuration. Generate a Key Pair for use with the remote machine. At the terminal enter rclone config . Follow the prompts to choose sftp . Enter the following values as they come up, using defaults for other values. name> Name of the remote for future reference and configuration host> Remote IP address or cheaha.rc.uab.edu for Cheaha user> The user you will log into on the remote machine key_file> The absolute path to the private key file on the local machine, something like ~/.ssh/private_key_ed25519 key_file_pass> The passphrase used to secure the private key file (optional, but highly recommended) Verify by using rclone lsd <name> . The official docuemntation for rclone sftp is here . Setting up UAB Cloud Remotes \u00b6 The setup process for UAB cloud remotes is generally the same, except for the specifics of authentication. The instruction template is outlined below and will point you to the authentication section specific to each remote when it becomes relevant. As you step through the process, you will ultimately open two terminal windows and a browser window, and will need to copy text between the terminal windows. The first terminal window will be used to setup the RClone cloud remote. The second terminal will be used to authenticate to that cloud service and gain a token that will be passed back to the first terminal. Authentication will happen by logging into the service in a browser window. This setup method is necessary for any machine where a browser is not readily available, such as a cloud.rc virtual machine. To facilitate setup on these machines, the second terminal will be opened on a machine with RClone and a browser. An example of what this setup might look like is given below. Important If you are using RClone in Windows Subsystem for Linux (WSL), you won't be able to open a browser using WSL. Instead, you will need to Install RClone on Windows and use the Windows Command Prompt terminal to use rclone authorize . Open a terminal on the device you wish to authorize to access the chosen cloud service provider using RClone. This terminal will be referred to as terminal-1. At terminal-1 enter rclone config . Follow the prompts to choose one of the following. The selection here will be used later and will be referred to as <remote> . UAB Box: select Box . <remote> will be replaced by box . UAB SharePoint Site: select Microsoft OneDrive . <remote> will be replaced by onedrive . UAB OneDrive: select Microsoft OneDrive . <remote> will be replaced by onedrive . Enter a short, memorable name for future reference when prompted with name> . Keep this <name> in mind as it will be how you access the remote when Using Commands . Press enter to leave all additional prompts blank until \"Use auto config?\". Type \"n\", for no, and press enter. The prompt should now read config_token> . On a machine with a browser, such as your personal computer, open a new terminal and enter rclone authorize \"<remote>\" . Replacing <remote> with the value from step (3). This terminal will be referred to as terminal-2. When the browser window opens, use it to authenticate to your selected service. Authenticate to UAB Box . Authenticate to Microsoft OneDrive . Terminal-2 will print a secret token, which will appear like in the following image. You will need to copy the portion highlighted in the image, between the lines with ---> and <--- . Copy and paste the token from the terminal-2 to terminal-1. Follow the remaining prompts. Verify success by using rclone lsd <name>: in terminal-1. Authenticating to Cloud Remotes \u00b6 Authenticating to UAB Box \u00b6 Click \"Use Single Sign On (SSO)\". Type in your UAB email address (not your @uabmc.edu email!). Click \"Authorize\". You will be redirected to the UAB SSO page. Authenticate with your blazerid credentials. You will be asked to grant permission to the RClone software. Click \"Grant access to Box\" if you want the software to work with Box. If you do not grant permission, you will not be able to use RClone with Box. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Return to Setting up UAB Cloud Remotes . Warning Tokens are set to expire after some time of disuse to decrease risk of a data breach. If your token expires, you can Reconnect to an Existing Remote rather than recreate the remote configuration completely from scratch. Authenticating to Microsoft OneDrive \u00b6 Type in your UAB email address (not your @uabmc.edu email!). Click \"Next\". If prompted, click \"Work or school account\". You will be asked to grant permission to the RClone software. Click \"Accept\" if you want the software to work with OneDrive. If you do not grant permission, you will not be able to use RClone with OneDrive. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Next you will return to the general instructions. Before you do, note that you'll be asked to choose which type of OneDrive service to access. The prompt will look like the image below. For UAB, the two relevant selections will be (1) to access your personal OneDrive space and (3) for a SharePoint Site, e.g. for a lab or department. With your selection in mind, return to Setting up UAB Cloud Remotes . Setting Up an S3 LTS Remote \u00b6 The full S3 configuration process can be done from a single command line terminal. Open a terminal and enter rclone config to begin the configuration process. Note The locations where you will need to input either a command or select an option are preceded with a $ for easier navigation. $ rclone config 2022 /02/22 13 :02:15 NOTICE: Config file \"/home/mdefende/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config # select 'n' to create a new remote $ n/s/q> n # name the new remote $ name> uablts At this point, you've created a new remote configuration called uablts. This will be the remote name used in further commands. You can name the remote whatever you would like, but will need to replace uablts in the instructions with whichever name you chose, if you chose a different name. ... 4 / Amazon Drive \\ ( amazon cloud drive ) 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS \\ ( s3 ) 6 / Backblaze B2 \\ ( b2 ) ... $ Storage> 5 ... 2 / Alibaba Cloud Object Storage System ( OSS ) formerly Aliyun \\ ( Alibaba ) 3 / Ceph Object Storage \\ ( Ceph ) 4 / Digital Ocean Spaces \\ ( DigitalOcean ) ... $ provider> 3 Option env_auth. Get AWS credentials from runtime ( environment variables or EC2/ECS meta data if no env vars ) . Only applies if access_key_id and secret_access_key is blank. Choose a number from below, or type in your own boolean value ( true or false ) . Press Enter for the default ( false ) . 1 / Enter AWS credentials in the next step. \\ ( false ) 2 / Get AWS credentials from the environment ( env vars or IAM ) . \\ ( true ) $ env_auth> 1 ( or leave blank ) Option access_key_id. AWS Access Key ID. Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ access_key_id> ( Enter your access key given to you by research computing ) Option secret_access_key. AWS Secret Access Key ( password ) . Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ secret_access_key> ( Enter your secret access key given to you by research computing here ) Option region. Region to connect to. Leave blank if you are using an S3 clone and you don 't have a region. Choose a number from below, or type in your own value. Press Enter to leave empty. / Use this if unsure. 1 | Will use v4 signatures and an empty region. \\ () / Use this only if v4 signatures don' t work. 2 | E.g. pre Jewel/v10 CEPH. \\ ( other-v2-signature ) $ region> ( Leave empty ) Option endpoint. Endpoint for S3 API. Required when using an S3 clone. Enter a value. Press Enter to leave empty. $ endpoint> s3.lts.rc.uab.edu From here, press Enter to accept default options until it gives you a summary of your connection [ uablts ] type = s3 provider = Ceph access_key_id = ****************** # these will be filled in on your screen secret_access_key = ******************************** endpoint = s3.lts.rc.uab.edu -------------------- y ) Yes this is OK ( default ) e ) Edit this remote d ) Delete this remote y/e/d> Make sure everything looks correct here, then press Enter. At this point, it will bring you back to the main configuration menu. You can choose the Quit Config option and exit back to a basic terminal. Reconnecting to an Existing Remote \u00b6 When your tokens expire, rather than recreate the remote from scratch, simply use the following command with your existing remote <name> . rclone config reconnect <name>: If you already have a token, you will be asked if you want to refresh it. Choose yes if so, then continue. You will be prompted with Use auto config? . If you are on a machine with no access to a browser, respond n , as in the original setup. Follow the steps in the appropriate section under Authenticating to Cloud Remotes , as in the original setup. Usage \u00b6 RClone is a powerful tool with many commands available. We will only cover a small subset of the available commands, as most are beyond typical usage, so please see the RClone documentation for more information. All commands have access to the global flags . An important global flag is --dry-run to show what will happen without actually executing the command, which can be helpful to prevent costly mistakes. Other Helpful Global Flags are also available. The various remotes each have their own individual page with their own specific flags, and are linked in the relevant Setting up Remotes section above. Important Remote paths are always prefixed by the name of the remote like cheaha:/path/to/files . The colon character : is required for all remote paths. Local paths have no prefix like /path/to/local/files . RClone can thus be used between any two machines that are configured where rclone is being used, including from the local machine to itself. In the following instructions, replace <remote:> by the appropriate remote name from configuration. To access local files, leave <remote:> off entirely. Important Remember to use quotes \" around paths with spaces like \"path\\to\\a folder with spaces\" Creating a Directory \u00b6 To create a directory use rclone mkdir <remote:><path> . Example: rclone mkdir box:manuscript . Listing Files and Directories \u00b6 To list files on a machine use rclone ls <remote:><path> . Example rclone ls box: . To list directories on a machine use rclone lsd <remote:><path> . Example: rclone lsd box: should show manuscript . Copying Files \u00b6 To copy files without changing their name, or to recursively copy directory content, use rclone copy <source:><path> <destination:><path> . Note that the directory contents are copied, so when copying a directory, be sure that directory exists on the remote and that you are copying into it. Example: rclone copy \"C:\\users\\Name\\My Documents\" box:manuscript To copy a single file and change its name, use rclone copyto <source:><path/oldname> <destination:><path/newname> . Example rclone copyto \"C:\\users\\Name\\My Documents\\manuscript.docx\" box:manuscript\\newest.docx Syncing Between Two Devices \u00b6 To make a destination directory's contents identical to a source directory, use rclone sync <source:><path> <destination:><path> Example: rclone sync cheaha:\"C:\\users\\Name\\My Documents\" box:manuscript . Danger rclone sync is a destructive operation and cannot be undone! If files exist on the destination that do not exist on the source, then they will be deleted permanently from the destination. To avoid accidental destruction of files use the --immutable flag . Other Helpful Global Flags \u00b6 Flag used with any RClone command are called global flags. Below are some useful global flags. -C or --checksum : Skip syncing files based on checksum instead of last modified time. --dry-run : Show what will happen if the command were executed. No changes are made. --immutable : Do not allow any files to be modified. Helpful to avoid unintended deletions and overwrites. --max-depth <integer> : Only recurse to <integer> depth within directory tree. Using rclone ls --max-depth 1 means only show top-level files in the current directory. -P or --progress : Show progress of command as it runs. --quiet : Print as little as possible. Useful in scripts. -u or --update : Skips files that are newer on the remote.","title":"RClone"},{"location":"data_management/transfer/rclone/#rclone","text":"RClone is a powerful command line tool for transferring and synchronizing files over the internet between various machines, servers and cloud storage services. It is highly recommended for small to moderate amounts of data. For very large amounts of data consider using Globus for increased robustness against failure. Where Globus is not available, rclone is still suitable. RClone requires a modest amount of setup time on local machines, but once setup can be used fairly easily. RClone uses the concepts of \"remotes\", which is an abstract term for any storage service or device that is not physically part of the local machine. Many remotes are offered, including SFTP and various UAB Cloud Storage Solutions . SFTP may be used to access Cheaha, cloud.rc and other laptop and desktop computers. To use RClone effectively, you'll need to setup remotes before using the various commands. Most file manipulation commands on Linux can be found in the RClone commands, but may have slightly different names, e.g. cp is rclone copy . RClone is very powerful and, as such, has a wide variety of configuration options and flags to fine tune behavior. We will only cover the basics needed to install the software, setup remotes relevant to work at UAB, and some basic usage commands.","title":"RClone"},{"location":"data_management/transfer/rclone/#installing","text":"","title":"Installing"},{"location":"data_management/transfer/rclone/#installing-on-cheaha","text":"On Cheaha, RClone is already installed as a Module . Use module load rclone to load it.","title":"Installing on Cheaha"},{"location":"data_management/transfer/rclone/#installing-on-linux-and-cloudrc","text":"See Installing Software for general good practice on installing software, then use the following command. curl https://rclone.org/install.sh | sudo bash Open a new terminal and enter rclone to verify installation.","title":"Installing on Linux and cloud.rc"},{"location":"data_management/transfer/rclone/#installing-on-windows","text":"It is highly recommended to install rclone in Windows Subsystem for Linux (WSL). To instead install natively on Windows, you will need to use the following instructions. Download the appropriate version from the downloads page. Extract rclone.exe into a memorable folder on your system. Do not put it into Program Files . In the Start Menu type env and look for the application \"Edit the system Environment Variables\" to open the System Properties dialog. Click the \"Environment Variables...\" button. Under \"User variables for $USER\" find the variable \"Path\". Click \"Path\" to select it. Click the \"Edit...\" button to open a new dialog. Click the \"New\" button. Type in the folder path to rclone.exe as C:/path/to/rclone_folder , modified appropriately. Click the \"OK\" button to confirm and close the dialog box. Click the \"OK\" button to confirm and close the Environment Variables dialog box. Click the \"OK\" button to confirm and close the System Properties dialog box. Verify the installation by typing \"cmd\" in the Start Menu and opening the Command Prompt application. Type rclone and you should see the rclone help text.","title":"Installing on Windows"},{"location":"data_management/transfer/rclone/#macos","text":"Follow the online instructions for installing with brew .","title":"MacOS"},{"location":"data_management/transfer/rclone/#setting-up-remotes","text":"RClone is capable of interfacing with many remote cloud services, as well as using sftp for connecting two personal computers or servers. We will only cover those cloud services relevant to UAB use. We will not cover how to connect to any other cloud services using RClone. More detailed information is available at the RClone documentation Important Cloud access tokens are always supplied with an expiration date for security reasons. You will need to repeat the setup process periodically to regain access via RClone. Note RClone has an unusual user interface, using alternating red and green blocks to differentiate list items. The colors do not convey any particular meaning beyond differentiation.","title":"Setting up Remotes"},{"location":"data_management/transfer/rclone/#setting-up-an-sftp-remote","text":"RClone connects two personal computers or servers using SFTP which is built on SSH, so a lot of these instructions mirror what would be done with an SSH configuration. Generate a Key Pair for use with the remote machine. At the terminal enter rclone config . Follow the prompts to choose sftp . Enter the following values as they come up, using defaults for other values. name> Name of the remote for future reference and configuration host> Remote IP address or cheaha.rc.uab.edu for Cheaha user> The user you will log into on the remote machine key_file> The absolute path to the private key file on the local machine, something like ~/.ssh/private_key_ed25519 key_file_pass> The passphrase used to secure the private key file (optional, but highly recommended) Verify by using rclone lsd <name> . The official docuemntation for rclone sftp is here .","title":"Setting up an SFTP Remote"},{"location":"data_management/transfer/rclone/#setting-up-uab-cloud-remotes","text":"The setup process for UAB cloud remotes is generally the same, except for the specifics of authentication. The instruction template is outlined below and will point you to the authentication section specific to each remote when it becomes relevant. As you step through the process, you will ultimately open two terminal windows and a browser window, and will need to copy text between the terminal windows. The first terminal window will be used to setup the RClone cloud remote. The second terminal will be used to authenticate to that cloud service and gain a token that will be passed back to the first terminal. Authentication will happen by logging into the service in a browser window. This setup method is necessary for any machine where a browser is not readily available, such as a cloud.rc virtual machine. To facilitate setup on these machines, the second terminal will be opened on a machine with RClone and a browser. An example of what this setup might look like is given below. Important If you are using RClone in Windows Subsystem for Linux (WSL), you won't be able to open a browser using WSL. Instead, you will need to Install RClone on Windows and use the Windows Command Prompt terminal to use rclone authorize . Open a terminal on the device you wish to authorize to access the chosen cloud service provider using RClone. This terminal will be referred to as terminal-1. At terminal-1 enter rclone config . Follow the prompts to choose one of the following. The selection here will be used later and will be referred to as <remote> . UAB Box: select Box . <remote> will be replaced by box . UAB SharePoint Site: select Microsoft OneDrive . <remote> will be replaced by onedrive . UAB OneDrive: select Microsoft OneDrive . <remote> will be replaced by onedrive . Enter a short, memorable name for future reference when prompted with name> . Keep this <name> in mind as it will be how you access the remote when Using Commands . Press enter to leave all additional prompts blank until \"Use auto config?\". Type \"n\", for no, and press enter. The prompt should now read config_token> . On a machine with a browser, such as your personal computer, open a new terminal and enter rclone authorize \"<remote>\" . Replacing <remote> with the value from step (3). This terminal will be referred to as terminal-2. When the browser window opens, use it to authenticate to your selected service. Authenticate to UAB Box . Authenticate to Microsoft OneDrive . Terminal-2 will print a secret token, which will appear like in the following image. You will need to copy the portion highlighted in the image, between the lines with ---> and <--- . Copy and paste the token from the terminal-2 to terminal-1. Follow the remaining prompts. Verify success by using rclone lsd <name>: in terminal-1.","title":"Setting up UAB Cloud Remotes"},{"location":"data_management/transfer/rclone/#authenticating-to-cloud-remotes","text":"","title":"Authenticating to Cloud Remotes"},{"location":"data_management/transfer/rclone/#authenticating-to-uab-box","text":"Click \"Use Single Sign On (SSO)\". Type in your UAB email address (not your @uabmc.edu email!). Click \"Authorize\". You will be redirected to the UAB SSO page. Authenticate with your blazerid credentials. You will be asked to grant permission to the RClone software. Click \"Grant access to Box\" if you want the software to work with Box. If you do not grant permission, you will not be able to use RClone with Box. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Return to Setting up UAB Cloud Remotes . Warning Tokens are set to expire after some time of disuse to decrease risk of a data breach. If your token expires, you can Reconnect to an Existing Remote rather than recreate the remote configuration completely from scratch.","title":"Authenticating to UAB Box"},{"location":"data_management/transfer/rclone/#authenticating-to-microsoft-onedrive","text":"Type in your UAB email address (not your @uabmc.edu email!). Click \"Next\". If prompted, click \"Work or school account\". You will be asked to grant permission to the RClone software. Click \"Accept\" if you want the software to work with OneDrive. If you do not grant permission, you will not be able to use RClone with OneDrive. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Next you will return to the general instructions. Before you do, note that you'll be asked to choose which type of OneDrive service to access. The prompt will look like the image below. For UAB, the two relevant selections will be (1) to access your personal OneDrive space and (3) for a SharePoint Site, e.g. for a lab or department. With your selection in mind, return to Setting up UAB Cloud Remotes .","title":"Authenticating to Microsoft OneDrive"},{"location":"data_management/transfer/rclone/#setting-up-an-s3-lts-remote","text":"The full S3 configuration process can be done from a single command line terminal. Open a terminal and enter rclone config to begin the configuration process. Note The locations where you will need to input either a command or select an option are preceded with a $ for easier navigation. $ rclone config 2022 /02/22 13 :02:15 NOTICE: Config file \"/home/mdefende/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config # select 'n' to create a new remote $ n/s/q> n # name the new remote $ name> uablts At this point, you've created a new remote configuration called uablts. This will be the remote name used in further commands. You can name the remote whatever you would like, but will need to replace uablts in the instructions with whichever name you chose, if you chose a different name. ... 4 / Amazon Drive \\ ( amazon cloud drive ) 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS \\ ( s3 ) 6 / Backblaze B2 \\ ( b2 ) ... $ Storage> 5 ... 2 / Alibaba Cloud Object Storage System ( OSS ) formerly Aliyun \\ ( Alibaba ) 3 / Ceph Object Storage \\ ( Ceph ) 4 / Digital Ocean Spaces \\ ( DigitalOcean ) ... $ provider> 3 Option env_auth. Get AWS credentials from runtime ( environment variables or EC2/ECS meta data if no env vars ) . Only applies if access_key_id and secret_access_key is blank. Choose a number from below, or type in your own boolean value ( true or false ) . Press Enter for the default ( false ) . 1 / Enter AWS credentials in the next step. \\ ( false ) 2 / Get AWS credentials from the environment ( env vars or IAM ) . \\ ( true ) $ env_auth> 1 ( or leave blank ) Option access_key_id. AWS Access Key ID. Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ access_key_id> ( Enter your access key given to you by research computing ) Option secret_access_key. AWS Secret Access Key ( password ) . Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ secret_access_key> ( Enter your secret access key given to you by research computing here ) Option region. Region to connect to. Leave blank if you are using an S3 clone and you don 't have a region. Choose a number from below, or type in your own value. Press Enter to leave empty. / Use this if unsure. 1 | Will use v4 signatures and an empty region. \\ () / Use this only if v4 signatures don' t work. 2 | E.g. pre Jewel/v10 CEPH. \\ ( other-v2-signature ) $ region> ( Leave empty ) Option endpoint. Endpoint for S3 API. Required when using an S3 clone. Enter a value. Press Enter to leave empty. $ endpoint> s3.lts.rc.uab.edu From here, press Enter to accept default options until it gives you a summary of your connection [ uablts ] type = s3 provider = Ceph access_key_id = ****************** # these will be filled in on your screen secret_access_key = ******************************** endpoint = s3.lts.rc.uab.edu -------------------- y ) Yes this is OK ( default ) e ) Edit this remote d ) Delete this remote y/e/d> Make sure everything looks correct here, then press Enter. At this point, it will bring you back to the main configuration menu. You can choose the Quit Config option and exit back to a basic terminal.","title":"Setting Up an S3 LTS Remote"},{"location":"data_management/transfer/rclone/#reconnecting-to-an-existing-remote","text":"When your tokens expire, rather than recreate the remote from scratch, simply use the following command with your existing remote <name> . rclone config reconnect <name>: If you already have a token, you will be asked if you want to refresh it. Choose yes if so, then continue. You will be prompted with Use auto config? . If you are on a machine with no access to a browser, respond n , as in the original setup. Follow the steps in the appropriate section under Authenticating to Cloud Remotes , as in the original setup.","title":"Reconnecting to an Existing Remote"},{"location":"data_management/transfer/rclone/#usage","text":"RClone is a powerful tool with many commands available. We will only cover a small subset of the available commands, as most are beyond typical usage, so please see the RClone documentation for more information. All commands have access to the global flags . An important global flag is --dry-run to show what will happen without actually executing the command, which can be helpful to prevent costly mistakes. Other Helpful Global Flags are also available. The various remotes each have their own individual page with their own specific flags, and are linked in the relevant Setting up Remotes section above. Important Remote paths are always prefixed by the name of the remote like cheaha:/path/to/files . The colon character : is required for all remote paths. Local paths have no prefix like /path/to/local/files . RClone can thus be used between any two machines that are configured where rclone is being used, including from the local machine to itself. In the following instructions, replace <remote:> by the appropriate remote name from configuration. To access local files, leave <remote:> off entirely. Important Remember to use quotes \" around paths with spaces like \"path\\to\\a folder with spaces\"","title":"Usage"},{"location":"data_management/transfer/rclone/#creating-a-directory","text":"To create a directory use rclone mkdir <remote:><path> . Example: rclone mkdir box:manuscript .","title":"Creating a Directory"},{"location":"data_management/transfer/rclone/#listing-files-and-directories","text":"To list files on a machine use rclone ls <remote:><path> . Example rclone ls box: . To list directories on a machine use rclone lsd <remote:><path> . Example: rclone lsd box: should show manuscript .","title":"Listing Files and Directories"},{"location":"data_management/transfer/rclone/#copying-files","text":"To copy files without changing their name, or to recursively copy directory content, use rclone copy <source:><path> <destination:><path> . Note that the directory contents are copied, so when copying a directory, be sure that directory exists on the remote and that you are copying into it. Example: rclone copy \"C:\\users\\Name\\My Documents\" box:manuscript To copy a single file and change its name, use rclone copyto <source:><path/oldname> <destination:><path/newname> . Example rclone copyto \"C:\\users\\Name\\My Documents\\manuscript.docx\" box:manuscript\\newest.docx","title":"Copying Files"},{"location":"data_management/transfer/rclone/#syncing-between-two-devices","text":"To make a destination directory's contents identical to a source directory, use rclone sync <source:><path> <destination:><path> Example: rclone sync cheaha:\"C:\\users\\Name\\My Documents\" box:manuscript . Danger rclone sync is a destructive operation and cannot be undone! If files exist on the destination that do not exist on the source, then they will be deleted permanently from the destination. To avoid accidental destruction of files use the --immutable flag .","title":"Syncing Between Two Devices"},{"location":"data_management/transfer/rclone/#other-helpful-global-flags","text":"Flag used with any RClone command are called global flags. Below are some useful global flags. -C or --checksum : Skip syncing files based on checksum instead of last modified time. --dry-run : Show what will happen if the command were executed. No changes are made. --immutable : Do not allow any files to be modified. Helpful to avoid unintended deletions and overwrites. --max-depth <integer> : Only recurse to <integer> depth within directory tree. Using rclone ls --max-depth 1 means only show top-level files in the current directory. -P or --progress : Show progress of command as it runs. --quiet : Print as little as possible. Useful in scripts. -u or --update : Skips files that are newer on the remote.","title":"Other Helpful Global Flags"},{"location":"education/case_studies/","text":"Case Studies \u00b6 Construction This page is a stub and is under construction. Parabricks \u00b6 How to Run a Millions of Jobs on Open Science Grid \u00b6","title":"Case Studies"},{"location":"education/case_studies/#case-studies","text":"Construction This page is a stub and is under construction.","title":"Case Studies"},{"location":"education/case_studies/#parabricks","text":"","title":"Parabricks"},{"location":"education/case_studies/#how-to-run-a-millions-of-jobs-on-open-science-grid","text":"","title":"How to Run a Millions of Jobs on Open Science Grid"},{"location":"education/courses/","text":"Courses \u00b6 Data Science Journal Club Course \u00b6 Intended for students interested in learning more about data science and its applications to research, the course is 1 credit hour pass/fail and offered every session (Fall, Spring, Summer). The course is listed as GBSC 720-VTR JC- Data Science Club and sometimes also as IDNE 790-VTR JC- Data Science Club . Students are largely expected to lead their own journey, with instructors facilitating and offering advice. We expect a good-faith effort to learn and grow in the course and the course has most value when students step outside their comfort zones. There are three student-led demonstrations required to pass, where we expect students to demonstrate how they have learned and grown from their exploration of the material. Some of the topics covered by past students include: Learning to use High Performance Computing (HPC) resources Learning software development good practices Learning git and github Learning data organization good practices Jupyter notebooks R Markdown notebooks Data visualization Data analytics Data processing pipelines Machine learning Deep learning The only prerequisite for the course is feeling comfortable using a computer. We are flexible with topics and welcome students of all levels of data science experience. If you have questions about the course please feel free to reach out to Support .","title":"Offered Courses"},{"location":"education/courses/#courses","text":"","title":"Courses"},{"location":"education/courses/#data-science-journal-club-course","text":"Intended for students interested in learning more about data science and its applications to research, the course is 1 credit hour pass/fail and offered every session (Fall, Spring, Summer). The course is listed as GBSC 720-VTR JC- Data Science Club and sometimes also as IDNE 790-VTR JC- Data Science Club . Students are largely expected to lead their own journey, with instructors facilitating and offering advice. We expect a good-faith effort to learn and grow in the course and the course has most value when students step outside their comfort zones. There are three student-led demonstrations required to pass, where we expect students to demonstrate how they have learned and grown from their exploration of the material. Some of the topics covered by past students include: Learning to use High Performance Computing (HPC) resources Learning software development good practices Learning git and github Learning data organization good practices Jupyter notebooks R Markdown notebooks Data visualization Data analytics Data processing pipelines Machine learning Deep learning The only prerequisite for the course is feeling comfortable using a computer. We are flexible with topics and welcome students of all levels of data science experience. If you have questions about the course please feel free to reach out to Support .","title":"Data Science Journal Club Course"},{"location":"education/research_computing_days/","text":"UAB Research Computing Day \u00b6 Research Computing Day is a dialog within the UAB research communityabout leveraging the power of computers to grow the depth of our investigation into the nature of the world that surrounds us. The annual event welcomes discussions on science, engineering, the arts and humanities focused on the drive to open new research frontiers with advances in technology. Whether computers are used to increase the accuracy of a model, interpret the ever-growing stream of data from new image collections and instruments, or engage with peers around the globe, UAB's status as a leading research community depends on the ability to incorporate these capabilities into the research process. By participating in the dialog of Research Computing Day at UAB, researchers can share how they are using these methods to enhance their research, gain new insights from peers, and contribute their voices to the growth of research at UAB. Background \u00b6 Since 2007, The Office of the Vice President for Information Technology has sponsored an annual dialog on the role of technology in research. These events joined UAB with national dialogs on the role of Cyberinfrastructure in research held at campuses across the country. Previous UAB Research Computing Days \u00b6 2007 -- Co-hosted along with the ASA site visit, providing an overview of new services and upcoming launch of the UABgrid pilot. (No web record) 2008 -- Focus on grid computing and collaboration technologies, in particular the caBIG program with guest speakers from Booz Allen Hamilton who managed the NCI caBIG program and SURA (agenda currently offline) 2010 -- Featured introduction to Galaxy platform for genetic sequencing by Dell staff scientist (agenda currently offline) 2011 -- Understanding growth of research computing support at peer institutions UNC and Emory 2012 -- Growing data sciences at UAB 2013 -- OpenStack at UAB 2016 -- HPC Expansion 2017 -- GPU expansion Research Computing Day 2018","title":"Research Computing Days"},{"location":"education/research_computing_days/#uab-research-computing-day","text":"Research Computing Day is a dialog within the UAB research communityabout leveraging the power of computers to grow the depth of our investigation into the nature of the world that surrounds us. The annual event welcomes discussions on science, engineering, the arts and humanities focused on the drive to open new research frontiers with advances in technology. Whether computers are used to increase the accuracy of a model, interpret the ever-growing stream of data from new image collections and instruments, or engage with peers around the globe, UAB's status as a leading research community depends on the ability to incorporate these capabilities into the research process. By participating in the dialog of Research Computing Day at UAB, researchers can share how they are using these methods to enhance their research, gain new insights from peers, and contribute their voices to the growth of research at UAB.","title":"UAB Research Computing Day"},{"location":"education/research_computing_days/#background","text":"Since 2007, The Office of the Vice President for Information Technology has sponsored an annual dialog on the role of technology in research. These events joined UAB with national dialogs on the role of Cyberinfrastructure in research held at campuses across the country.","title":"Background"},{"location":"education/research_computing_days/#previous-uab-research-computing-days","text":"2007 -- Co-hosted along with the ASA site visit, providing an overview of new services and upcoming launch of the UABgrid pilot. (No web record) 2008 -- Focus on grid computing and collaboration technologies, in particular the caBIG program with guest speakers from Booz Allen Hamilton who managed the NCI caBIG program and SURA (agenda currently offline) 2010 -- Featured introduction to Galaxy platform for genetic sequencing by Dell staff scientist (agenda currently offline) 2011 -- Understanding growth of research computing support at peer institutions UNC and Emory 2012 -- Growing data sciences at UAB 2013 -- OpenStack at UAB 2016 -- HPC Expansion 2017 -- GPU expansion Research Computing Day 2018","title":"Previous UAB Research Computing Days"},{"location":"education/social_media/","text":"Social Media \u00b6 Construction This page is a stub and is under construction. GitHub \u00b6 https://github.com/uabrc YouTube \u00b6 https://www.youtube.com/channel/UCZoOS2e699Ge0DND1oy1BJQ","title":"Social Media"},{"location":"education/social_media/#social-media","text":"Construction This page is a stub and is under construction.","title":"Social Media"},{"location":"education/social_media/#github","text":"https://github.com/uabrc","title":"GitHub"},{"location":"education/social_media/#youtube","text":"https://www.youtube.com/channel/UCZoOS2e699Ge0DND1oy1BJQ","title":"YouTube"},{"location":"grants/budgets/","text":"Grant Budgets \u00b6 Construction This page is a stub and is under construction. Research Computing Buy-in Program \u00b6 Computation \u00b6 Additional Storage \u00b6","title":"Budgeting"},{"location":"grants/budgets/#grant-budgets","text":"Construction This page is a stub and is under construction.","title":"Grant Budgets"},{"location":"grants/budgets/#research-computing-buy-in-program","text":"","title":"Research Computing Buy-in Program"},{"location":"grants/budgets/#computation","text":"","title":"Computation"},{"location":"grants/budgets/#additional-storage","text":"","title":"Additional Storage"},{"location":"grants/facilities/","text":"UAB Cyberinfrastructure Plan and Grant Facilities Document \u00b6 Construction This page is a stub and is under construction. Click Here To Download The Research Computing Facilities Grant Document. Please feel free to use this document to support your grant applications.","title":"Facilities and Plan"},{"location":"grants/facilities/#uab-cyberinfrastructure-plan-and-grant-facilities-document","text":"Construction This page is a stub and is under construction. Click Here To Download The Research Computing Facilities Grant Document. Please feel free to use this document to support your grant applications.","title":"UAB Cyberinfrastructure Plan and Grant Facilities Document"},{"location":"grants/overview/","text":"Overview of Research Computing Grant Resources \u00b6 Construction This page is a stub and is under construction.","title":"Overview"},{"location":"grants/overview/#overview-of-research-computing-grant-resources","text":"Construction This page is a stub and is under construction.","title":"Overview of Research Computing Grant Resources"},{"location":"grants/publications/","text":"Grant Descriptions \u00b6 Short \u00b6 UAB IT Research Computing maintains high performance compute (HPC) and storage resources for investigators. The Cheaha high performance compute cluster provides 8192 CPU cores and 72 GPUs interconnected via an InfiniBand network, providing over 619 TFLOP/s of aggregate theoretical peak performance. A high-performance, 12PB raw GPFS storage on DDN SFA12KX hardware is also connected to these compute nodes via the Infiniband fabric, available to all UAB investigators. Detailed \u00b6 Note Under construction. Acknoweldgement in Publications \u00b6 To acknowledge the use of Cheaha in published work, for compute time or substantial technical assistance, please consider adding the following to the acknowledgements section of your publication: The authors gratefully acknowledge the resources provided by the University of Alabama at Birmingham IT-Research Computing group for high performance computing (HPC) support and CPU time on the Cheaha compute cluster. If Globus was used to transfer data to/from Cheaha, please consider adding the following to the acknowledgements section of your publication: This work was supported in part by the National Science Foundation under Grants Nos. OAC-1541310, the University of Alabama at Birmingham, and the Alabama Innovation Fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the University of Alabama at Birmingham. Detailed Hardware Information \u00b6 For more detailed information on compute hardware please see: Detailed Hardware Information","title":"Descriptions"},{"location":"grants/publications/#grant-descriptions","text":"","title":"Grant Descriptions"},{"location":"grants/publications/#short","text":"UAB IT Research Computing maintains high performance compute (HPC) and storage resources for investigators. The Cheaha high performance compute cluster provides 8192 CPU cores and 72 GPUs interconnected via an InfiniBand network, providing over 619 TFLOP/s of aggregate theoretical peak performance. A high-performance, 12PB raw GPFS storage on DDN SFA12KX hardware is also connected to these compute nodes via the Infiniband fabric, available to all UAB investigators.","title":"Short"},{"location":"grants/publications/#detailed","text":"Note Under construction.","title":"Detailed"},{"location":"grants/publications/#acknoweldgement-in-publications","text":"To acknowledge the use of Cheaha in published work, for compute time or substantial technical assistance, please consider adding the following to the acknowledgements section of your publication: The authors gratefully acknowledge the resources provided by the University of Alabama at Birmingham IT-Research Computing group for high performance computing (HPC) support and CPU time on the Cheaha compute cluster. If Globus was used to transfer data to/from Cheaha, please consider adding the following to the acknowledgements section of your publication: This work was supported in part by the National Science Foundation under Grants Nos. OAC-1541310, the University of Alabama at Birmingham, and the Alabama Innovation Fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the University of Alabama at Birmingham.","title":"Acknoweldgement in Publications"},{"location":"grants/publications/#detailed-hardware-information","text":"For more detailed information on compute hardware please see: Detailed Hardware Information","title":"Detailed Hardware Information"},{"location":"help/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Below are some common issues, their most common causes, and their most common resolutions. If after checking for your issue below, please feel free to reach out to us at support@listserv.uab.edu . Cheaha \u00b6 Authentication Issues \u00b6 Why is my account on hold? \u00b6 There are a number of reasons your account was placed on hold. Please see Account on Hold for more information. Why is my account not authorized? \u00b6 There are a number of reasons your account may not be authorized. Please see Account Authorization Error for more information. Common Issues \u00b6 Why is my job taking a long time to start? \u00b6 There are a few common reasons why a job can take a long time to start. These apply to jobs on the Open OnDemand web portal, and at the terminal. There is a long queue wait time. If possible, you can move ahead in the queue by canceling and resubmitting your job with fewer requested resources. Your job is requesting more resources than are available or allowed. Double check our resource limits for various situations: Requesting many jobs at once? Our Partitions table lists global quotas for each partition. When these limits are reached, no more jobs will be started until some resources are freed from other jobs. Requesting jobs on a single node? Our Node Summary table lists resource limits for each partition. These are physical limits of available hardware, and can't be exceeded. There is a system outage. If this is the case, information will be distributed by Research Computing. Note Under construction. Why do I get an error when I try to submit an Open OnDemand interactive job? \u00b6 If you are seeing a \"Failed to submit session\" error in a red box when you submit an interactive job on Open OnDemand, like the following, please try reading the error text first. If the cause of the error is not obvious from the text, please read on for some common causes. If none of the below common causes apply, please contact Support . If the error text starts with Disk quota exceeded then your storage space in /data/user/$USER is likely full. Please see Storage for more information on options for alternative storage locations, or contact Support . Why do I get an error when I try to launch an Open OnDemand HPC Interactive session? \u00b6 If you are seeing an error like the following when launching an HPC Interactive job, please read on for the most likely solution. The most common cause is that the command conda init was used, creating a block in your .bashrc file that looks like the following section. # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup = \" $( '/home/<YOUR_USER>/miniconda3/bin/conda' 'shell.bash' 'hook' 2 > /dev/null ) \" if [ $? -eq 0 ] ; then eval \" $__conda_setup \" else if [ -f \"/home/<YOUR_USER>/miniconda3/etc/profile.d/conda.sh\" ] ; then . \"/home/<YOUR_USER>/miniconda3/etc/profile.d/conda.sh\" else export PATH = \"/home/<YOUR_USER>/miniconda3/bin: $PATH \" fi fi unset __conda_setup # <<< conda initialize <<< Anaconda is managed as a Module on Cheaha , so it is unnecessary to use conda init , and can cause issues with Open OnDemand . To avoid the issue reoccurring, please do not use conda init on Cheaha. To resolve this issue, you'll need to locate the hidden file .bashrc in the file browser pane and select it. Click the \"Edit\" button. In the new tab that opens, delete the section shown above and click \"Save\". Please try to launch your job again. If it still doesn't work, please contact us Why do I get a gray screen after launching my Open OnDemand RStudio Server session? \u00b6 Most commonly, delays in launching Open OnDemand RStudio Server are caused by a large quantities of files referenced in the .rstudio directory in the home directory. This can also commonly occur when a large amount of data was loaded in the workspace in your previous RStudio session. To resolve the issue, you'll need to locate the hidden directory .rstudio in your $HOME folder. Once you've found it, delete it, then try recreating your job. Warning Deleting the .rstudio directory will delete the workspace you had open, including any scripts that were unsaved. Saved scripts will be fine. Why is my Open OnDemand RStudio Server session giving me a 502 or 503 error? \u00b6 See here . Why is my Anaconda environment not appearing in my Jupyter Notebook kernels? \u00b6 See here . What is a segfault error? \u00b6 A segfault, or segmentation fault, often causes unexpected termination of software. These errors are caused by software attempting to access memory that it isn't allowed to access. An error typically includes a long string of hexadecimal characters like 0x000055ea4064c135 , but with different values, and one or more of segmentation fault , segfault and/or SIGSEGV . These issues are most commonly caused by programming errors, but can be related to out-of-memory errors. If you encounter a segmentation fault, first try increasing the memory requested for the job. It may help to learn more about SLURM and job efficiency . What is a bus error? \u00b6 A bus error is almost always related to a process requesting more memory than is available. In other words, the job ran out of memory. The simplest solution is to increase the requested memory for the job. It may help to learn more about SLURM and job efficiency . I copied a script from a Windows machine to Cheaha and am getting a bad interpreter error \u00b6 Bad interpreter errors usually mean that there are symbols in the script that the shell interpreter cannot read correctly. This is usually due to the difference in newline symbols used in Windows ( \\r\\n ) and Unix ( \\n ). Once these newline symbols are changed, the script should be able to run on Cheaha. To convert a script written in Windows to be compatible on Linux, use the dos2unix command. Just give it the name of the script to convert and it should convert it correctly. dos2unix example_script.sh If you are using development tools like Notepad++ or VSCode , there are built-in ways to remove the carriage returns. NP++: Go to Edit >> EOL Conversion >> Unix (LF). This must be done for individual files. VSCode: Open Settings and search for EoL. Change the default EoL character to \\n . This will be applied globally if changed in your User Options. Why am I seeing an InvalidAccount error when submitting jobs when I could submit jobs fine previously? \u00b6 If you see this error when submitting jobs, please contact support . There are a few causes for this error that will need to be investigated. Why can't Tensorflow find a GPU on Cheaha? \u00b6 When using Tensorflow on Cheaha, if you see an error like Not creating XLA devices, tf_xla_enable_xla_devices not set or are unable to find the GPU, check the following. Ensure you are correctly requesting GPUs with the SLURM flag --gres=gpu:1 . Change 1 to the appropriate number of GPUs needed. Check our Hardware page for limits. Ensure you are loading the appropriate CUDA toolkit using e.g. module load cuda11.2/toolkit/11.1.2 . You can check which modules are available using module avail toolkit at the terminal. Be sure you are loading the correct module. To check which module is required for your version of Tensorflow, see the toolkit requirements chart here https://www.tensorflow.org/install/source#gpu . Issues in Progress \u00b6 This section gives workarounds for some issues that are currently being investigated by the team. Once the issue has been resolved, the section will be updated with the permanent fix. Why are my R Packages Not Installing Correctly Using the RStudio App \u00b6 When using the RStudio Server app (the non-deprecated version) on Open OnDemand , some packages that need C++ compilation will not install correctly when run directly from either the RStudio console or the Install Packages button. As a temporary workaround, you can install the packages from the terminal inside RStudio. Select the Terminal tab next to the Console tab Load the same R module you loaded when requesting the session using the module load R/<insert version> command Start R using the R command Install the packages you need using R's install.packages command (or whichever other package installer you need, such as BiocManager::install ) For some people, the utils package may not be loaded initially causing R to not be able to find the install.packages command. Use library(utils) first, then use install.packages . Once your packages are installed, close the terminal by clicking the dropdown and selecting Close Terminal . This will free the resources the terminal R session was using for RStudio to be able to use again. Your R packages should now be available for use. My script uses a Singularity container and seems to freeze until it times out \u00b6 When using Singularity containers in scripts and the amount of memory used by the container exceeds the amount requested for the job, the script will hang until the time limit is reached. Typically, scripts are immediately killed when memory used exceeds memory requested, but that is not the behavior with containers. If your scripts using containers seem to hang and not produce anything until the job ends, use the seff command to investigate the memory usage after the job ends (see it's entry in our docs ). If it is using the full amount of memory requested, try upping the amount of memory and submit the job again. To find the job number for past jobs, look further down . How To \u00b6 How do I locate hidden dot files on Cheaha? \u00b6 Dot files are files or directories that start with the character . as in the file .bashrc or the .conda directory. If you are using Open OnDemand, please navigate to https://rc.uab.edu , login and click the Files dropdown in the top navigation menu. Click \"Home Directory\". More detailed instructions on the file browser can be found at Open OnDemand Files . Once you've opened the file browser, check the checkbox labeled \"Show Dotfiles\" in the top-right of the page. If you are using the terminal, you'll need to use cd to navigate to the appropriate directory and enter use ls with the -a flag to list all files and directories, including hidden ones. How do I find my Job ID number? \u00b6 All jobs submitted to Slurm on Cheaha are assigned a unique identifier called the Job ID or jobid . Using Open OnDemand ? If the job is still running see My Interactive Sessions to locate the Job ID. If the job is no longer running, or the job card is missing, use the last bullet point for sacct usage. Using the terminal? Just submitted the job? Using srun <flags> --pty /bin/bash ? If you are still in the job context, type echo $SLURM_JOB_ID . Otherwise use the final bullet point for sacct usage. Using srun without --pty ? Use the final bullet point for sacct usage. Using sbatch ? The Job ID is printed to the command line automatically on submission. Job still running? Use squeue -u $USER to show all your running jobs. Job not running? Use the final bullet point sacct usage. How to use sacct to find a job. If you know the date of the job, use the following command at the terminal. The date following -s is the start date, choose the same day the job was submitted. The following -e is the end date, choose the day after -s . You will be provided with a list of jobs run on the -s day, and can sift through them to find your job. sacct -u $USER -s YYYY-MM-DD -e YYYY-MM-DD Otherwise contact Support for assistance. How do I find my Open OnDemand Session Information? \u00b6 All jobs created using Open OnDemand will create a folder in your home directory containing diagnostic information about those jobs. This information can be helpful for diagnosing issues when using Open OnDemand. To review the folder contents, click the Session ID link in the job card under the My Interactive Sessions of Open OnDemand. If asked, the simplest way to give us the information is the following. Please visit the File Browser page for a visual guide to the features of the Open OnDemand File Browser, which will help with the following instructions. Please also visit the My Interactive Sessions page to familiarize yourself with job cards. Find the card for the appropriate job. Click the Session ID link of the card from (1) to open the File Browser . Make note of the Session ID value, which should be something like bbd7ad60-a37c-47aa-a8fb-d7ca6cf39b3c but with different characters. In the \"Working Directory\" section of the interface, click the parent folder name to go up one directory level. In the \"File List\" locate the folder with the matching Session ID from step (2). Click that folder once to select it. Click the \"Download\" button in the \"File Command Menu\" to download a zipped copy of the folder and its contents. Pass the downloaded Zip file to Support . UAB Cloud \u00b6 Why is my instance failing to start? \u00b6 Please see Instances Failing to Start . Why does my instance show an error in the Instances table? \u00b6 Please see Instances Failing to Start . How do I regain access to an instance? \u00b6 If you've lose the only copy of your private key for accessing a cloud.rc virtual machine (VM), there are a few options. Is your instance volume backed (the default behavior)? No : Then your instance is stateless and can be simply be deleted and restarted. Yes : Will your volume be deleted when the instance is deleted (not default behavior)? Yes : Take a snapshot of your current instance to create an image , then delete your instance and create from image . No : Delete your instance and create an instance from the existing volume. Globus \u00b6 Why can't other users access data I transferred to a project space on Cheaha? \u00b6 When transferring using Globus, file permissions from the source file are not transferred with the file due to limitations purposefully placed by Globus. When transferring to any space on Cheaha, the most likely permission sets are user read and write only with no group permissions. Researchers will need to manually change permissions for the files they transfer using Globus to add group permissions if desired. You can use the chmod command for this: # If you transferred a single file, you can do this chmod g+rw <path/to/file> # If you transferred a directory and need to give permissions to all files in the directory, you can use this chmod -R g+rw <path/to/file> The g+rw adds group read and write permissions. You can add only read permissions by omitting the w if desired. For more information on Globus file permissions, you can read their FAQ entry . Long-Term Storage \u00b6 Why do I need to add the trailing / to the end of path names in my S3 commands? \u00b6 In the Unix file system, the / is a protected character the specifies a branch in the file tree, and so / cannot be used in the name of a file or folder. This is not the case for S3 storage. Everything in S3 storage is stored at the top level of the bucket no matter the source's file and folder structure. However, people have become so used to having a folder structure and how it provides an organization system, so S3 interfaces pretend to have one. When a folder is uploaded, the files in the folder will have that folder name appended to the beginning of the file name along with the / separator. Same for if files are uploaded to existing \"folders\" in S3. These paths are called prefixes in S3 terms. Because / is not protected in S3, all of the / characters in a path are included with the prefix, so if you want to perform a command on a prefix in S3, you must include the trailing / because it is actually a part of the prefix name. GitLab \u00b6 Why am I not able to remove researchers from repositories? \u00b6 If you are not able to remove one or more researchers from a given repository, please check the following. Individual researchers can be members of a group, of a subgroup, of a repository inside groups and subgroups, or any combination. Be aware that GitLab repositories can inherity membership from groups. Is the affected repository part of a group or subgroup? If yes, try removing the researcher from the group or subgroup. Work your way up any nested subgroups if it continues to fail. If no, please contact Support . Miscellaneous \u00b6 Shell Commands \u00b6 Why do my shell commands not work as expected? \u00b6 There are a number of possible causes for shell commands not working as expected. Ensure that you are in the expected environment. Make sure you are using Cheaha, cloud.rc, or your local machine, as appropriate. Ensure that you have any needed modules loaded or environment variables set. Be very careful when copy-pasting commands from rich-text sources. Some sources can have invisible, special formatting characters that can corrupt the pasted command. Be sure to use plain-text editors like Notepad++ or VSCode, instead of rich-text editors, when storing commands for future use. Some examples of rich-text sources include: Websites and HTML Word, Excel, PowerPoint, etc. PDF documents","title":"FAQ"},{"location":"help/faq/#frequently-asked-questions-faq","text":"Below are some common issues, their most common causes, and their most common resolutions. If after checking for your issue below, please feel free to reach out to us at support@listserv.uab.edu .","title":"Frequently Asked Questions (FAQ)"},{"location":"help/faq/#cheaha","text":"","title":"Cheaha"},{"location":"help/faq/#authentication-issues","text":"","title":"Authentication Issues"},{"location":"help/faq/#why-is-my-account-on-hold","text":"There are a number of reasons your account was placed on hold. Please see Account on Hold for more information.","title":"Why is my account on hold?"},{"location":"help/faq/#why-is-my-account-not-authorized","text":"There are a number of reasons your account may not be authorized. Please see Account Authorization Error for more information.","title":"Why is my account not authorized?"},{"location":"help/faq/#common-issues","text":"","title":"Common Issues"},{"location":"help/faq/#why-is-my-job-taking-a-long-time-to-start","text":"There are a few common reasons why a job can take a long time to start. These apply to jobs on the Open OnDemand web portal, and at the terminal. There is a long queue wait time. If possible, you can move ahead in the queue by canceling and resubmitting your job with fewer requested resources. Your job is requesting more resources than are available or allowed. Double check our resource limits for various situations: Requesting many jobs at once? Our Partitions table lists global quotas for each partition. When these limits are reached, no more jobs will be started until some resources are freed from other jobs. Requesting jobs on a single node? Our Node Summary table lists resource limits for each partition. These are physical limits of available hardware, and can't be exceeded. There is a system outage. If this is the case, information will be distributed by Research Computing. Note Under construction.","title":"Why is my job taking a long time to start?"},{"location":"help/faq/#why-do-i-get-an-error-when-i-try-to-submit-an-open-ondemand-interactive-job","text":"If you are seeing a \"Failed to submit session\" error in a red box when you submit an interactive job on Open OnDemand, like the following, please try reading the error text first. If the cause of the error is not obvious from the text, please read on for some common causes. If none of the below common causes apply, please contact Support . If the error text starts with Disk quota exceeded then your storage space in /data/user/$USER is likely full. Please see Storage for more information on options for alternative storage locations, or contact Support .","title":"Why do I get an error when I try to submit an Open OnDemand interactive job?"},{"location":"help/faq/#why-do-i-get-an-error-when-i-try-to-launch-an-open-ondemand-hpc-interactive-session","text":"If you are seeing an error like the following when launching an HPC Interactive job, please read on for the most likely solution. The most common cause is that the command conda init was used, creating a block in your .bashrc file that looks like the following section. # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup = \" $( '/home/<YOUR_USER>/miniconda3/bin/conda' 'shell.bash' 'hook' 2 > /dev/null ) \" if [ $? -eq 0 ] ; then eval \" $__conda_setup \" else if [ -f \"/home/<YOUR_USER>/miniconda3/etc/profile.d/conda.sh\" ] ; then . \"/home/<YOUR_USER>/miniconda3/etc/profile.d/conda.sh\" else export PATH = \"/home/<YOUR_USER>/miniconda3/bin: $PATH \" fi fi unset __conda_setup # <<< conda initialize <<< Anaconda is managed as a Module on Cheaha , so it is unnecessary to use conda init , and can cause issues with Open OnDemand . To avoid the issue reoccurring, please do not use conda init on Cheaha. To resolve this issue, you'll need to locate the hidden file .bashrc in the file browser pane and select it. Click the \"Edit\" button. In the new tab that opens, delete the section shown above and click \"Save\". Please try to launch your job again. If it still doesn't work, please contact us","title":"Why do I get an error when I try to launch an Open OnDemand HPC Interactive session?"},{"location":"help/faq/#why-do-i-get-a-gray-screen-after-launching-my-open-ondemand-rstudio-server-session","text":"Most commonly, delays in launching Open OnDemand RStudio Server are caused by a large quantities of files referenced in the .rstudio directory in the home directory. This can also commonly occur when a large amount of data was loaded in the workspace in your previous RStudio session. To resolve the issue, you'll need to locate the hidden directory .rstudio in your $HOME folder. Once you've found it, delete it, then try recreating your job. Warning Deleting the .rstudio directory will delete the workspace you had open, including any scripts that were unsaved. Saved scripts will be fine.","title":"Why do I get a gray screen after launching my Open OnDemand RStudio Server session?"},{"location":"help/faq/#why-is-my-open-ondemand-rstudio-server-session-giving-me-a-502-or-503-error","text":"See here .","title":"Why is my Open OnDemand RStudio Server session giving me a 502 or 503 error?"},{"location":"help/faq/#why-is-my-anaconda-environment-not-appearing-in-my-jupyter-notebook-kernels","text":"See here .","title":"Why is my Anaconda environment not appearing in my Jupyter Notebook kernels?"},{"location":"help/faq/#what-is-a-segfault-error","text":"A segfault, or segmentation fault, often causes unexpected termination of software. These errors are caused by software attempting to access memory that it isn't allowed to access. An error typically includes a long string of hexadecimal characters like 0x000055ea4064c135 , but with different values, and one or more of segmentation fault , segfault and/or SIGSEGV . These issues are most commonly caused by programming errors, but can be related to out-of-memory errors. If you encounter a segmentation fault, first try increasing the memory requested for the job. It may help to learn more about SLURM and job efficiency .","title":"What is a segfault error?"},{"location":"help/faq/#what-is-a-bus-error","text":"A bus error is almost always related to a process requesting more memory than is available. In other words, the job ran out of memory. The simplest solution is to increase the requested memory for the job. It may help to learn more about SLURM and job efficiency .","title":"What is a bus error?"},{"location":"help/faq/#i-copied-a-script-from-a-windows-machine-to-cheaha-and-am-getting-a-bad-interpreter-error","text":"Bad interpreter errors usually mean that there are symbols in the script that the shell interpreter cannot read correctly. This is usually due to the difference in newline symbols used in Windows ( \\r\\n ) and Unix ( \\n ). Once these newline symbols are changed, the script should be able to run on Cheaha. To convert a script written in Windows to be compatible on Linux, use the dos2unix command. Just give it the name of the script to convert and it should convert it correctly. dos2unix example_script.sh If you are using development tools like Notepad++ or VSCode , there are built-in ways to remove the carriage returns. NP++: Go to Edit >> EOL Conversion >> Unix (LF). This must be done for individual files. VSCode: Open Settings and search for EoL. Change the default EoL character to \\n . This will be applied globally if changed in your User Options.","title":"I copied a script from a Windows machine to Cheaha and am getting a bad interpreter error"},{"location":"help/faq/#why-am-i-seeing-an-invalidaccount-error-when-submitting-jobs-when-i-could-submit-jobs-fine-previously","text":"If you see this error when submitting jobs, please contact support . There are a few causes for this error that will need to be investigated.","title":"Why am I seeing an InvalidAccount error when submitting jobs when I could submit jobs fine previously?"},{"location":"help/faq/#why-cant-tensorflow-find-a-gpu-on-cheaha","text":"When using Tensorflow on Cheaha, if you see an error like Not creating XLA devices, tf_xla_enable_xla_devices not set or are unable to find the GPU, check the following. Ensure you are correctly requesting GPUs with the SLURM flag --gres=gpu:1 . Change 1 to the appropriate number of GPUs needed. Check our Hardware page for limits. Ensure you are loading the appropriate CUDA toolkit using e.g. module load cuda11.2/toolkit/11.1.2 . You can check which modules are available using module avail toolkit at the terminal. Be sure you are loading the correct module. To check which module is required for your version of Tensorflow, see the toolkit requirements chart here https://www.tensorflow.org/install/source#gpu .","title":"Why can't Tensorflow find a GPU on Cheaha?"},{"location":"help/faq/#issues-in-progress","text":"This section gives workarounds for some issues that are currently being investigated by the team. Once the issue has been resolved, the section will be updated with the permanent fix.","title":"Issues in Progress"},{"location":"help/faq/#why-are-my-r-packages-not-installing-correctly-using-the-rstudio-app","text":"When using the RStudio Server app (the non-deprecated version) on Open OnDemand , some packages that need C++ compilation will not install correctly when run directly from either the RStudio console or the Install Packages button. As a temporary workaround, you can install the packages from the terminal inside RStudio. Select the Terminal tab next to the Console tab Load the same R module you loaded when requesting the session using the module load R/<insert version> command Start R using the R command Install the packages you need using R's install.packages command (or whichever other package installer you need, such as BiocManager::install ) For some people, the utils package may not be loaded initially causing R to not be able to find the install.packages command. Use library(utils) first, then use install.packages . Once your packages are installed, close the terminal by clicking the dropdown and selecting Close Terminal . This will free the resources the terminal R session was using for RStudio to be able to use again. Your R packages should now be available for use.","title":"Why are my R Packages Not Installing Correctly Using the RStudio App"},{"location":"help/faq/#my-script-uses-a-singularity-container-and-seems-to-freeze-until-it-times-out","text":"When using Singularity containers in scripts and the amount of memory used by the container exceeds the amount requested for the job, the script will hang until the time limit is reached. Typically, scripts are immediately killed when memory used exceeds memory requested, but that is not the behavior with containers. If your scripts using containers seem to hang and not produce anything until the job ends, use the seff command to investigate the memory usage after the job ends (see it's entry in our docs ). If it is using the full amount of memory requested, try upping the amount of memory and submit the job again. To find the job number for past jobs, look further down .","title":"My script uses a Singularity container and seems to freeze until it times out"},{"location":"help/faq/#how-to","text":"","title":"How To"},{"location":"help/faq/#how-do-i-locate-hidden-dot-files-on-cheaha","text":"Dot files are files or directories that start with the character . as in the file .bashrc or the .conda directory. If you are using Open OnDemand, please navigate to https://rc.uab.edu , login and click the Files dropdown in the top navigation menu. Click \"Home Directory\". More detailed instructions on the file browser can be found at Open OnDemand Files . Once you've opened the file browser, check the checkbox labeled \"Show Dotfiles\" in the top-right of the page. If you are using the terminal, you'll need to use cd to navigate to the appropriate directory and enter use ls with the -a flag to list all files and directories, including hidden ones.","title":"How do I locate hidden dot files on Cheaha?"},{"location":"help/faq/#how-do-i-find-my-job-id-number","text":"All jobs submitted to Slurm on Cheaha are assigned a unique identifier called the Job ID or jobid . Using Open OnDemand ? If the job is still running see My Interactive Sessions to locate the Job ID. If the job is no longer running, or the job card is missing, use the last bullet point for sacct usage. Using the terminal? Just submitted the job? Using srun <flags> --pty /bin/bash ? If you are still in the job context, type echo $SLURM_JOB_ID . Otherwise use the final bullet point for sacct usage. Using srun without --pty ? Use the final bullet point for sacct usage. Using sbatch ? The Job ID is printed to the command line automatically on submission. Job still running? Use squeue -u $USER to show all your running jobs. Job not running? Use the final bullet point sacct usage. How to use sacct to find a job. If you know the date of the job, use the following command at the terminal. The date following -s is the start date, choose the same day the job was submitted. The following -e is the end date, choose the day after -s . You will be provided with a list of jobs run on the -s day, and can sift through them to find your job. sacct -u $USER -s YYYY-MM-DD -e YYYY-MM-DD Otherwise contact Support for assistance.","title":"How do I find my Job ID number?"},{"location":"help/faq/#how-do-i-find-my-open-ondemand-session-information","text":"All jobs created using Open OnDemand will create a folder in your home directory containing diagnostic information about those jobs. This information can be helpful for diagnosing issues when using Open OnDemand. To review the folder contents, click the Session ID link in the job card under the My Interactive Sessions of Open OnDemand. If asked, the simplest way to give us the information is the following. Please visit the File Browser page for a visual guide to the features of the Open OnDemand File Browser, which will help with the following instructions. Please also visit the My Interactive Sessions page to familiarize yourself with job cards. Find the card for the appropriate job. Click the Session ID link of the card from (1) to open the File Browser . Make note of the Session ID value, which should be something like bbd7ad60-a37c-47aa-a8fb-d7ca6cf39b3c but with different characters. In the \"Working Directory\" section of the interface, click the parent folder name to go up one directory level. In the \"File List\" locate the folder with the matching Session ID from step (2). Click that folder once to select it. Click the \"Download\" button in the \"File Command Menu\" to download a zipped copy of the folder and its contents. Pass the downloaded Zip file to Support .","title":"How do I find my Open OnDemand Session Information?"},{"location":"help/faq/#uab-cloud","text":"","title":"UAB Cloud"},{"location":"help/faq/#why-is-my-instance-failing-to-start","text":"Please see Instances Failing to Start .","title":"Why is my instance failing to start?"},{"location":"help/faq/#why-does-my-instance-show-an-error-in-the-instances-table","text":"Please see Instances Failing to Start .","title":"Why does my instance show an error in the Instances table?"},{"location":"help/faq/#how-do-i-regain-access-to-an-instance","text":"If you've lose the only copy of your private key for accessing a cloud.rc virtual machine (VM), there are a few options. Is your instance volume backed (the default behavior)? No : Then your instance is stateless and can be simply be deleted and restarted. Yes : Will your volume be deleted when the instance is deleted (not default behavior)? Yes : Take a snapshot of your current instance to create an image , then delete your instance and create from image . No : Delete your instance and create an instance from the existing volume.","title":"How do I regain access to an instance?"},{"location":"help/faq/#globus","text":"","title":"Globus"},{"location":"help/faq/#why-cant-other-users-access-data-i-transferred-to-a-project-space-on-cheaha","text":"When transferring using Globus, file permissions from the source file are not transferred with the file due to limitations purposefully placed by Globus. When transferring to any space on Cheaha, the most likely permission sets are user read and write only with no group permissions. Researchers will need to manually change permissions for the files they transfer using Globus to add group permissions if desired. You can use the chmod command for this: # If you transferred a single file, you can do this chmod g+rw <path/to/file> # If you transferred a directory and need to give permissions to all files in the directory, you can use this chmod -R g+rw <path/to/file> The g+rw adds group read and write permissions. You can add only read permissions by omitting the w if desired. For more information on Globus file permissions, you can read their FAQ entry .","title":"Why can't other users access data I transferred to a project space on Cheaha?"},{"location":"help/faq/#long-term-storage","text":"","title":"Long-Term Storage"},{"location":"help/faq/#why-do-i-need-to-add-the-trailing-to-the-end-of-path-names-in-my-s3-commands","text":"In the Unix file system, the / is a protected character the specifies a branch in the file tree, and so / cannot be used in the name of a file or folder. This is not the case for S3 storage. Everything in S3 storage is stored at the top level of the bucket no matter the source's file and folder structure. However, people have become so used to having a folder structure and how it provides an organization system, so S3 interfaces pretend to have one. When a folder is uploaded, the files in the folder will have that folder name appended to the beginning of the file name along with the / separator. Same for if files are uploaded to existing \"folders\" in S3. These paths are called prefixes in S3 terms. Because / is not protected in S3, all of the / characters in a path are included with the prefix, so if you want to perform a command on a prefix in S3, you must include the trailing / because it is actually a part of the prefix name.","title":"Why do I need to add the trailing / to the end of path names in my S3 commands?"},{"location":"help/faq/#gitlab","text":"","title":"GitLab"},{"location":"help/faq/#why-am-i-not-able-to-remove-researchers-from-repositories","text":"If you are not able to remove one or more researchers from a given repository, please check the following. Individual researchers can be members of a group, of a subgroup, of a repository inside groups and subgroups, or any combination. Be aware that GitLab repositories can inherity membership from groups. Is the affected repository part of a group or subgroup? If yes, try removing the researcher from the group or subgroup. Work your way up any nested subgroups if it continues to fail. If no, please contact Support .","title":"Why am I not able to remove researchers from repositories?"},{"location":"help/faq/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"help/faq/#shell-commands","text":"","title":"Shell Commands"},{"location":"help/faq/#why-do-my-shell-commands-not-work-as-expected","text":"There are a number of possible causes for shell commands not working as expected. Ensure that you are in the expected environment. Make sure you are using Cheaha, cloud.rc, or your local machine, as appropriate. Ensure that you have any needed modules loaded or environment variables set. Be very careful when copy-pasting commands from rich-text sources. Some sources can have invisible, special formatting characters that can corrupt the pasted command. Be sure to use plain-text editors like Notepad++ or VSCode, instead of rich-text editors, when storing commands for future use. Some examples of rich-text sources include: Websites and HTML Word, Excel, PowerPoint, etc. PDF documents","title":"Why do my shell commands not work as expected?"},{"location":"help/support/","text":"How to Request Support \u00b6 How Do I Create a Support Ticket? \u00b6 To Create a support ticket, send a descriptive email to support@listserv.uab.edu to create a ticket. Bonus points for including the following details. General Issues \u00b6 What is your goal? What steps were taken? What was expected? What actually happened? How was the cluster accessed? Web Portal, SSH, VNC, etc.? What software were you using? Please be as specific as possible. The command module list can be helpful here. Outages \u00b6 What part of the cluster is affected? Please list any relevant affected nodes or other hardware that is not accessible. If you are unable to access the cluster please state that instead. What were you working on when you noticed the outage? How were you accessing the cluster? Web Portal, SSH, VNC, etc.? Finding Common Information \u00b6 Please see our FAQ , especially How To . How Do I Request Or Change A Project Space? \u00b6 Projects are collaborative data and code storage spaces with controlled access. Any UAB investigator with a legitimate research need can request a project storage space. Please send an email with the following information, depending on what you need. For more details on project storage, please see our Storage page. New Projects \u00b6 What is the purpose of the project space? What should we name the project? Short, descriptive, memorable names work best. The name will be used as the project folder name in our file system, so alphanumeric, underscore and dash characters only please. Who should have access? Please provide a list of blazerids. The intended project owner will always have access. Access Management \u00b6 What is the name of the project? Who should be given access? Who should no longer have access? Storage \u00b6 What is the name of the project? How much additional or total space will be needed? How do I request new software installed? \u00b6 Before making a request for new software on Cheaha, please try searching our modules or searching for packages on Anaconda . If you are not able to find a suitable module or package and would like a new piece of software installed on Cheaha, please create a ticket with the name of the software, the version number, and a link to the installation instructions. Office Hours \u00b6 For our office hours links please see Contact Us .","title":"Support"},{"location":"help/support/#how-to-request-support","text":"","title":"How to Request Support"},{"location":"help/support/#how-do-i-create-a-support-ticket","text":"To Create a support ticket, send a descriptive email to support@listserv.uab.edu to create a ticket. Bonus points for including the following details.","title":"How Do I Create a Support Ticket?"},{"location":"help/support/#general-issues","text":"What is your goal? What steps were taken? What was expected? What actually happened? How was the cluster accessed? Web Portal, SSH, VNC, etc.? What software were you using? Please be as specific as possible. The command module list can be helpful here.","title":"General Issues"},{"location":"help/support/#outages","text":"What part of the cluster is affected? Please list any relevant affected nodes or other hardware that is not accessible. If you are unable to access the cluster please state that instead. What were you working on when you noticed the outage? How were you accessing the cluster? Web Portal, SSH, VNC, etc.?","title":"Outages"},{"location":"help/support/#finding-common-information","text":"Please see our FAQ , especially How To .","title":"Finding Common Information"},{"location":"help/support/#how-do-i-request-or-change-a-project-space","text":"Projects are collaborative data and code storage spaces with controlled access. Any UAB investigator with a legitimate research need can request a project storage space. Please send an email with the following information, depending on what you need. For more details on project storage, please see our Storage page.","title":"How Do I Request Or Change A Project Space?"},{"location":"help/support/#new-projects","text":"What is the purpose of the project space? What should we name the project? Short, descriptive, memorable names work best. The name will be used as the project folder name in our file system, so alphanumeric, underscore and dash characters only please. Who should have access? Please provide a list of blazerids. The intended project owner will always have access.","title":"New Projects"},{"location":"help/support/#access-management","text":"What is the name of the project? Who should be given access? Who should no longer have access?","title":"Access Management"},{"location":"help/support/#storage","text":"What is the name of the project? How much additional or total space will be needed?","title":"Storage"},{"location":"help/support/#how-do-i-request-new-software-installed","text":"Before making a request for new software on Cheaha, please try searching our modules or searching for packages on Anaconda . If you are not able to find a suitable module or package and would like a new piece of software installed on Cheaha, please create a ticket with the name of the software, the version number, and a link to the installation instructions.","title":"How do I request new software installed?"},{"location":"help/support/#office-hours","text":"For our office hours links please see Contact Us .","title":"Office Hours"},{"location":"kubernetes/startup/","text":"UAB Kubernetes \u00b6 Construction This page is a stub and is under construction. Kubernetes (K8s, named because there are 8 letters between 'K' and 's') can be used to automate workflows of all kinds. Researchers can use KSoftware engineers can use it to create GitLab runners to execute continuous integration workflows to deploy code updates seemlessly. UAB Kubernetes Computation Resources \u00b6 4 DGX A100 nodes GPUs aren\u2019t virtualized, but one DGX node can be split into 1/7th of a GPU 8 A100s per DGX node * 7/7ths = 56/7ths of a GPU possible Can be used for classes learning to use GPUs Finding containers to use K8s will search DockerHub and other container registries automagically. Can add additional registries","title":"Startup"},{"location":"kubernetes/startup/#uab-kubernetes","text":"Construction This page is a stub and is under construction. Kubernetes (K8s, named because there are 8 letters between 'K' and 's') can be used to automate workflows of all kinds. Researchers can use KSoftware engineers can use it to create GitLab runners to execute continuous integration workflows to deploy code updates seemlessly.","title":"UAB Kubernetes"},{"location":"kubernetes/startup/#uab-kubernetes-computation-resources","text":"4 DGX A100 nodes GPUs aren\u2019t virtualized, but one DGX node can be split into 1/7th of a GPU 8 A100s per DGX node * 7/7ths = 56/7ths of a GPU possible Can be used for classes learning to use GPUs Finding containers to use K8s will search DockerHub and other container registries automagically. Can add additional registries","title":"UAB Kubernetes Computation Resources"},{"location":"national_ci/nih/","text":"National Institutes of Health (NIH) Cyberinfrastructure \u00b6 Construction This page is a stub and is under construction.","title":"NIH Cyberinfrastructure"},{"location":"national_ci/nih/#national-institutes-of-health-nih-cyberinfrastructure","text":"Construction This page is a stub and is under construction.","title":"National Institutes of Health (NIH) Cyberinfrastructure"},{"location":"national_ci/nsf_access/","text":"National Science Foundation (NSF) Cyberinfrastructure \u00b6 Construction This page is a stub and is under construction.","title":"NSF Cyberinfrastructure"},{"location":"national_ci/nsf_access/#national-science-foundation-nsf-cyberinfrastructure","text":"Construction This page is a stub and is under construction.","title":"National Science Foundation (NSF) Cyberinfrastructure"},{"location":"national_ci/osg/","text":"The Open Science Grid (OSG) \u00b6 Construction This page is a stub and is under construction.","title":"The Open Science Grid"},{"location":"national_ci/osg/#the-open-science-grid-osg","text":"Construction This page is a stub and is under construction.","title":"The Open Science Grid (OSG)"},{"location":"uab_cloud/installing_software/","text":"Installing Software on Instances \u00b6 An important part of managing instances is the installation of software. This page assumes you have a working instance and can SSH into it. This page assumes you are using an Ubuntu image. We highly recommend building your research software stack into a Container . While there is a learning curve and some setup time, containers make replicating and sharing environments simpler. Everything you develop is packaged into a self-contained unit that can be run on virtually any modern Linux system. A particular command sudo will be used extensively. Be warned that sudo grants any commands used administrator privileges. If you use sudo with untrustworthy software, you may be allowing an attacker to compromise your system. Danger The sudo command should be used carefully and judiciously, as it creates security risks. Use with caution. Before Installing Software \u00b6 Before installing software, good practice is updating and upgrading operating system packages. For some software this is required. These updates often include critical security and bug fixes. To update the instance operating system, enter the following at the command line. sudo apt update sudo apt upgrade Installing Software \u00b6 Most common software packages and NVIDIA drivers are available as apt packages. Some packages are available using custom installers. Be sure the website and installer author are trustworthy before installing! Finding Packages \u00b6 Try using Google to locate the name of the package with something like ubuntu apt <keywords> Try using https://packages.ubuntu.com Try apt-cache search <keyword> Ask Support for help Installing Packages \u00b6 If the software is available via apt then use sudo apt install <package> . An example would be sudo apt install git to install git software. If the software uses a custom installer, then follow the instructions provided by the software's documentation. An example would be Miniconda , where a shell script is downloaded and then executed using bash installer.sh . Common Examples \u00b6 Installing NVidia Drivers \u00b6 Run the commands in Before Installing Software . sudo apt install ubuntu-drivers-common ubuntu-drivers devices Find the line with \"recommended\" and install the package on that line with sudo apt install nvidia-driver-### Reboot the instance Installing Miniconda \u00b6 We recommend installing Miniconda on cloud.rc instances, as opposed to Anaconda, to conserve storage space. Run the commands in Before Installing Software . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Tip Consider installing Mamba to speed up environment installation. Installing Singularity \u00b6 Follow the instructions located at https://sylabs.io/guides/3.9/user-guide/quick_start.html#install-system-dependencies under \"Debian-based systems\". Run the commands in Before Installing Software . Run the following sudo apt-get install -y \\ build-essential \\ libseccomp-dev \\ pkg-config \\ squashfs-tools \\ cryptsetup Install Go language using the following export VERSION = 1 .17.2 OS = linux ARCH = amd64 && \\ # Replace the values as needed wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && \\ # Downloads the required Go package sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && \\ # Extracts the archive rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc Download SingularityCE export VERSION = 3 .9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v ${ VERSION } /singularity-ce- ${ VERSION } .tar.gz && \\ tar -xzf singularity-ce- ${ VERSION } .tar.gz && \\ cd singularity-ce- ${ VERSION } Compile SingularityCE ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Note For other versions of the Singularity documentation, visit https://sylabs.io/docs/ .","title":"Installing Software"},{"location":"uab_cloud/installing_software/#installing-software-on-instances","text":"An important part of managing instances is the installation of software. This page assumes you have a working instance and can SSH into it. This page assumes you are using an Ubuntu image. We highly recommend building your research software stack into a Container . While there is a learning curve and some setup time, containers make replicating and sharing environments simpler. Everything you develop is packaged into a self-contained unit that can be run on virtually any modern Linux system. A particular command sudo will be used extensively. Be warned that sudo grants any commands used administrator privileges. If you use sudo with untrustworthy software, you may be allowing an attacker to compromise your system. Danger The sudo command should be used carefully and judiciously, as it creates security risks. Use with caution.","title":"Installing Software on Instances"},{"location":"uab_cloud/installing_software/#before-installing-software","text":"Before installing software, good practice is updating and upgrading operating system packages. For some software this is required. These updates often include critical security and bug fixes. To update the instance operating system, enter the following at the command line. sudo apt update sudo apt upgrade","title":"Before Installing Software"},{"location":"uab_cloud/installing_software/#installing-software","text":"Most common software packages and NVIDIA drivers are available as apt packages. Some packages are available using custom installers. Be sure the website and installer author are trustworthy before installing!","title":"Installing Software"},{"location":"uab_cloud/installing_software/#finding-packages","text":"Try using Google to locate the name of the package with something like ubuntu apt <keywords> Try using https://packages.ubuntu.com Try apt-cache search <keyword> Ask Support for help","title":"Finding Packages"},{"location":"uab_cloud/installing_software/#installing-packages","text":"If the software is available via apt then use sudo apt install <package> . An example would be sudo apt install git to install git software. If the software uses a custom installer, then follow the instructions provided by the software's documentation. An example would be Miniconda , where a shell script is downloaded and then executed using bash installer.sh .","title":"Installing Packages"},{"location":"uab_cloud/installing_software/#common-examples","text":"","title":"Common Examples"},{"location":"uab_cloud/installing_software/#installing-nvidia-drivers","text":"Run the commands in Before Installing Software . sudo apt install ubuntu-drivers-common ubuntu-drivers devices Find the line with \"recommended\" and install the package on that line with sudo apt install nvidia-driver-### Reboot the instance","title":"Installing NVidia Drivers"},{"location":"uab_cloud/installing_software/#installing-miniconda","text":"We recommend installing Miniconda on cloud.rc instances, as opposed to Anaconda, to conserve storage space. Run the commands in Before Installing Software . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Tip Consider installing Mamba to speed up environment installation.","title":"Installing Miniconda"},{"location":"uab_cloud/installing_software/#installing-singularity","text":"Follow the instructions located at https://sylabs.io/guides/3.9/user-guide/quick_start.html#install-system-dependencies under \"Debian-based systems\". Run the commands in Before Installing Software . Run the following sudo apt-get install -y \\ build-essential \\ libseccomp-dev \\ pkg-config \\ squashfs-tools \\ cryptsetup Install Go language using the following export VERSION = 1 .17.2 OS = linux ARCH = amd64 && \\ # Replace the values as needed wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && \\ # Downloads the required Go package sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && \\ # Extracts the archive rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc Download SingularityCE export VERSION = 3 .9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v ${ VERSION } /singularity-ce- ${ VERSION } .tar.gz && \\ tar -xzf singularity-ce- ${ VERSION } .tar.gz && \\ cd singularity-ce- ${ VERSION } Compile SingularityCE ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Note For other versions of the Singularity documentation, visit https://sylabs.io/docs/ .","title":"Installing Singularity"},{"location":"uab_cloud/instance_setup_basic/","text":"Instance Setup and Tutorial \u00b6 Instances are the basic unit of compute on cloud.rc. Requesting an instance involves a number of steps, and requires that a Network has already been setup, along with certain Security settings and features. It is also possible to attach persistent reusable Volumes to instances. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Creating an Instance \u00b6 Creating an instance is possibly a step you'll perform often, depending on your workflow. There are many smaller steps to create an instance, so please take care to check all the fields when you create an instance. These instructions require that you've set up a Network and followed all of the instructions on the linked page. You should have a Network, Subnet, Router and Floating IP. You will also need to setup a Key Pair and an SSH Security Group . Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Instances\". Click \"Launch Instance\" to open a dialog box. Fill out the dialog box completely. There are several tabs that will need to be completed. \"Details\" tab. Enter an \"Instance Name\". See Naming Conventions . Enter a \"Description\". Select \"nova\" in the \"Availability Zone\" drop down box. Select \"1\" in the \"Count\" field. Click \"Next >\" to move to the \"Source\" tab. \"Source\" tab. Sources determine what operating system or pre-defined image will be used as the starting point for your operating system (OS). Select \"Image\" in the \"Select Boot Source\" drop down box. Select \"Yes\" under \"Create New Volume\". Choose an appropriate \"Volume Size\" in GB . Note that for many single-use instances, 20 GB is more than enough. If you need more because you have persistent data, please create a persistent volume<volume_setup_basic> . Select \"Yes\" or \"No\" under \"Delete Volume on Instance Delete\" \"Yes\" is a good choice if you don't care about reusing the OS. \"No\" is a good choice if the OS volume will be reused. Pick an image from the list under the \"Available\" section. Use the search box to help find the image that best suits your research needs. When you find the best image, click the button with an up arrow next to the image. The image will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Flavor\" tab. \"Flavor\" tab. Flavors determine what hardware will be available to your instance, including cpus, memory and gpus. Pick an instance flavor form the list under the \"Available\" section. Use the search box to help find the flavor that best suits your needs. When you find the best flavor, click the button with an up arrow next to the flavor. The flavor will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Networks\" tab. \"Networks\" tab. Networks determine how your instance will talk to the internet and other instances. See Network for more information. Pick a network from the list under the \"Available' section. A Network may already be picked in the \"Allocated\" section. If this is not the correct Network, use the down arrow next to it to remove it from the \"Allocated\" section. If the Network is correct, skip (ii.) through (iv.). Use the search box to help find the Network that best suits your needs. When you find the best Network, click the button with an up arrow next to the Network. The Network will move to the \"Allocated\" section above the \"available\" section. Click \"Next >\" to move to the \"Network Ports\" tab. \"Network Ports\" tab. Coming Soon! Leave this tab empty. Click \"Next >\" to move to the \"Security Groups\" tab. \"Security Groups tab. Security Groups allow for fine-grained control over external access to your instance. For more information see Creating a Security Group for more information. Pick the \"ssh\" Security Group from the \"Available\" section by pressing the up arrow next to it. The \"default\" Security Group should already be in the \"Allocated\" section. Click \"Next >\" to move to the \"Key Pair\" tab. \"Key Pair\" tab. Key Pairs allow individual access rights to the instance via SSH. For more information see Creating a Key Pair . Pick one or more key pairs from the list under the \"Available\" section. A Key Pair may already be picked in the \"Allocated\" section. If this is not the correct \"Key Pair\", use the down arrow next to it to remove it form the \"Allocated\" section. If the Key Pair is correct, skip (ii.) through (iv.). Use the search box to help find the Key Pair that best suits your needs. When you find the best Key Pair(s), click the button with an up arrow next to the Key Pair(s). The Key Pair(s) will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Configuration\" tab. \"Configuration\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Server Groups\" tab. \"Server Groups\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Scheduler Hints\" tab. \"Scheduler Hints\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Metadata\" tab. \"Metadata\" tab. Coming Soon! Skip this tab. Click \"Launch Instance\" to launch the instance. Redirects to the \"Instances\" page. There should be a new entry in the table. The instance will take some time to build and boot. When the Status column entry says \"Active\" please move to the next steps. Associate Floating IP. In the \"Actions\" column entry, click the drop down triangle and select \"Associate Floating IP\". A dialog box will open. Select an IP address in the \"IP Address\" drop down box. Select a port in the \"Port to be associated\" drop down box. Click \"Associate\" to return to the \"Instances\" page and associate the selected IP. At this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You can also Transfer Data to the instance. Important Please be considerate of other users. When you no longer need instances for processing or development, please delete them to free up resources. Instances Failing to Start \u00b6 There are a number of reasons an instance might fail. We are able to provide direct support for instances which fail to start for reasons outside the instance itself. To help us correct the error, you'll need to have information from the instance page. Below is an example of a failed instance in the \"Instances\" table, helpfully named failed_instance . Note the \"Error\" label under the \"Status\" column. In the \"Instances\" table, click the name of your failed instance. You should see a page like below, with some basic metadata about the instance as well as a \"Fault\" section. We will need to \"ID\" and the reason for the fault. In this case, the instance failed because it could not allocate a GPU, as all GPUs were allocated at the time of its creation. It is not possible to diagnose the specifics without consulting us, so please feel free to contact Support . Instances can fail for other reasons as well, please contact Support with the \"ID\" and \"Fault\" information. For instances which fail due to internal reasons, i.e. while using SSH or an application, we are still able to provide support but it will have to be on a case-by-case basis. Be prepared to walk us through the steps you took to set up the instance and any software, as well as any data processing steps, leading up to the failure. SSH Into the Instance \u00b6 If you are following the tutorial, then at this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You will need to Install an SSH Client Once your machine has an ssh client, use the following command. If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos or whatever is appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP , and the value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . Install an SSH Client to use SSH from your local machine to your cloud instance. Manage Your Private Key Start the SSH Agent to enable your system to remember your private key. Add a Private Key to the ssh agent to remember it for future use. Verify the SSH Client Works . Use the following command to connect ssh ubuntu@<floating ip> -i ~/.ssh/<private_key_file> If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos , or whatever may be appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP . The value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . (optional, but helpful) Set Up a Configuration File to simplify the command used to make a connection. Note Reusing a floating IP for a new instance can result in a \"Remote Host Identification Has Changed\" error, preventing connection. Please see Remove an Invalid Host Fingerprint . Streamlining SSH \u00b6 Refer to Setting up a Configuration File in Cloud Remote Access . Next Steps \u00b6 Now you are ready to Install Software and, optionally, Create a Persistent Volume . Deleting an Instance \u00b6 Note Deleting Instances is not part of the tutorial, and is here as a reference. To delete an instance, return to the \"Instances\" page using the left-hand navigation pane. In the table, find the row with the instance you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Instance\" to open a confirmation dialog. Click \"Delete Instance\" again to delete the instance permanently. Warning It is highly recommended to shut off an instance before deleting it. Continuing the Tutorial \u00b6 Now that you have set up a Network , Security Policies and an Instance , you are done with the tutorial, congratulations! There is one remaining optional step. If you need a persistent data volume to move between instances, please check our Volumes page.","title":"(3) Instances"},{"location":"uab_cloud/instance_setup_basic/#instance-setup-and-tutorial","text":"Instances are the basic unit of compute on cloud.rc. Requesting an instance involves a number of steps, and requires that a Network has already been setup, along with certain Security settings and features. It is also possible to attach persistent reusable Volumes to instances. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.","title":"Instance Setup and Tutorial"},{"location":"uab_cloud/instance_setup_basic/#creating-an-instance","text":"Creating an instance is possibly a step you'll perform often, depending on your workflow. There are many smaller steps to create an instance, so please take care to check all the fields when you create an instance. These instructions require that you've set up a Network and followed all of the instructions on the linked page. You should have a Network, Subnet, Router and Floating IP. You will also need to setup a Key Pair and an SSH Security Group . Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Instances\". Click \"Launch Instance\" to open a dialog box. Fill out the dialog box completely. There are several tabs that will need to be completed. \"Details\" tab. Enter an \"Instance Name\". See Naming Conventions . Enter a \"Description\". Select \"nova\" in the \"Availability Zone\" drop down box. Select \"1\" in the \"Count\" field. Click \"Next >\" to move to the \"Source\" tab. \"Source\" tab. Sources determine what operating system or pre-defined image will be used as the starting point for your operating system (OS). Select \"Image\" in the \"Select Boot Source\" drop down box. Select \"Yes\" under \"Create New Volume\". Choose an appropriate \"Volume Size\" in GB . Note that for many single-use instances, 20 GB is more than enough. If you need more because you have persistent data, please create a persistent volume<volume_setup_basic> . Select \"Yes\" or \"No\" under \"Delete Volume on Instance Delete\" \"Yes\" is a good choice if you don't care about reusing the OS. \"No\" is a good choice if the OS volume will be reused. Pick an image from the list under the \"Available\" section. Use the search box to help find the image that best suits your research needs. When you find the best image, click the button with an up arrow next to the image. The image will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Flavor\" tab. \"Flavor\" tab. Flavors determine what hardware will be available to your instance, including cpus, memory and gpus. Pick an instance flavor form the list under the \"Available\" section. Use the search box to help find the flavor that best suits your needs. When you find the best flavor, click the button with an up arrow next to the flavor. The flavor will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Networks\" tab. \"Networks\" tab. Networks determine how your instance will talk to the internet and other instances. See Network for more information. Pick a network from the list under the \"Available' section. A Network may already be picked in the \"Allocated\" section. If this is not the correct Network, use the down arrow next to it to remove it from the \"Allocated\" section. If the Network is correct, skip (ii.) through (iv.). Use the search box to help find the Network that best suits your needs. When you find the best Network, click the button with an up arrow next to the Network. The Network will move to the \"Allocated\" section above the \"available\" section. Click \"Next >\" to move to the \"Network Ports\" tab. \"Network Ports\" tab. Coming Soon! Leave this tab empty. Click \"Next >\" to move to the \"Security Groups\" tab. \"Security Groups tab. Security Groups allow for fine-grained control over external access to your instance. For more information see Creating a Security Group for more information. Pick the \"ssh\" Security Group from the \"Available\" section by pressing the up arrow next to it. The \"default\" Security Group should already be in the \"Allocated\" section. Click \"Next >\" to move to the \"Key Pair\" tab. \"Key Pair\" tab. Key Pairs allow individual access rights to the instance via SSH. For more information see Creating a Key Pair . Pick one or more key pairs from the list under the \"Available\" section. A Key Pair may already be picked in the \"Allocated\" section. If this is not the correct \"Key Pair\", use the down arrow next to it to remove it form the \"Allocated\" section. If the Key Pair is correct, skip (ii.) through (iv.). Use the search box to help find the Key Pair that best suits your needs. When you find the best Key Pair(s), click the button with an up arrow next to the Key Pair(s). The Key Pair(s) will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Configuration\" tab. \"Configuration\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Server Groups\" tab. \"Server Groups\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Scheduler Hints\" tab. \"Scheduler Hints\" tab. Coming Soon! Skip this tab. Click \"Next >\" to move to the \"Metadata\" tab. \"Metadata\" tab. Coming Soon! Skip this tab. Click \"Launch Instance\" to launch the instance. Redirects to the \"Instances\" page. There should be a new entry in the table. The instance will take some time to build and boot. When the Status column entry says \"Active\" please move to the next steps. Associate Floating IP. In the \"Actions\" column entry, click the drop down triangle and select \"Associate Floating IP\". A dialog box will open. Select an IP address in the \"IP Address\" drop down box. Select a port in the \"Port to be associated\" drop down box. Click \"Associate\" to return to the \"Instances\" page and associate the selected IP. At this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You can also Transfer Data to the instance. Important Please be considerate of other users. When you no longer need instances for processing or development, please delete them to free up resources.","title":"Creating an Instance"},{"location":"uab_cloud/instance_setup_basic/#instances-failing-to-start","text":"There are a number of reasons an instance might fail. We are able to provide direct support for instances which fail to start for reasons outside the instance itself. To help us correct the error, you'll need to have information from the instance page. Below is an example of a failed instance in the \"Instances\" table, helpfully named failed_instance . Note the \"Error\" label under the \"Status\" column. In the \"Instances\" table, click the name of your failed instance. You should see a page like below, with some basic metadata about the instance as well as a \"Fault\" section. We will need to \"ID\" and the reason for the fault. In this case, the instance failed because it could not allocate a GPU, as all GPUs were allocated at the time of its creation. It is not possible to diagnose the specifics without consulting us, so please feel free to contact Support . Instances can fail for other reasons as well, please contact Support with the \"ID\" and \"Fault\" information. For instances which fail due to internal reasons, i.e. while using SSH or an application, we are still able to provide support but it will have to be on a case-by-case basis. Be prepared to walk us through the steps you took to set up the instance and any software, as well as any data processing steps, leading up to the failure.","title":"Instances Failing to Start"},{"location":"uab_cloud/instance_setup_basic/#ssh-into-the-instance","text":"If you are following the tutorial, then at this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You will need to Install an SSH Client Once your machine has an ssh client, use the following command. If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos or whatever is appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP , and the value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . Install an SSH Client to use SSH from your local machine to your cloud instance. Manage Your Private Key Start the SSH Agent to enable your system to remember your private key. Add a Private Key to the ssh agent to remember it for future use. Verify the SSH Client Works . Use the following command to connect ssh ubuntu@<floating ip> -i ~/.ssh/<private_key_file> If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos , or whatever may be appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP . The value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . (optional, but helpful) Set Up a Configuration File to simplify the command used to make a connection. Note Reusing a floating IP for a new instance can result in a \"Remote Host Identification Has Changed\" error, preventing connection. Please see Remove an Invalid Host Fingerprint .","title":"SSH Into the Instance"},{"location":"uab_cloud/instance_setup_basic/#streamlining-ssh","text":"Refer to Setting up a Configuration File in Cloud Remote Access .","title":"Streamlining SSH"},{"location":"uab_cloud/instance_setup_basic/#next-steps","text":"Now you are ready to Install Software and, optionally, Create a Persistent Volume .","title":"Next Steps"},{"location":"uab_cloud/instance_setup_basic/#deleting-an-instance","text":"Note Deleting Instances is not part of the tutorial, and is here as a reference. To delete an instance, return to the \"Instances\" page using the left-hand navigation pane. In the table, find the row with the instance you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Instance\" to open a confirmation dialog. Click \"Delete Instance\" again to delete the instance permanently. Warning It is highly recommended to shut off an instance before deleting it.","title":"Deleting an Instance"},{"location":"uab_cloud/instance_setup_basic/#continuing-the-tutorial","text":"Now that you have set up a Network , Security Policies and an Instance , you are done with the tutorial, congratulations! There is one remaining optional step. If you need a persistent data volume to move between instances, please check our Volumes page.","title":"Continuing the Tutorial"},{"location":"uab_cloud/introduction/","text":"UAB Cloud \u00b6 Our cloud.rc portal, based on OpenStack cloud software, provides a home for more permanent research applications such as web pages and database hosting, as well as a place where researchers can more fluidly develop applications for high performance compute. Resource quotas are set to ensure that every researcher has a fair share. Please be sure to free up resources when they are no longer needed by deleting instances and volumes. Currently, access to cloud.rc must be made while on the UAB Campus Network or on the UAB Campus Virtual Private Network (VPN). For more information about using the UAB Campus VPN, please visit VPN - UAB IT . To get started using cloud.rc, please navigate to https://dashboard.cloud.rc.uab.edu/ . A view of the dashboard is shown below. Cloud.rc runs on Openstack. If you are new to Openstack or to cloud.rc, it is highly recommended to follow our tutorial to learn how to set up all of the necessary components of a virtual machine (VM) setup. In the tutorial we cover: Networks Security Policies Instances Volumes (optional) The tutorial is intended to be followed in this order. Doing it out of order may result in errors and issues. If you encounter any unexpected issues, unclear instructions or have questions or comments, please contact Support . Otherwise, feel free to start the tutorial with Networks . Naming Conventions \u00b6 Entities on cloud.rc must be named a certain way or difficult-to-diagnose errors may occur. Entities includes instances, volumes, networks, routers, and anything else that you are allowed to give a name to. Please use the following rules when naming entities: Must: use only letters, numbers, dash - and underscore _ . Must: have the first character in the name be a letter. Should: use short, descriptive, memorable names.","title":"Introduction"},{"location":"uab_cloud/introduction/#uab-cloud","text":"Our cloud.rc portal, based on OpenStack cloud software, provides a home for more permanent research applications such as web pages and database hosting, as well as a place where researchers can more fluidly develop applications for high performance compute. Resource quotas are set to ensure that every researcher has a fair share. Please be sure to free up resources when they are no longer needed by deleting instances and volumes. Currently, access to cloud.rc must be made while on the UAB Campus Network or on the UAB Campus Virtual Private Network (VPN). For more information about using the UAB Campus VPN, please visit VPN - UAB IT . To get started using cloud.rc, please navigate to https://dashboard.cloud.rc.uab.edu/ . A view of the dashboard is shown below. Cloud.rc runs on Openstack. If you are new to Openstack or to cloud.rc, it is highly recommended to follow our tutorial to learn how to set up all of the necessary components of a virtual machine (VM) setup. In the tutorial we cover: Networks Security Policies Instances Volumes (optional) The tutorial is intended to be followed in this order. Doing it out of order may result in errors and issues. If you encounter any unexpected issues, unclear instructions or have questions or comments, please contact Support . Otherwise, feel free to start the tutorial with Networks .","title":"UAB Cloud"},{"location":"uab_cloud/introduction/#naming-conventions","text":"Entities on cloud.rc must be named a certain way or difficult-to-diagnose errors may occur. Entities includes instances, volumes, networks, routers, and anything else that you are allowed to give a name to. Please use the following rules when naming entities: Must: use only letters, numbers, dash - and underscore _ . Must: have the first character in the name be a letter. Should: use short, descriptive, memorable names.","title":"Naming Conventions"},{"location":"uab_cloud/network_setup_basic/","text":"Network Setup and Tutorial \u00b6 Networking setup should be a one-time setup. While Floating IPs fall under the Networking fold-out, they should be allocated and released together with instances to maximize security. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Networks \u00b6 Creating a Network \u00b6 Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network entry should already be in the table. Click \"+ Create Network\" to open a dialog box. Fill out the dialog box. Only the \"Network\" tab is important, we will create a subnet as a separate step. Enter a \"Network Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Uncheck \"Create Subnet\". We will do this as a separate step. The other tabs should be removed. Leave the \"Availability Zone Hints\" box empty. Click \"Create\". Redirects to the \"Networks\" page. There should be a new entry in the table with the name given in (4.a) Deleting a Network \u00b6 Note Deleting Networks is not part of the tutorial, and is here as a reference. To delete a network, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the network you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Network\" to open a confirmation dialog. Click \"Delete Network\" again to delete the network permanently. Important You will not be able to delete the network if it has a subnet with any connected routers or ports. They will need to be removed or deleted first. Subnets \u00b6 Creating a Subnet \u00b6 Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network should already be an entry in the table. At least one other entry must be in the table. See Creating a Network . Under the \"Actions\" column, select the drop-down triangle button in the row corresponding to the network you want to add a subnet to. Click \"Create Subnet\" in the drop-down to open a dialog box. Fill out the dialog box. The \"Subnet\" tab. Enter a \"Subnet Name\". See Naming Conventions . Enter 192.168.0.0/24 as the \"Network Address\". The trailing /24 allocates the entire range from 192.168.0.0 through 192.168.0.255 to the subnet. Ensure \"IPv4\" is selected in the \"IP Version\" drop-down box. Leave \"Gateway IP\" empty to use the default value of 192.168.0.0 . Leave \"Disable Gateway\" unchecked. Click the \"Next >>\" button to move to the \"Subnet Details\" tab. Note If you receive an error like Failed to create subnet `192.168.0.0/24`... Invalid input for operation: Gateway is not valid on a subnet. Try changing the gateway IP address to 192.168.0.1 and trying again. The \"Subnet Details\" tab. Leave \"Enable DHCP\" checked. Enter 192.168.0.20,192.168.0.100 in the \"Allocation Pools\" box. The IP addresses in that range will be assigned to instances on this subnet. Leave \"DNS Name Servers\" empty. Leave \"Host Routes\" empty. Click \"Create\". Redirects to the \"Overview\" page for the network the subnet was added to. Click the \"Subnets\" tab next to \"Overview\" to verify the subnet was added to the table for this network. Deleting a Subnet \u00b6 Note Deleting Subnets is not part of the tutorial, and is here as a reference. To delete a subnet, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the associated subnet, and click the name of the network to go to that network's page. Click on the \"Subnets\" tab to go to the subnets table. In the table, find the row with the subnet you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Subnet\" to open a confirmation dialog. Click \"Delete Subnet\" again to delete the subnet permanently. Important You will not be able to delete the subnet if it is associated with any routers or ports. They will need to be removed or deleted first. Routers \u00b6 Creating a Router \u00b6 To follow these directions for creating a router, a Network and Subnet must already exist. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Routers\" in the fold-out menu. Click \"+ Create Router\" to open a dialog box. Fill out the dialog box. Enter a \"Router Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Select \"uab-campus\" in the \"External Network\" drop down box. Leave the \"Availability Zone Hints\" box empty. Click \"Create Router\". Redirects to the \"Routers\" page. There should be a new entry in the table with the name given in (4.a) Now we need to connect the router to our subnet. Click the name of the new entry under the \"Name\" column to open the router \"Overview\" page. Click the \"Interfaces\" tab. Click \"+ Add Interface\" to open a dialog box. Fill out the dialog box. Select an existing network-subnet pair in the \"Subnet\" drop down box. If this is your only router on the selected subnet, leave \"IP Address\" empty to use the subnet gateway. Click \"Submit\" Redirects to the \"Interfaces\" page for the router. There should be a new entry in the table. Deleting a Router \u00b6 Note Deleting Routers is not part of the tutorial, and is here as a reference. To delete a router, return to the \"Routers\" page using the left-hand navigation pane. In the table, find the row with the router you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Router\" to open a confirmation dialog. Click \"Delete Router\" again to delete the router permanently. Floating IPs \u00b6 Creating a Floating IP \u00b6 Floating IPs are required if you want an instance to talk to devices on the internet. These IPs are a shared resource, so they must be allocated when needed and released when no longer needed. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Floating IPs\". Click \"Allocate IP to Project\" to open a dialog box. Fill out the dialog box. Select \"uab-campus\" in the \"Pool\" drop down box. Enter a \"Description\". Leave \"DNS Domain\" empty. Leave \"DNS Name\" empty. Click \"Allocate IP\". Redirects to the \"Floating IPs\" page. There should be a new entry in the table. Releasing a Floating IP \u00b6 Note Releasing Floating IPs is not part of the tutorial, and is here as a reference. To release a floating IP, return to the \"Floating IPs\" page using the left-hand navigation pane. In the table, find the row with the floating IP you wish to release, and click the drop-down arrow under \"Actions\" in that row. Then click \"Release Floating IP\" to open a confirmation dialog. Click \"Release Floating IP\" again to release the floating IP. Continuing the Tutorial \u00b6 Now that you have set up a Network , the next step is to apply Security Policies to be able to communicate with it. To continue the tutorial, please visit Security Policies next.","title":"(1) Networks"},{"location":"uab_cloud/network_setup_basic/#network-setup-and-tutorial","text":"Networking setup should be a one-time setup. While Floating IPs fall under the Networking fold-out, they should be allocated and released together with instances to maximize security. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.","title":"Network Setup and Tutorial"},{"location":"uab_cloud/network_setup_basic/#networks","text":"","title":"Networks"},{"location":"uab_cloud/network_setup_basic/#creating-a-network","text":"Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network entry should already be in the table. Click \"+ Create Network\" to open a dialog box. Fill out the dialog box. Only the \"Network\" tab is important, we will create a subnet as a separate step. Enter a \"Network Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Uncheck \"Create Subnet\". We will do this as a separate step. The other tabs should be removed. Leave the \"Availability Zone Hints\" box empty. Click \"Create\". Redirects to the \"Networks\" page. There should be a new entry in the table with the name given in (4.a)","title":"Creating a Network"},{"location":"uab_cloud/network_setup_basic/#deleting-a-network","text":"Note Deleting Networks is not part of the tutorial, and is here as a reference. To delete a network, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the network you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Network\" to open a confirmation dialog. Click \"Delete Network\" again to delete the network permanently. Important You will not be able to delete the network if it has a subnet with any connected routers or ports. They will need to be removed or deleted first.","title":"Deleting a Network"},{"location":"uab_cloud/network_setup_basic/#subnets","text":"","title":"Subnets"},{"location":"uab_cloud/network_setup_basic/#creating-a-subnet","text":"Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network should already be an entry in the table. At least one other entry must be in the table. See Creating a Network . Under the \"Actions\" column, select the drop-down triangle button in the row corresponding to the network you want to add a subnet to. Click \"Create Subnet\" in the drop-down to open a dialog box. Fill out the dialog box. The \"Subnet\" tab. Enter a \"Subnet Name\". See Naming Conventions . Enter 192.168.0.0/24 as the \"Network Address\". The trailing /24 allocates the entire range from 192.168.0.0 through 192.168.0.255 to the subnet. Ensure \"IPv4\" is selected in the \"IP Version\" drop-down box. Leave \"Gateway IP\" empty to use the default value of 192.168.0.0 . Leave \"Disable Gateway\" unchecked. Click the \"Next >>\" button to move to the \"Subnet Details\" tab. Note If you receive an error like Failed to create subnet `192.168.0.0/24`... Invalid input for operation: Gateway is not valid on a subnet. Try changing the gateway IP address to 192.168.0.1 and trying again. The \"Subnet Details\" tab. Leave \"Enable DHCP\" checked. Enter 192.168.0.20,192.168.0.100 in the \"Allocation Pools\" box. The IP addresses in that range will be assigned to instances on this subnet. Leave \"DNS Name Servers\" empty. Leave \"Host Routes\" empty. Click \"Create\". Redirects to the \"Overview\" page for the network the subnet was added to. Click the \"Subnets\" tab next to \"Overview\" to verify the subnet was added to the table for this network.","title":"Creating a Subnet"},{"location":"uab_cloud/network_setup_basic/#deleting-a-subnet","text":"Note Deleting Subnets is not part of the tutorial, and is here as a reference. To delete a subnet, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the associated subnet, and click the name of the network to go to that network's page. Click on the \"Subnets\" tab to go to the subnets table. In the table, find the row with the subnet you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Subnet\" to open a confirmation dialog. Click \"Delete Subnet\" again to delete the subnet permanently. Important You will not be able to delete the subnet if it is associated with any routers or ports. They will need to be removed or deleted first.","title":"Deleting a Subnet"},{"location":"uab_cloud/network_setup_basic/#routers","text":"","title":"Routers"},{"location":"uab_cloud/network_setup_basic/#creating-a-router","text":"To follow these directions for creating a router, a Network and Subnet must already exist. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Routers\" in the fold-out menu. Click \"+ Create Router\" to open a dialog box. Fill out the dialog box. Enter a \"Router Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Select \"uab-campus\" in the \"External Network\" drop down box. Leave the \"Availability Zone Hints\" box empty. Click \"Create Router\". Redirects to the \"Routers\" page. There should be a new entry in the table with the name given in (4.a) Now we need to connect the router to our subnet. Click the name of the new entry under the \"Name\" column to open the router \"Overview\" page. Click the \"Interfaces\" tab. Click \"+ Add Interface\" to open a dialog box. Fill out the dialog box. Select an existing network-subnet pair in the \"Subnet\" drop down box. If this is your only router on the selected subnet, leave \"IP Address\" empty to use the subnet gateway. Click \"Submit\" Redirects to the \"Interfaces\" page for the router. There should be a new entry in the table.","title":"Creating a Router"},{"location":"uab_cloud/network_setup_basic/#deleting-a-router","text":"Note Deleting Routers is not part of the tutorial, and is here as a reference. To delete a router, return to the \"Routers\" page using the left-hand navigation pane. In the table, find the row with the router you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Router\" to open a confirmation dialog. Click \"Delete Router\" again to delete the router permanently.","title":"Deleting a Router"},{"location":"uab_cloud/network_setup_basic/#floating-ips","text":"","title":"Floating IPs"},{"location":"uab_cloud/network_setup_basic/#creating-a-floating-ip","text":"Floating IPs are required if you want an instance to talk to devices on the internet. These IPs are a shared resource, so they must be allocated when needed and released when no longer needed. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Floating IPs\". Click \"Allocate IP to Project\" to open a dialog box. Fill out the dialog box. Select \"uab-campus\" in the \"Pool\" drop down box. Enter a \"Description\". Leave \"DNS Domain\" empty. Leave \"DNS Name\" empty. Click \"Allocate IP\". Redirects to the \"Floating IPs\" page. There should be a new entry in the table.","title":"Creating a Floating IP"},{"location":"uab_cloud/network_setup_basic/#releasing-a-floating-ip","text":"Note Releasing Floating IPs is not part of the tutorial, and is here as a reference. To release a floating IP, return to the \"Floating IPs\" page using the left-hand navigation pane. In the table, find the row with the floating IP you wish to release, and click the drop-down arrow under \"Actions\" in that row. Then click \"Release Floating IP\" to open a confirmation dialog. Click \"Release Floating IP\" again to release the floating IP.","title":"Releasing a Floating IP"},{"location":"uab_cloud/network_setup_basic/#continuing-the-tutorial","text":"Now that you have set up a Network , the next step is to apply Security Policies to be able to communicate with it. To continue the tutorial, please visit Security Policies next.","title":"Continuing the Tutorial"},{"location":"uab_cloud/remote_access/","text":"Remote Access to Instances \u00b6 All of the access methods described below are built on top of ssh and require completion of the steps in Basic Security Setup to use with cloud.rc . Some of these steps are referenced in that document. Command Line via SSH \u00b6 SSH stands for S ecure SH ell and is a powerful tool for executing terminal commands on remote machines. It is widely used and ubiquitous, and a number of other technologies are built on top of SSH, like sftp and scp for transferring files. It is also the primary mode of command line communication with Research Computing technologies like Cheaha and cloud.rc. Install an SSH Client \u00b6 There are two main steps to working with SSH efficiently. The first is to ensure you have an SSH client installed, which will let your local machine communicate with remote machines. The second is to ensure you have ssh-agent running in each terminal window to automate management of key files. The ssh-agent software comes with most SSH clients, but does not always run automatically. How to start the ssh-agent software automatically varies depending on operating system and shell flavor, which we will describe below. Install an SSH Client (Linux) \u00b6 Virtually all Linux distributions come with SSH preinstalled and configured appropriately for ease of use, including automatically starting the ssh-agent . Install an SSH Client (MacOS) \u00b6 MacOS comes with an SSH client installed. If you are on version Leopard 10.5.1 or lower, you may want to have the ssh-agent start automatically using the command sudo touch /var/db/useLS at a terminal window. Versions newer than Leopard 10.5.1 start the ssh-agent automatically. Install an SSH Client (Windows) \u00b6 There are several options for installing an SSH client on Windows, described below. It is highly recommended to install Windows Subsystem for Linux (WSL) as it provides a complete Linux environment within Windows. Windows Subsystem For Linux (WSL) \u00b6 Follow the instructions starting here to install Windows Subsystem for Linux. WSL shells do not automatically start or share the ssh-agent . To fix this we recommend installing keychain to automatically manage the ssh-agent . Run the following command depending on your Linux distribution. DEB-based (Debian, Ubuntu): sudo apt install keychain RPM-based (CentOS, Fedora, openSUSE): sudo yum install keychain Then modify the .*rc file for your shell, generally .bashrc or .zshrc , to automate the ssh-agent by adding the following line. - eval `keychain -q --eval --agents ssh` Tip You can access WSL files from within Windows in two ways. In the WSL terminal, enter explorer.exe . to open a File Explorer window in the current directory. In Windows, open a File Explorer window, click in the top navigation bar and enter \\\\wsl$ . Then select your distribution from the file window to access the filesystem of that WSL operating system. OpenSSH for Windows \u00b6 Follow the instructions here to install the OpenSSH client. Only install the OpenSSH server if you need it, otherwise skip that part. Once the OpenSSH client is installed, you'll want to enable the OpenSSH Agent service on your local machine to streamline adding and using keys. Open the Start Menu and search for \"Services\", and open the result shown in the image. Find the \"OpenSSH Authentication Agent\" service in the list. Double click it, or right-click it and select \"Properties\". In the dialog box, under the \"General\" tab, look for \"Startup Type\". Click the drop-down menu and select \"Automatic (Delayed Start)\". Click \"Apply\" at the bottom-right corner. This will cause the ssh-agent service to start when Windows starts. The \"Start\" button under the horizontal line should become enabled. Click it to start the ssh-agent service now. Git Bash terminal (Git for Windows) \u00b6 The fine folks at Git have worked very hard to package everything needed to use Git on Windows into one installer. This includes a Linux command line interface emulator, Bash and SSH. Visit https://git-scm.com to download and install. Follow the installer instructions. It is recommended to use all of the default installation options. Once installed, locate \"Git Bash\" on your machine to open the Bash terminal. It should be searchable in the Start Menu. To automate running ssh-agent add the following block to the file .bash_profile in the ~ directory within Git Bash. Then use source .bash_profile to start the ssh-agent , or open a new terminal. Note If such a file does not exist, please add it using nano .bash_profile to create a new file in the nano text editor. Copy and paste the block below into the text editor window. The character ^ means Ctrl . Use ^x ( Ctrl + X ) to exit, and continue following the prompts to save the file, using Ctrl shortcuts as needed. env = ~/.ssh/agent.env agent_load_env () { test -f \" $env \" && . \" $env \" > | /dev/null ; } agent_start () { ( umask 077 ; ssh-agent > | \" $env \" ) . \" $env \" > | /dev/null ; } agent_load_env # agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running agent_run_state = $( ssh-add -l > | /dev/null 2 > & 1 ; echo $? ) if [ ! \" $SSH_AUTH_SOCK \" ] || [ $agent_run_state = 2 ] ; then agent_start ssh-add elif [ \" $SSH_AUTH_SOCK \" ] && [ $agent_run_state = 1 ] ; then ssh-add fi Generating Key Pairs \u00b6 The instructions for generating key pairs are identical for all operating systems. GitHub maintains excellent documentation on generating key pairs. The gist of those instructions follows. Open a terminal window. Use the command ssh-keygen -t ed25519 -C \"your_email@example.com\" You will be prompted to choose a location to store the key, including a file name. You will be prompted to enter a passphrase to secure the key. It is highly recommended to secure your key pair with a passphrase to minimize risk. Managing Keys \u00b6 The instructions below are the same for all operating systems with one small exception noted below. Important If at any point you encounter an error like below, please check to be sure your ssh-agent is running based on how you Installed your SSH Client . Could not open a connection to your authentication agent. Starting the SSH Agent for a Single Session \u00b6 If ssh-agent isn't already running and you encounter an error, use the following commands to start the ssh-agent depending on your environment. It is highly recommended to use the most appropriate method described in Install an SSH Client to have ssh-agent start automatically. Linux, MacOS, Git Bash, WSL: eval $(ssh-agent -s) Windows OpenSSH: start-ssh-agent Add a Private Key \u00b6 Move the key file to the .ssh directory under your home directory. Navigate to the .ssh folder in a terminal window. Run ssh-add <private_key_file> Bug For Linux users and WSL on Windows users. If you experience a Warning: Unprotected Private Key File error when using ssh-add , your ssh file and directory permissions may be incorrect. To fix, please use the following commands. sudo chmod 600 ~/.ssh/<private_key_file> sudo chmod 644 ~/.ssh/known_hosts # if you have ever connected to a remote machine sudo chmod 644 ~/.ssh/config # if you have a config file sudo chmod 755 ~/.ssh Tip MacOS allows storing passphrases to the builtin Keychain with a special flag. Use ssh-add -K <path/to/private_key_file> to permanently store the passphrase that goes with the key file. Remove a Private Key \u00b6 Run ssh-add -d <path/to/private_key_file> Push a New Public Key File to a Remote Machine \u00b6 To push a new public key file to a remote machine, please use the ssh-copy-id command. If your ssh-agent is running and has a known-good private key added, then the command below will work as expected and add the <new_public_keyfile>.pub to the remote machine. You must also have the private key counterpart <new_private_keyfile> with the same name as the public key file, without the .pub extension. ssh-copy-id -i ~/.ssh/<new_public_keyfile> <user>@<remote_ip> The value <user> should be replaced with the remote user you will login as. The value <remote_ip> should be replaced with the IP address of the remote machine. To verify, use ssh -i ~/.ssh/<new_private_keyfile>.pub <user>@<remote_ip> . Remove an Invalid Host Fingerprint \u00b6 Danger The following command should only be run when reusing a floating IP for a new instance in a cloud context. Using it arbitrarily for remote machines you do not control can result in a security breach. Be absolutely certain you trust the source of the key change. A \"Remote Host Identification Has Changed\" error can be resolved by using the following command. It looks like the image below. Run ssh-keygen -R <hostname> where <hostname> is the URL or IP address of the remote machine. Setting up a Configuration File \u00b6 SSH configuration files help streamline the process of logging in to remote terminals by storing commonly-used arguments and flags for each host. To create a configuration file, navigate to your .ssh directory. Create a new plain text file called config with no extension. Open the file and add content like the following. Note that indent matters. Variable values in <> will be replaced with appropriate values before saving. Host <host> HostName <remote_ip> User <user> IdentityFile <absolute_path_to_private_key_file> Be sure to give a meaningful name under <host> so you can easily refer back to this config later and for ease of typing when using ssh with this configuration. Only letters, numbers, dashes and underscores are allowed, and it must start with a letter. The value <remote_ip> can be any remote machine relevant to your work. For cloud.rc it should be whatever IP was assigned in Creating a Floating IP . The value <user> should be whatever user name you will log in as. For cloud.rc, ubuntu or centos are typical, depending on instance operating system. The value <path_to_private_key_file> is the absolute path to the private key file, e.g. the path to your .ssh folder followed by the <private_key_file> file name. For cloud.rc this will be whatever private key file was generated in Creating a Key Pair . Save the config file. Start a new terminal and use the command ssh <host> , with no other flags, to test. SSH Client Usage \u00b6 If you've Set up a Configuration File , simply use ssh <host> , using the configuration name, to connect. If you haven't set up a configuration file, use the following. ssh <user>@<remote_ip> -i <private_key_file> Where user is the remote username, remote_ip is the IP address of the remote machine, and <private_key_file> is the private key file used for access the remote machine. See Generating Key Pairs for general instructions on creating a key pair, or Creating a Key Pair for cloud.rc specific instructions. Graphical Interface \u00b6 MobaXTerm \u00b6 Construction Under construction. Data Transfer \u00b6 SCP \u00b6 SCP stands for S ecure C o P y and works like the cp command, but allows transferring files and directories with remote machines. SCP is built on top of SSH and is installed with most SSH Clients. To install SCP, see Install an SSH Client . Warning The OpenSSH developers recommend using SFTP instead of SCP. Future releases of OpenSSH will have SCP use SFTP protocol. The value <user> is the user you will login as on the remote machine <hostname> . Note that if you are using an SSH Configuration File with Host <host> , then replace all of <user>@<hostname> with just <host> , as you would with SSH. scp <source_file> <user>@<hostname>:<destination_file> # single file scp <source_file> <user>@<hostname>: # retains the file name scp -r <source_directory> <user>@<hostname>:<destination_directory> # full directory scp -r <source_directory> <user>@<hostname>: # retains the directory name Examples: # file scp script.py cheaha: scp script.py user@cheaha.rc.uab.edu:~/existing/shared_script.py scp script.py user@<cloud_vm_ip>: # directory scp -r my_scripts/ user@cheaha.rc.uab.edu: scp -r my_scripts/ cheaha:my_shared_scripts/ SFTP \u00b6 SFTP stands for S ecure F ile T ransfer P rotocol and allows transferring files and directories with remote machines. SFTP is built on top of SSH and is installed with most SSH Clients. To install SFTP, see Install an SSH Client . SFTP works differently from SCP , as it has an interactive prompt. When connected to a remote, the prompt sftp> will appears and enable use of SFTP commands. SFTP can also be used in batch mode with the -b <batch_file> argument. The plaintext <batch_file> should contain one SFTP command per line. To connect, use sftp <user>@<hostname> where <user> is the user you will login as on the remote machine <hostname> . If you are using an SSH Configuration File with Host <host> , you may use sftp <host> . You may optionally use sftp <host>:/path/to/dir to start in a specific directory. Some examples of commands are given below. A complete list is available here # general commands sftp> pwd # remote current directory sftp> lpwd # local current directory sftp> ls # contents of remote pwd sftp> lls # contents of local pwd sftp> mkdir my_dir # create my_dir on remote sftp> lmkdir my_dir # create my_dir on local sftp> cd my_dir # change directory on remote sftp> lcd my_dir # change directory on local # copy to remote sftp> put <local_file> <optional_remote_path> sftp> put -r <local_directory> <optional_remote_path> # if optional remote path is not supplied, uses pwd # file sftp> put script.py # copies file to pwd, same name sftp> put script.py shared_script.py # copies file to pwd, renames sftp> put script.py all_scripts/ # copies to pwd existing subdirectory of pwd # directory, must use trailing '/' character! sftp> put -r my_scripts/ # copies directory to pwd sftp> put -r my_scripts/ all_scripts/ # copies to existing subdirectory of pwd # copy from remote, same syntax as put, reversed direction sftp> get <remote_file> <optional_local_path> sftp> get -r <remote_directory> <optional_local_path> # if optional local path is not supplied, uses lpwd RClone \u00b6 Construction Under construction.","title":"Remote Access"},{"location":"uab_cloud/remote_access/#remote-access-to-instances","text":"All of the access methods described below are built on top of ssh and require completion of the steps in Basic Security Setup to use with cloud.rc . Some of these steps are referenced in that document.","title":"Remote Access to Instances"},{"location":"uab_cloud/remote_access/#command-line-via-ssh","text":"SSH stands for S ecure SH ell and is a powerful tool for executing terminal commands on remote machines. It is widely used and ubiquitous, and a number of other technologies are built on top of SSH, like sftp and scp for transferring files. It is also the primary mode of command line communication with Research Computing technologies like Cheaha and cloud.rc.","title":"Command Line via SSH"},{"location":"uab_cloud/remote_access/#install-an-ssh-client","text":"There are two main steps to working with SSH efficiently. The first is to ensure you have an SSH client installed, which will let your local machine communicate with remote machines. The second is to ensure you have ssh-agent running in each terminal window to automate management of key files. The ssh-agent software comes with most SSH clients, but does not always run automatically. How to start the ssh-agent software automatically varies depending on operating system and shell flavor, which we will describe below.","title":"Install an SSH Client"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-linux","text":"Virtually all Linux distributions come with SSH preinstalled and configured appropriately for ease of use, including automatically starting the ssh-agent .","title":"Install an SSH Client (Linux)"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-macos","text":"MacOS comes with an SSH client installed. If you are on version Leopard 10.5.1 or lower, you may want to have the ssh-agent start automatically using the command sudo touch /var/db/useLS at a terminal window. Versions newer than Leopard 10.5.1 start the ssh-agent automatically.","title":"Install an SSH Client (MacOS)"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-windows","text":"There are several options for installing an SSH client on Windows, described below. It is highly recommended to install Windows Subsystem for Linux (WSL) as it provides a complete Linux environment within Windows.","title":"Install an SSH Client (Windows)"},{"location":"uab_cloud/remote_access/#windows-subsystem-for-linux-wsl","text":"Follow the instructions starting here to install Windows Subsystem for Linux. WSL shells do not automatically start or share the ssh-agent . To fix this we recommend installing keychain to automatically manage the ssh-agent . Run the following command depending on your Linux distribution. DEB-based (Debian, Ubuntu): sudo apt install keychain RPM-based (CentOS, Fedora, openSUSE): sudo yum install keychain Then modify the .*rc file for your shell, generally .bashrc or .zshrc , to automate the ssh-agent by adding the following line. - eval `keychain -q --eval --agents ssh` Tip You can access WSL files from within Windows in two ways. In the WSL terminal, enter explorer.exe . to open a File Explorer window in the current directory. In Windows, open a File Explorer window, click in the top navigation bar and enter \\\\wsl$ . Then select your distribution from the file window to access the filesystem of that WSL operating system.","title":"Windows Subsystem For Linux (WSL)"},{"location":"uab_cloud/remote_access/#openssh-for-windows","text":"Follow the instructions here to install the OpenSSH client. Only install the OpenSSH server if you need it, otherwise skip that part. Once the OpenSSH client is installed, you'll want to enable the OpenSSH Agent service on your local machine to streamline adding and using keys. Open the Start Menu and search for \"Services\", and open the result shown in the image. Find the \"OpenSSH Authentication Agent\" service in the list. Double click it, or right-click it and select \"Properties\". In the dialog box, under the \"General\" tab, look for \"Startup Type\". Click the drop-down menu and select \"Automatic (Delayed Start)\". Click \"Apply\" at the bottom-right corner. This will cause the ssh-agent service to start when Windows starts. The \"Start\" button under the horizontal line should become enabled. Click it to start the ssh-agent service now.","title":"OpenSSH for Windows"},{"location":"uab_cloud/remote_access/#git-bash-terminal-git-for-windows","text":"The fine folks at Git have worked very hard to package everything needed to use Git on Windows into one installer. This includes a Linux command line interface emulator, Bash and SSH. Visit https://git-scm.com to download and install. Follow the installer instructions. It is recommended to use all of the default installation options. Once installed, locate \"Git Bash\" on your machine to open the Bash terminal. It should be searchable in the Start Menu. To automate running ssh-agent add the following block to the file .bash_profile in the ~ directory within Git Bash. Then use source .bash_profile to start the ssh-agent , or open a new terminal. Note If such a file does not exist, please add it using nano .bash_profile to create a new file in the nano text editor. Copy and paste the block below into the text editor window. The character ^ means Ctrl . Use ^x ( Ctrl + X ) to exit, and continue following the prompts to save the file, using Ctrl shortcuts as needed. env = ~/.ssh/agent.env agent_load_env () { test -f \" $env \" && . \" $env \" > | /dev/null ; } agent_start () { ( umask 077 ; ssh-agent > | \" $env \" ) . \" $env \" > | /dev/null ; } agent_load_env # agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running agent_run_state = $( ssh-add -l > | /dev/null 2 > & 1 ; echo $? ) if [ ! \" $SSH_AUTH_SOCK \" ] || [ $agent_run_state = 2 ] ; then agent_start ssh-add elif [ \" $SSH_AUTH_SOCK \" ] && [ $agent_run_state = 1 ] ; then ssh-add fi","title":"Git Bash terminal (Git for Windows)"},{"location":"uab_cloud/remote_access/#generating-key-pairs","text":"The instructions for generating key pairs are identical for all operating systems. GitHub maintains excellent documentation on generating key pairs. The gist of those instructions follows. Open a terminal window. Use the command ssh-keygen -t ed25519 -C \"your_email@example.com\" You will be prompted to choose a location to store the key, including a file name. You will be prompted to enter a passphrase to secure the key. It is highly recommended to secure your key pair with a passphrase to minimize risk.","title":"Generating Key Pairs"},{"location":"uab_cloud/remote_access/#managing-keys","text":"The instructions below are the same for all operating systems with one small exception noted below. Important If at any point you encounter an error like below, please check to be sure your ssh-agent is running based on how you Installed your SSH Client . Could not open a connection to your authentication agent.","title":"Managing Keys"},{"location":"uab_cloud/remote_access/#starting-the-ssh-agent-for-a-single-session","text":"If ssh-agent isn't already running and you encounter an error, use the following commands to start the ssh-agent depending on your environment. It is highly recommended to use the most appropriate method described in Install an SSH Client to have ssh-agent start automatically. Linux, MacOS, Git Bash, WSL: eval $(ssh-agent -s) Windows OpenSSH: start-ssh-agent","title":"Starting the SSH Agent for a Single Session"},{"location":"uab_cloud/remote_access/#add-a-private-key","text":"Move the key file to the .ssh directory under your home directory. Navigate to the .ssh folder in a terminal window. Run ssh-add <private_key_file> Bug For Linux users and WSL on Windows users. If you experience a Warning: Unprotected Private Key File error when using ssh-add , your ssh file and directory permissions may be incorrect. To fix, please use the following commands. sudo chmod 600 ~/.ssh/<private_key_file> sudo chmod 644 ~/.ssh/known_hosts # if you have ever connected to a remote machine sudo chmod 644 ~/.ssh/config # if you have a config file sudo chmod 755 ~/.ssh Tip MacOS allows storing passphrases to the builtin Keychain with a special flag. Use ssh-add -K <path/to/private_key_file> to permanently store the passphrase that goes with the key file.","title":"Add a Private Key"},{"location":"uab_cloud/remote_access/#remove-a-private-key","text":"Run ssh-add -d <path/to/private_key_file>","title":"Remove a Private Key"},{"location":"uab_cloud/remote_access/#push-a-new-public-key-file-to-a-remote-machine","text":"To push a new public key file to a remote machine, please use the ssh-copy-id command. If your ssh-agent is running and has a known-good private key added, then the command below will work as expected and add the <new_public_keyfile>.pub to the remote machine. You must also have the private key counterpart <new_private_keyfile> with the same name as the public key file, without the .pub extension. ssh-copy-id -i ~/.ssh/<new_public_keyfile> <user>@<remote_ip> The value <user> should be replaced with the remote user you will login as. The value <remote_ip> should be replaced with the IP address of the remote machine. To verify, use ssh -i ~/.ssh/<new_private_keyfile>.pub <user>@<remote_ip> .","title":"Push a New Public Key File to a Remote Machine"},{"location":"uab_cloud/remote_access/#remove-an-invalid-host-fingerprint","text":"Danger The following command should only be run when reusing a floating IP for a new instance in a cloud context. Using it arbitrarily for remote machines you do not control can result in a security breach. Be absolutely certain you trust the source of the key change. A \"Remote Host Identification Has Changed\" error can be resolved by using the following command. It looks like the image below. Run ssh-keygen -R <hostname> where <hostname> is the URL or IP address of the remote machine.","title":"Remove an Invalid Host Fingerprint"},{"location":"uab_cloud/remote_access/#setting-up-a-configuration-file","text":"SSH configuration files help streamline the process of logging in to remote terminals by storing commonly-used arguments and flags for each host. To create a configuration file, navigate to your .ssh directory. Create a new plain text file called config with no extension. Open the file and add content like the following. Note that indent matters. Variable values in <> will be replaced with appropriate values before saving. Host <host> HostName <remote_ip> User <user> IdentityFile <absolute_path_to_private_key_file> Be sure to give a meaningful name under <host> so you can easily refer back to this config later and for ease of typing when using ssh with this configuration. Only letters, numbers, dashes and underscores are allowed, and it must start with a letter. The value <remote_ip> can be any remote machine relevant to your work. For cloud.rc it should be whatever IP was assigned in Creating a Floating IP . The value <user> should be whatever user name you will log in as. For cloud.rc, ubuntu or centos are typical, depending on instance operating system. The value <path_to_private_key_file> is the absolute path to the private key file, e.g. the path to your .ssh folder followed by the <private_key_file> file name. For cloud.rc this will be whatever private key file was generated in Creating a Key Pair . Save the config file. Start a new terminal and use the command ssh <host> , with no other flags, to test.","title":"Setting up a Configuration File"},{"location":"uab_cloud/remote_access/#ssh-client-usage","text":"If you've Set up a Configuration File , simply use ssh <host> , using the configuration name, to connect. If you haven't set up a configuration file, use the following. ssh <user>@<remote_ip> -i <private_key_file> Where user is the remote username, remote_ip is the IP address of the remote machine, and <private_key_file> is the private key file used for access the remote machine. See Generating Key Pairs for general instructions on creating a key pair, or Creating a Key Pair for cloud.rc specific instructions.","title":"SSH Client Usage"},{"location":"uab_cloud/remote_access/#graphical-interface","text":"","title":"Graphical Interface"},{"location":"uab_cloud/remote_access/#mobaxterm","text":"Construction Under construction.","title":"MobaXTerm"},{"location":"uab_cloud/remote_access/#data-transfer","text":"","title":"Data Transfer"},{"location":"uab_cloud/remote_access/#scp","text":"SCP stands for S ecure C o P y and works like the cp command, but allows transferring files and directories with remote machines. SCP is built on top of SSH and is installed with most SSH Clients. To install SCP, see Install an SSH Client . Warning The OpenSSH developers recommend using SFTP instead of SCP. Future releases of OpenSSH will have SCP use SFTP protocol. The value <user> is the user you will login as on the remote machine <hostname> . Note that if you are using an SSH Configuration File with Host <host> , then replace all of <user>@<hostname> with just <host> , as you would with SSH. scp <source_file> <user>@<hostname>:<destination_file> # single file scp <source_file> <user>@<hostname>: # retains the file name scp -r <source_directory> <user>@<hostname>:<destination_directory> # full directory scp -r <source_directory> <user>@<hostname>: # retains the directory name Examples: # file scp script.py cheaha: scp script.py user@cheaha.rc.uab.edu:~/existing/shared_script.py scp script.py user@<cloud_vm_ip>: # directory scp -r my_scripts/ user@cheaha.rc.uab.edu: scp -r my_scripts/ cheaha:my_shared_scripts/","title":"SCP"},{"location":"uab_cloud/remote_access/#sftp","text":"SFTP stands for S ecure F ile T ransfer P rotocol and allows transferring files and directories with remote machines. SFTP is built on top of SSH and is installed with most SSH Clients. To install SFTP, see Install an SSH Client . SFTP works differently from SCP , as it has an interactive prompt. When connected to a remote, the prompt sftp> will appears and enable use of SFTP commands. SFTP can also be used in batch mode with the -b <batch_file> argument. The plaintext <batch_file> should contain one SFTP command per line. To connect, use sftp <user>@<hostname> where <user> is the user you will login as on the remote machine <hostname> . If you are using an SSH Configuration File with Host <host> , you may use sftp <host> . You may optionally use sftp <host>:/path/to/dir to start in a specific directory. Some examples of commands are given below. A complete list is available here # general commands sftp> pwd # remote current directory sftp> lpwd # local current directory sftp> ls # contents of remote pwd sftp> lls # contents of local pwd sftp> mkdir my_dir # create my_dir on remote sftp> lmkdir my_dir # create my_dir on local sftp> cd my_dir # change directory on remote sftp> lcd my_dir # change directory on local # copy to remote sftp> put <local_file> <optional_remote_path> sftp> put -r <local_directory> <optional_remote_path> # if optional remote path is not supplied, uses pwd # file sftp> put script.py # copies file to pwd, same name sftp> put script.py shared_script.py # copies file to pwd, renames sftp> put script.py all_scripts/ # copies to pwd existing subdirectory of pwd # directory, must use trailing '/' character! sftp> put -r my_scripts/ # copies directory to pwd sftp> put -r my_scripts/ all_scripts/ # copies to existing subdirectory of pwd # copy from remote, same syntax as put, reversed direction sftp> get <remote_file> <optional_local_path> sftp> get -r <remote_directory> <optional_local_path> # if optional local path is not supplied, uses lpwd","title":"SFTP"},{"location":"uab_cloud/remote_access/#rclone","text":"Construction Under construction.","title":"RClone"},{"location":"uab_cloud/security_setup_basic/","text":"Security Policy Setup and Tutorial \u00b6 These instructions show you how to prepare to use SSH with your instances. At a minimum, an SSH security group and at least one key pair must be created. Other security groups can and should be added as needed for additional services. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Creating a Security Group \u00b6 Security Groups are used to set rules for how external devices can connect to your instances. Here we will create an SSH Security Group using a method that can be applied to other types of connections. Applications you develop may need other ports opened, so you may need to create additional security groups to handle those. Security groups may be reused across multiple instances. Click \"Networks\" in the left-hand navigation pane to open the fold-out menu. Click \"Security Groups\" in the fold out menu. Click \"+ Create Security Group\" to open a dialog box. Fill out the dialog box. Under \"Name\" enter ssh . Leave \"Description\" empty. Click \"Create Security Group\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be an entry for \"Egress IPv4\" and \"Egress IPv6\". Leave these alone. Click \"+ Add Rule\" to open a dialog box. Select \"SSH\" in the \"Rule\" drop down box. This will change the remaining fields. Leave \"Description\" empty. Select \"CIDR\" in the \"Remote\" drop down box. Type 0.0.0.0/0 in the \"CIDR\" box. For the sake of this tutorial, this value is fine. For properly securing virtual machines, see the \"Warning\" below for more information on better practice. Warning Using the value 0.0.0.0/0 for CIDR is short-hand for saying \"All possible IP addresses\". While cloud.rc is protected from external sources by the UAB firewall, using 0.0.0.0/0 does expose your virtual machine to all machines on the UAB internal network. Using the value 0.0.0.0/0 is the same as saying \"I trust the UAB firewall to protect my VM, and I trust UAB faculty, staff and students to not harm my VM\". Better practice is to limit the CIDR scope to only the IP address ranges that are relevant to your goals. As with all of cybersecurity, there is a security/convenience tradeoff to be made, and properly scoping CIDR will take more work than just using 0.0.0.0/0 . CIDR calculators are available on the internet to assist with calculation, just search for CIDR Calculator . Click \"Add\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be a new entry in the table. Deleting a Security Group \u00b6 Note Deleting Security Groups is not part of the tutorial, and is here as a reference. To delete a security group, return to the \"Security Groups\" page using the left-hand navigation pane. In the table, find the row with the security group you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Security Group\" to open a confirmation dialog. Click \"Delete Security Group\" again to delete the security group permanently. Creating a Key pair \u00b6 A Key Pair is required for SSH access to cloud.rc instances for security reasons. To use a Key Pair and SSH, you will need to Install an SSH Client on your local machine. Key Pairs are security devices used to authenticate and connect to a remote machine, like Cheaha or cloud.rc instances, and use Public-key cryptography to encrypt the connection. As the name suggests, there are two parts: a public key which is placed on the remote machine, and a private key which is kept secret on your personal machine. While key pairs can be reused between instances, we highly recommend using a new key pair with each instance to minimize risk if a private key becomes compromised. See Good Practices for more information. There are two ways to create a key pair: Use the cloud.rc interface to generate a key pair remotely and download the private key file . Use your personal computer to generate a key pair locally and upload the public key file. Good Practices \u00b6 Good practice is to only use one key pair per person and per local machine. So if you have two computers, each will need its own key pair. Using the same key pair for multiple machines means that they all become compromised when that key is compromised. Using different key pairs for each means only one machine becomes compromised. If you have two people, each will need their own key pair. Private keys are secrets and should not be passed between people, because there is no way to control it once it has been shared with even one other person. Copying the key increases the risk of the system being compromised by an attacker. If the key has to be revoked, you revoke access for every user at once. If you must share access, create a key pair for each additional person to increase security and convenience. Using a password protected Key Pair is highly recommended for additional security, as it buys time to revoke a key pair if it is compromised by an attacker. Currently, this is only possible by uploading a custom public key generated on your local machine. Generating a Key Pair on cloud.rc \u00b6 Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Click \"+ Create Key Pair\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop down box. Click \"+ Create Key Pair\" Opens a download file dialog box in your browser to download a file containing the secret private key. The file may have extension .pem or .crt depending on your operating system. Download the private key file. For security reasons this will be your only chance to ever obtain the private key from cloud.rc. If you lose this file you will have to generate a new Key Pair. Redirects to the \"Key Pairs\" page. There should be a new entry in the table. To add the private key on your local machine please see \"Add key\" under Add a Private Key . Generating a Key Pair Locally \u00b6 To generate a key pair, see instructions located at Generating Key Pairs . Click \"Import Public Key\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop-down box. Click \"Browse...\" to upload a public key file from your custom key pair OR copy-paste the content of that key file into the \"Public Key\" box. Danger Do not upload your private key file! The private key file must stay secret to ensure proper security, and it should never leave the computer it was generated on. Using the Key Pair \u00b6 Please see SSH Into the Instance for more information on using the Key Pair. Revoking a Key Pair \u00b6 Note Revoking Key Pairs is not part of the tutorial, and is here as a reference. Revoking a key pair from cloud.rc is simple. First, log on to the interface. Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Find the key pair you wish to revoke and click the \"Delete Key Pair\" button in that row. Optionally, Remove the Private Key from your local machine. This step is not necessary to ensure security, but can help maintain a clean environment. Continuing the Tutorial \u00b6 Now that you've set up a Network and Security Policies , you're ready to create a virtual machine (VM) Instance to work with. To continue the tutorial, please visit Instances next.","title":"(2) Security Policies"},{"location":"uab_cloud/security_setup_basic/#security-policy-setup-and-tutorial","text":"These instructions show you how to prepare to use SSH with your instances. At a minimum, an SSH security group and at least one key pair must be created. Other security groups can and should be added as needed for additional services. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.","title":"Security Policy Setup and Tutorial"},{"location":"uab_cloud/security_setup_basic/#creating-a-security-group","text":"Security Groups are used to set rules for how external devices can connect to your instances. Here we will create an SSH Security Group using a method that can be applied to other types of connections. Applications you develop may need other ports opened, so you may need to create additional security groups to handle those. Security groups may be reused across multiple instances. Click \"Networks\" in the left-hand navigation pane to open the fold-out menu. Click \"Security Groups\" in the fold out menu. Click \"+ Create Security Group\" to open a dialog box. Fill out the dialog box. Under \"Name\" enter ssh . Leave \"Description\" empty. Click \"Create Security Group\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be an entry for \"Egress IPv4\" and \"Egress IPv6\". Leave these alone. Click \"+ Add Rule\" to open a dialog box. Select \"SSH\" in the \"Rule\" drop down box. This will change the remaining fields. Leave \"Description\" empty. Select \"CIDR\" in the \"Remote\" drop down box. Type 0.0.0.0/0 in the \"CIDR\" box. For the sake of this tutorial, this value is fine. For properly securing virtual machines, see the \"Warning\" below for more information on better practice. Warning Using the value 0.0.0.0/0 for CIDR is short-hand for saying \"All possible IP addresses\". While cloud.rc is protected from external sources by the UAB firewall, using 0.0.0.0/0 does expose your virtual machine to all machines on the UAB internal network. Using the value 0.0.0.0/0 is the same as saying \"I trust the UAB firewall to protect my VM, and I trust UAB faculty, staff and students to not harm my VM\". Better practice is to limit the CIDR scope to only the IP address ranges that are relevant to your goals. As with all of cybersecurity, there is a security/convenience tradeoff to be made, and properly scoping CIDR will take more work than just using 0.0.0.0/0 . CIDR calculators are available on the internet to assist with calculation, just search for CIDR Calculator . Click \"Add\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be a new entry in the table.","title":"Creating a Security Group"},{"location":"uab_cloud/security_setup_basic/#deleting-a-security-group","text":"Note Deleting Security Groups is not part of the tutorial, and is here as a reference. To delete a security group, return to the \"Security Groups\" page using the left-hand navigation pane. In the table, find the row with the security group you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Security Group\" to open a confirmation dialog. Click \"Delete Security Group\" again to delete the security group permanently.","title":"Deleting a Security Group"},{"location":"uab_cloud/security_setup_basic/#creating-a-key-pair","text":"A Key Pair is required for SSH access to cloud.rc instances for security reasons. To use a Key Pair and SSH, you will need to Install an SSH Client on your local machine. Key Pairs are security devices used to authenticate and connect to a remote machine, like Cheaha or cloud.rc instances, and use Public-key cryptography to encrypt the connection. As the name suggests, there are two parts: a public key which is placed on the remote machine, and a private key which is kept secret on your personal machine. While key pairs can be reused between instances, we highly recommend using a new key pair with each instance to minimize risk if a private key becomes compromised. See Good Practices for more information. There are two ways to create a key pair: Use the cloud.rc interface to generate a key pair remotely and download the private key file . Use your personal computer to generate a key pair locally and upload the public key file.","title":"Creating a Key pair"},{"location":"uab_cloud/security_setup_basic/#good-practices","text":"Good practice is to only use one key pair per person and per local machine. So if you have two computers, each will need its own key pair. Using the same key pair for multiple machines means that they all become compromised when that key is compromised. Using different key pairs for each means only one machine becomes compromised. If you have two people, each will need their own key pair. Private keys are secrets and should not be passed between people, because there is no way to control it once it has been shared with even one other person. Copying the key increases the risk of the system being compromised by an attacker. If the key has to be revoked, you revoke access for every user at once. If you must share access, create a key pair for each additional person to increase security and convenience. Using a password protected Key Pair is highly recommended for additional security, as it buys time to revoke a key pair if it is compromised by an attacker. Currently, this is only possible by uploading a custom public key generated on your local machine.","title":"Good Practices"},{"location":"uab_cloud/security_setup_basic/#generating-a-key-pair-on-cloudrc","text":"Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Click \"+ Create Key Pair\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop down box. Click \"+ Create Key Pair\" Opens a download file dialog box in your browser to download a file containing the secret private key. The file may have extension .pem or .crt depending on your operating system. Download the private key file. For security reasons this will be your only chance to ever obtain the private key from cloud.rc. If you lose this file you will have to generate a new Key Pair. Redirects to the \"Key Pairs\" page. There should be a new entry in the table. To add the private key on your local machine please see \"Add key\" under Add a Private Key .","title":"Generating a Key Pair on cloud.rc"},{"location":"uab_cloud/security_setup_basic/#generating-a-key-pair-locally","text":"To generate a key pair, see instructions located at Generating Key Pairs . Click \"Import Public Key\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop-down box. Click \"Browse...\" to upload a public key file from your custom key pair OR copy-paste the content of that key file into the \"Public Key\" box. Danger Do not upload your private key file! The private key file must stay secret to ensure proper security, and it should never leave the computer it was generated on.","title":"Generating a Key Pair Locally"},{"location":"uab_cloud/security_setup_basic/#using-the-key-pair","text":"Please see SSH Into the Instance for more information on using the Key Pair.","title":"Using the Key Pair"},{"location":"uab_cloud/security_setup_basic/#revoking-a-key-pair","text":"Note Revoking Key Pairs is not part of the tutorial, and is here as a reference. Revoking a key pair from cloud.rc is simple. First, log on to the interface. Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Find the key pair you wish to revoke and click the \"Delete Key Pair\" button in that row. Optionally, Remove the Private Key from your local machine. This step is not necessary to ensure security, but can help maintain a clean environment.","title":"Revoking a Key Pair"},{"location":"uab_cloud/security_setup_basic/#continuing-the-tutorial","text":"Now that you've set up a Network and Security Policies , you're ready to create a virtual machine (VM) Instance to work with. To continue the tutorial, please visit Instances next.","title":"Continuing the Tutorial"},{"location":"uab_cloud/snapshots/","text":"Working with Snapshots \u00b6 Snapshots are instances or volumes frozen at a moment in time, able to be used in the future. Think of snapshots as a photograph of the state of an instance or volume. Anything done to an instance or volume after the snapshot is taken won't affect the snapshot. We can also create a new instance or volume from an existing snapshot, and continue from that point in time. An instance snapshot is referred to as an image. Volume snapshots do not have a special name. Images or Instance Snapshots \u00b6 Creating an Image \u00b6 Images are a helpful way to store the state of an instance for later use. Repeating tedious tasks like Software Installs can be avoided by taking a snapshot at a known-good point during set up of an instance environment, saving time in the future if something goes wrong. Images may also be shared with other users to simplify workflows and onboarding new collaborators. To create an image please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. To take a snapshot of a particular instance, click the drop down menu under the \"Actions\" column in the row of the desired instance. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Images\" page, where your new image will appear in its own row in the table. Note Notice the image has a size of zero bytes, which is expected and does not affect the ability to create instances. Images are a convenience pointer to the underlying volume snapshot, so they have no size themselves. The underlying volume snapshot does have a fixed size. To see the size of the underlying volume snapshot, click \"Volumes\" and then \"Snapshots\" in the left hand navigation menu. Creating an Instance from an Image \u00b6 To create an instance from an image, follow the directions below, assuming you have Created an Image . Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. Click the \"Launch Instance\" button. A dialog box will open. Follow the instructions at Basic Instance Setup until you get to the \"Source\" tab. In the \"Source\" tab, select \"Instance Snapshot\" under the \"Select Boot Source\" drop down menu. The \"Available\" table will change, and should contain your previously created instance snapshots. Press the up arrow in the appropriate row of the \"Available\" table to move that instance snapshot to the \"Allocated\" table. Note On the \"Flavor\" tab, only flavors with large enough disk capacity to hold the snapshot will be allowed. Flavors that are too small will show a yellow triangular caution symbol. Examples are shown below for a 40 GB instance snapshot. Continue following the instructions at Basic Instance Setup to start the instance. Sharing Instance Snapshots \u00b6 Construction Under construction. Deleting an Image \u00b6 To delete an image, return to the \"Images\" page using the left-hand navigation pane. In the table, find the row with the image you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Image\" to open a confirmation dialog. Click \"Delete Image\" again to delete the image permanently. Important You will not be able to delete the image if it has an associated volume snapshot or volume . They will need to be removed or deleted first. Volume Snapshots \u00b6 Creating a Volume Snapshot \u00b6 Volume snapshots are a helpful way to store the state of a volume for later use. They are used as the backing for Images, or Instance Snapshots , and have the same benefits. Most volume snapshots are created as part of an instance, but to create a volume snapshot directly please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Volumes\" and then \"Volumes\" in the left-hand navigation menu to open the \"Volumes\" page. To take a snapshot of a particular volume, click the drop down menu under the \"Actions\" column in the row of the desired volume. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Volume Snapshots\" page, where your new snapshot will appear in its own row in the table. Sharing Volume Snapshots \u00b6 Construction Under construction. Deleting a Volume Snapshot \u00b6 To delete a volume snapshot, return to the \"Volume Snapshots\" page using the left-hand navigation pane. In the table, find the row with the volume snapshot you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume Snapshot\" to open a confirmation dialog. Click \"Delete Volume Snapshot\" again to delete the volume snapshot permanently.","title":"Snapshots and Images"},{"location":"uab_cloud/snapshots/#working-with-snapshots","text":"Snapshots are instances or volumes frozen at a moment in time, able to be used in the future. Think of snapshots as a photograph of the state of an instance or volume. Anything done to an instance or volume after the snapshot is taken won't affect the snapshot. We can also create a new instance or volume from an existing snapshot, and continue from that point in time. An instance snapshot is referred to as an image. Volume snapshots do not have a special name.","title":"Working with Snapshots"},{"location":"uab_cloud/snapshots/#images-or-instance-snapshots","text":"","title":"Images or Instance Snapshots"},{"location":"uab_cloud/snapshots/#creating-an-image","text":"Images are a helpful way to store the state of an instance for later use. Repeating tedious tasks like Software Installs can be avoided by taking a snapshot at a known-good point during set up of an instance environment, saving time in the future if something goes wrong. Images may also be shared with other users to simplify workflows and onboarding new collaborators. To create an image please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. To take a snapshot of a particular instance, click the drop down menu under the \"Actions\" column in the row of the desired instance. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Images\" page, where your new image will appear in its own row in the table. Note Notice the image has a size of zero bytes, which is expected and does not affect the ability to create instances. Images are a convenience pointer to the underlying volume snapshot, so they have no size themselves. The underlying volume snapshot does have a fixed size. To see the size of the underlying volume snapshot, click \"Volumes\" and then \"Snapshots\" in the left hand navigation menu.","title":"Creating an Image"},{"location":"uab_cloud/snapshots/#creating-an-instance-from-an-image","text":"To create an instance from an image, follow the directions below, assuming you have Created an Image . Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. Click the \"Launch Instance\" button. A dialog box will open. Follow the instructions at Basic Instance Setup until you get to the \"Source\" tab. In the \"Source\" tab, select \"Instance Snapshot\" under the \"Select Boot Source\" drop down menu. The \"Available\" table will change, and should contain your previously created instance snapshots. Press the up arrow in the appropriate row of the \"Available\" table to move that instance snapshot to the \"Allocated\" table. Note On the \"Flavor\" tab, only flavors with large enough disk capacity to hold the snapshot will be allowed. Flavors that are too small will show a yellow triangular caution symbol. Examples are shown below for a 40 GB instance snapshot. Continue following the instructions at Basic Instance Setup to start the instance.","title":"Creating an Instance from an Image"},{"location":"uab_cloud/snapshots/#sharing-instance-snapshots","text":"Construction Under construction.","title":"Sharing Instance Snapshots"},{"location":"uab_cloud/snapshots/#deleting-an-image","text":"To delete an image, return to the \"Images\" page using the left-hand navigation pane. In the table, find the row with the image you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Image\" to open a confirmation dialog. Click \"Delete Image\" again to delete the image permanently. Important You will not be able to delete the image if it has an associated volume snapshot or volume . They will need to be removed or deleted first.","title":"Deleting an Image"},{"location":"uab_cloud/snapshots/#volume-snapshots","text":"","title":"Volume Snapshots"},{"location":"uab_cloud/snapshots/#creating-a-volume-snapshot","text":"Volume snapshots are a helpful way to store the state of a volume for later use. They are used as the backing for Images, or Instance Snapshots , and have the same benefits. Most volume snapshots are created as part of an instance, but to create a volume snapshot directly please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Volumes\" and then \"Volumes\" in the left-hand navigation menu to open the \"Volumes\" page. To take a snapshot of a particular volume, click the drop down menu under the \"Actions\" column in the row of the desired volume. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Volume Snapshots\" page, where your new snapshot will appear in its own row in the table.","title":"Creating a Volume Snapshot"},{"location":"uab_cloud/snapshots/#sharing-volume-snapshots","text":"Construction Under construction.","title":"Sharing Volume Snapshots"},{"location":"uab_cloud/snapshots/#deleting-a-volume-snapshot","text":"To delete a volume snapshot, return to the \"Volume Snapshots\" page using the left-hand navigation pane. In the table, find the row with the volume snapshot you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume Snapshot\" to open a confirmation dialog. Click \"Delete Volume Snapshot\" again to delete the volume snapshot permanently.","title":"Deleting a Volume Snapshot"},{"location":"uab_cloud/volume_setup_basic/","text":"Volume Setup and Tutorial \u00b6 These instructions are intended for researchers who want to setup a persistent volume for use across instances. To follow these instructions you'll need to have already setup an Instance . Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Creating a Volume \u00b6 Click the \"Volumes\" fold-out in the left-hand navigation pane - the fold-out should open. Click \"Volumes\" within the fold-out to open the \"Volumes\" table page. Click \"+ Create Volume\" to open a dialog box. Fill out the dialog box. Enter a \"Volume Name\". See Naming Conventions . Enter a \"Description\". Select \"No source, empty volume\" in the \"Volume Source\" drop-down box to create an empty volume. Select \"__DEFAULT__\" in the \"Type\" drop down box. Select a size in GB appropriate for your needs. Select \"nova\" in the \"Availability Zone\" drop down box. Select \"No group\" in the \"Group\" drop down box. Click \"Create Volume\" Returns to the \"Volumes\" table page. There will be a new entry in the \"Volumes\" table. Attaching a Volume to a Running Instance \u00b6 To attach a volume you must have already created at least one using the cloud.rc interface. More information can be found in [link] Open the instances table by clicking \"Compute\" in the left-hand navigation pane and clicking \"Instances\". In the \"Actions\" column entry, click the drop down triangle button and select \"Attach Volume\". A dialog box will open. Select a volume in the \"Volume ID\" drop down box. Click \"Attach Volume\". Now the volume should be attached to the instance. From here you may format the volume and mount it. Formatting a Volume \u00b6 To format a volume, you must have created a volume and attached it to an instance capable of formatting it correctly. These instructions assume a Linux operating system. Click \"Compute\" in the left-hand navigation pane, then open the \"Instances\" menu. Click the name of any instance you wish to use to format the volume. Then click \"Overview\". Scroll down to \"Volumes Attached\" and make note of the <mount> part of <volume-name> on <mount> for your attached volume as it will be used in later steps. SSH into the instance from your local machine or from Cheaha. Verify the volume is attached by using sudo fdisk -l | egrep \"<mount>\"\" Format the volume using sudo fdisk \"<mount>\" You will be in the fdisk utility. Enter n to create a new partition. Enter p to make it the primary partition. Enter numeral 1 to make it the first partition. Press enter to accept the default first sector. Press enter to accept the default last sector. Enter t to change partition type. Enter numerals 83 to change to Linux partition type. Enter p to display the partition setup. Note that the partition will be labeled <mount>1 . This literally whatever <mount> was from earlier followed by the numeral 1 . Further steps will refer to this as <pmount> Enter w to execute the setup prepared in the previous substeps. Verify the volume is not mounted using sudo mount | egrep \"<mount>\" . If there is no output, then move to the next step. If there is some output then use sudo umount -l \"<mount>\" to unmount the volume and verify again. Create the filesystem using sudo mkfs.ext4 \"<pmount>\" . Ensure that the output looks like the following: ubuntu@my-instance:~$ sudo mkfs.ext4 /dev/vdb1 mke2fs 1 .45.5 ( 07 -Jan-2020 ) Discarding device blocks: done Creating filesystem with 26214144 4k blocks and 6553600 inodes Filesystem UUID: 335704a9-2435-440a-aeea-8ae29438ac64 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 , 1605632 , 654208 , 4096000 , 7962624 , 11239424 , 20480000 , 23887872 Allocating group tables: done Writing inode tables: done Creating journal ( 131072 blocks ) : done Writing superblocks and filesystem accounting information: done The volume is now formatted and ready for mounting within an attached instance OS. You will need to make note of <pmount> for when you are ready to mount the volume to an instance. Mounting a Volume in an Instance \u00b6 Mounting a volume needs to be done once per instance it will be attached to. It is assumed you've already created and formatted a volume and attached it to some instance. You'll need the <pmount> label from when you formatted the volume. SSH into the instance from your local machine or from Cheaha. Obtain the uuid of the volume using sudo blkid | egrep \"<pmount>\" . This will be referred to as <uuid> in future steps. Create a directory to mount the volume as. A good choice is sudo mkdir /mnt/<volume-name> where <volume-name> is something meaningful for you or your project. This directory will be referred to as <directory> in future steps. Mount the volume to the directory using sudo mount -U <uuid> <directory> . Verify the volume is mounted using df -h | egrep <pmount> Edit the fstab file to make mounting persistent across instance reboots. Edit the file using sudo nano /etc/fstab . Add the following line to the file: /dev/disk/by-uuid/<uuid> <directory> auto defaults,nofail 0 3 Verify fstab was modified correctly by soft rebooting the instance and verifying the mount again using df -h | egrep \"<pmount>\" . Set access control using the following commands: sudo apt install acl # or yum install, etc., if not already installed sudo setfacl -R -m u:<username>:rwx <directory> Verify the access controls were modified correctly by creating a test file and then listing files in <directory> to ensure the file was created. The following commands will achieve this: cd <directory> touch testfile ls The volume is now mounted to your instance and ready for use and re-use across sessions and reboots. Deleting a Volume \u00b6 Note Deleting a Volume is not part of the tutorial, and is here as a reference. To delete a volume, return to the \"Volumes\" page using the left-hand navigation pane. In the table, find the row with the volume you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume\" to open a confirmation dialog. Click \"Delete Volume\" again to delete the volume permanently. Important It will not be possible to delete a volume if it has an associated volume snapshot . The snapshot will need to be deleted first.","title":"(4) Volumes (optional)"},{"location":"uab_cloud/volume_setup_basic/#volume-setup-and-tutorial","text":"These instructions are intended for researchers who want to setup a persistent volume for use across instances. To follow these instructions you'll need to have already setup an Instance . Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake.","title":"Volume Setup and Tutorial"},{"location":"uab_cloud/volume_setup_basic/#creating-a-volume","text":"Click the \"Volumes\" fold-out in the left-hand navigation pane - the fold-out should open. Click \"Volumes\" within the fold-out to open the \"Volumes\" table page. Click \"+ Create Volume\" to open a dialog box. Fill out the dialog box. Enter a \"Volume Name\". See Naming Conventions . Enter a \"Description\". Select \"No source, empty volume\" in the \"Volume Source\" drop-down box to create an empty volume. Select \"__DEFAULT__\" in the \"Type\" drop down box. Select a size in GB appropriate for your needs. Select \"nova\" in the \"Availability Zone\" drop down box. Select \"No group\" in the \"Group\" drop down box. Click \"Create Volume\" Returns to the \"Volumes\" table page. There will be a new entry in the \"Volumes\" table.","title":"Creating a Volume"},{"location":"uab_cloud/volume_setup_basic/#attaching-a-volume-to-a-running-instance","text":"To attach a volume you must have already created at least one using the cloud.rc interface. More information can be found in [link] Open the instances table by clicking \"Compute\" in the left-hand navigation pane and clicking \"Instances\". In the \"Actions\" column entry, click the drop down triangle button and select \"Attach Volume\". A dialog box will open. Select a volume in the \"Volume ID\" drop down box. Click \"Attach Volume\". Now the volume should be attached to the instance. From here you may format the volume and mount it.","title":"Attaching a Volume to a Running Instance"},{"location":"uab_cloud/volume_setup_basic/#formatting-a-volume","text":"To format a volume, you must have created a volume and attached it to an instance capable of formatting it correctly. These instructions assume a Linux operating system. Click \"Compute\" in the left-hand navigation pane, then open the \"Instances\" menu. Click the name of any instance you wish to use to format the volume. Then click \"Overview\". Scroll down to \"Volumes Attached\" and make note of the <mount> part of <volume-name> on <mount> for your attached volume as it will be used in later steps. SSH into the instance from your local machine or from Cheaha. Verify the volume is attached by using sudo fdisk -l | egrep \"<mount>\"\" Format the volume using sudo fdisk \"<mount>\" You will be in the fdisk utility. Enter n to create a new partition. Enter p to make it the primary partition. Enter numeral 1 to make it the first partition. Press enter to accept the default first sector. Press enter to accept the default last sector. Enter t to change partition type. Enter numerals 83 to change to Linux partition type. Enter p to display the partition setup. Note that the partition will be labeled <mount>1 . This literally whatever <mount> was from earlier followed by the numeral 1 . Further steps will refer to this as <pmount> Enter w to execute the setup prepared in the previous substeps. Verify the volume is not mounted using sudo mount | egrep \"<mount>\" . If there is no output, then move to the next step. If there is some output then use sudo umount -l \"<mount>\" to unmount the volume and verify again. Create the filesystem using sudo mkfs.ext4 \"<pmount>\" . Ensure that the output looks like the following: ubuntu@my-instance:~$ sudo mkfs.ext4 /dev/vdb1 mke2fs 1 .45.5 ( 07 -Jan-2020 ) Discarding device blocks: done Creating filesystem with 26214144 4k blocks and 6553600 inodes Filesystem UUID: 335704a9-2435-440a-aeea-8ae29438ac64 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 , 1605632 , 654208 , 4096000 , 7962624 , 11239424 , 20480000 , 23887872 Allocating group tables: done Writing inode tables: done Creating journal ( 131072 blocks ) : done Writing superblocks and filesystem accounting information: done The volume is now formatted and ready for mounting within an attached instance OS. You will need to make note of <pmount> for when you are ready to mount the volume to an instance.","title":"Formatting a Volume"},{"location":"uab_cloud/volume_setup_basic/#mounting-a-volume-in-an-instance","text":"Mounting a volume needs to be done once per instance it will be attached to. It is assumed you've already created and formatted a volume and attached it to some instance. You'll need the <pmount> label from when you formatted the volume. SSH into the instance from your local machine or from Cheaha. Obtain the uuid of the volume using sudo blkid | egrep \"<pmount>\" . This will be referred to as <uuid> in future steps. Create a directory to mount the volume as. A good choice is sudo mkdir /mnt/<volume-name> where <volume-name> is something meaningful for you or your project. This directory will be referred to as <directory> in future steps. Mount the volume to the directory using sudo mount -U <uuid> <directory> . Verify the volume is mounted using df -h | egrep <pmount> Edit the fstab file to make mounting persistent across instance reboots. Edit the file using sudo nano /etc/fstab . Add the following line to the file: /dev/disk/by-uuid/<uuid> <directory> auto defaults,nofail 0 3 Verify fstab was modified correctly by soft rebooting the instance and verifying the mount again using df -h | egrep \"<pmount>\" . Set access control using the following commands: sudo apt install acl # or yum install, etc., if not already installed sudo setfacl -R -m u:<username>:rwx <directory> Verify the access controls were modified correctly by creating a test file and then listing files in <directory> to ensure the file was created. The following commands will achieve this: cd <directory> touch testfile ls The volume is now mounted to your instance and ready for use and re-use across sessions and reboots.","title":"Mounting a Volume in an Instance"},{"location":"uab_cloud/volume_setup_basic/#deleting-a-volume","text":"Note Deleting a Volume is not part of the tutorial, and is here as a reference. To delete a volume, return to the \"Volumes\" page using the left-hand navigation pane. In the table, find the row with the volume you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume\" to open a confirmation dialog. Click \"Delete Volume\" again to delete the volume permanently. Important It will not be possible to delete a volume if it has an associated volume snapshot . The snapshot will need to be deleted first.","title":"Deleting a Volume"},{"location":"workflow_solutions/getting_containers/","text":"Fantastic Containers and Where to Find Them \u00b6 Construction This page is a stub and is under construction. Container Repositories \u00b6 UAB GitLab Container Registry \u00b6 DockerHub \u00b6 Podman et al. \u00b6 Using Containers \u00b6 Using containers on Cheaha \u00b6 Using containers on UAB Cloud (cloud.rc) \u00b6 Using containers on UAB Kubernetes \u00b6","title":"Getting Containers"},{"location":"workflow_solutions/getting_containers/#fantastic-containers-and-where-to-find-them","text":"Construction This page is a stub and is under construction.","title":"Fantastic Containers and Where to Find Them"},{"location":"workflow_solutions/getting_containers/#container-repositories","text":"","title":"Container Repositories"},{"location":"workflow_solutions/getting_containers/#uab-gitlab-container-registry","text":"","title":"UAB GitLab Container Registry"},{"location":"workflow_solutions/getting_containers/#dockerhub","text":"","title":"DockerHub"},{"location":"workflow_solutions/getting_containers/#podman-et-al","text":"","title":"Podman et al."},{"location":"workflow_solutions/getting_containers/#using-containers","text":"","title":"Using Containers"},{"location":"workflow_solutions/getting_containers/#using-containers-on-cheaha","text":"","title":"Using containers on Cheaha"},{"location":"workflow_solutions/getting_containers/#using-containers-on-uab-cloud-cloudrc","text":"","title":"Using containers on UAB Cloud (cloud.rc)"},{"location":"workflow_solutions/getting_containers/#using-containers-on-uab-kubernetes","text":"","title":"Using containers on UAB Kubernetes"},{"location":"workflow_solutions/getting_software_with_git/","text":"Git \u00b6 Construction This page is a stub and is under construction.","title":"Getting Software from GitHub and GitLab"},{"location":"workflow_solutions/getting_software_with_git/#git","text":"Construction This page is a stub and is under construction.","title":"Git"},{"location":"workflow_solutions/shell/","text":"Shell Reference \u00b6 Introductory Guides \u00b6 The shell is a powerful tool, and with great power comes great responsibility. The following warnings are not intended to frighten, but to give a sense of respect for the power of shell commands. Most commands are perfectly safe, and often when they do something unexpected it can be fixed with some work. We will do our best to warn you of commands with greater potential for destruction, but no documentation is perfect. We are not responsible for accidental deletions or overwrites caused inadvertently, or otherwise, by any commands run by researchers. Be warned that directories, files and file contents that are deleted or overwritten cannot be restored by us under any circumstances. Researchers are responsible for maintaining backups of their files. If in doubt about a command please contact Contact Us for guidance. Educational Resources \u00b6 The internet has thousands of guides for using the shell. Rather than devise our own complete lesson plan for using the shell, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the shell lesson, see https://swcarpentry.github.io/shell-novice/ . At the shell prompt, you can also use the command curl cheat.sh/<command> to get a simple-to-understand explanation of what the command does and how to use it (see curl ). Below is an example for the pwd command . Reference \u00b6 Command Concepts \u00b6 Commands are entered at the prompt. The prompt can take many forms, typically something like one of the following. Common features are: (1) a prompt character, often the dollar sign $ ; (2) a caret to indicate where characters will be inserted when you type, typically a blinking underscore _ or rectangle; (3) color to enhance meaning of various components. Bash on Cheaha... Git Bash on Windows Desktop... Oh My Zsh on Debian... Commands take the form command [optional] <required> . The word command should be replaced with the literal name of the command, such as pwd , ls and cd , among many others. The text [optional] is for flags and inputs that are not required to run the command or that have default values. These flags can be useful for modifying the behavior or output of the command. The text [required] is for flags and inputs that are required to run the command. These must be supplied by the user or the command will not function or produce an error. Flags start with the character - as with the -l flag in ls -l (see ls ). Flags that do not require input can be combined as ls -al . Flags that require input may not be combined as with the flags -n and -m 2 in grep -n -m 2 pattern textfile.txt (see grep ). All inputs are separated by the space character Space . If you wish to or must use a space character in an input, that input must be surrounded by quotation marks. Note that single quotes and double quotes have different behavior. Single quotes '' interpret all characters between them literally. Double quotes \"\" interprets special characters . In most cases, especially with variable contents, double quotes \"\" are preferred. All commands are run in a process. By default, commands run at the shell prompt are run in the shell process, and wait for execution to stop before returning control to you. It is possible to regain control earlier in a number of ways. Warning Copying commands from rich-text sources, such as .pdf , Microsoft Office and webpages, can result in copying special or invisible unicode characters. These characters can cause commands to behave unexpectedly and can be difficult to diagnose. Instead, please try pasting your command into a plain-text editor, like notepad, before copying to the shell prompt. How do I regain control of the prompt while a command is running? \u00b6 Running commands may be terminated using Ctrl + C . Pressing it once will request a graceful termination of the running command. Pressing it more than one will attempt to immediately kill the program. Open a new shell terminal and use that instead. Start the command as command [optional] <required> & . Note the trailing ampersand character & , which causes the command to be run asynchronously in the background. How do I terminate a process running asynchronously? \u00b6 Danger The commands listed here can cause loss of work by termination of incorrect processes if not used carefully. To kill a process running in another shell terminal or running in the background, use either kill or pkill together with an appropriate signal flag. The flag -15 sends SIGTERM which will allow the program to terminate itself gracefully. The flag -9 sends SIGKILL and will immediately terminate the process, in case -15 is not working. kill <signal> <pid> if you know the process id <pid> of the process. Use ps -u <username> to see your running processes. On Cheaha you can use ps -u $USER as a shortcut. pkill <signal> <name-pattern> if you know the name of the process. Warning Using pkill requires carefully thinking of an appropriate name pattern. An incorrect name pattern can cause unwanted termination of processes that may be important to you. Process termination cannot be stopped or undone. Special (Escaped) Characters \u00b6 Backslash \\ is used to write literal versions of certain special characters. Backslash is also called the escape character, and the special characters are also called escape sequences. Character escape sequences are useful in situations where you need a representation of a character, instead of the result of pressing the corresponding key on your keyboard. For example, if you want to store a newline character in a string then you can't just press Enter . If you did, you would immediately execute the command before you finished the string. Instead you can type \\n , which is the escape sequence for a newline character. \\t is interpreted as a tab character. The plain-text equivalent of pressing Tab . \\n is interpreted as a newline character. The plain-text equivalent of pressing Enter . \\\\ is interpreted as a single backslash. At the shell prompt, double quotes \"\" interpret newline characters, while single quotes '' do not. Below is an example file containing escaped characters (shown in nano ), and its interpreted output (shown with echo ). Piping and Command Chains \u00b6 Commands may be composed into chains using pipes with the pipe character | . For example, ls -l | wc -l counts the number of lines returned by ls -l (see ls and wc ). Warning This construct does not accurately count the number of items in a directory, and is only for demonstration purposes. Do not use this exact command chain in practice. Redirects \u00b6 Command inputs and outputs may be redirected with the characters < for input and > for output. Output redirects using > overwrite the contents of existing files and are destructive. Using >> in place of > appends contents to a file, rather than overwriting the contents. Inputs come from STDIN or 0 . Typical output is written to STDOUT or 1 and errors are written to STDERR or 2 . ls -l 1> dirlist stores the directory listing to the file dirlist . In this case using > is the same as 1> . ls -l doesnotexist 1> error 2>&1 stores the error message to the file error . The text 2>&1 means write STDERR to STDOUT . wc -l 0< lines reads the contents of file lines and counts the number of lines. Note that for wc this is not needed, but may be needed for other commands. In this case using < is the same as 0< . For more information on the commands used in the examples, see ls , wc and cat . Danger Output redirects using > are destructive. The contents of the target file are immediately overwritten when the command is executed. It is not possible to recover the contents of the file under any circumstances. Researchers are responsible for maintaining backups of their files. Path Concepts \u00b6 The working directory is the directory you are currently in and may be identified using the command pwd . Dot . is a shortcut for the working directory. This is only used in some contexts. Double dot .. is a shortcut for the immediate parent directory of whatever comes before it. Twiddle ~ is a shortcut for your home directory. Forwardslash / is the path to the root directory of the filesystem, which has no parent. Files and directory names starting with . are hidden. Paths are formed of text-based directory names separated by / Absolute paths start at the root directory, e.g. /home/user/documents/ . Relative paths start at the working directory, e.g. bin . Below are examples of constructed paths tested with ls . Glob Syntax \u00b6 Glob is a shorthand syntax for dealing with many files and directories matching simple patterns. Question mark ? matches a single character. c?t matches both cat and cut . Star * matches any string. c* matches cat , cut , and clatter . *.png matches all png files. Double star ** matches any number of directories with any names. **/*.png matches all png files within any subdirectory of the working directory. This is not commonly used, but extremely useful for some applications. Below are examples of glob usage tested with ls . Environment Concepts \u00b6 Environment variables may be assigned by using var=value where var is the variable name and value is its value. Below is an example tested with echo . Environment variable values may be expanded by using \"${var}\" where var is the variable name. On Cheaha... \"${HOME}\" expands to the path to your home directory. \"${USER_DATA}\" expands to the path to your data/user/<username> directory. \"${USER}\" expands to your user name. Always use double quotes around variables. DO use \"${HOME}\" , do NOT use ${HOME} . Double quotes ensure that space characters in expanded values are handled appropriately. Below, note the error occurring without double quotes due to the spaces in the directory name. Double quoting the variable fixes the error. The commands used to test are ls , cd and pwd . Expanding a variable that isn't defined returns an empty string and does not produce an error, but may cause unexpected behavior. Environment variables may be expanded in paths and command arguments to save effort and time. \"${USER_DATA}/project/inputs\" command ${VARIABLE} Warning Modifying, changing or overwriting existing environment variables while in a shell session can result in unexpected behavior. The environment can be reset to its default starting state by exiting the shell session and starting a new session. Script Concepts \u00b6 Scripts are a way to bundle many commands together and execute them in sequence. Scripts should start with the intended interpreter using a hash-bang like #!/bin/bash . Most commonly bash is the intended interpreter on our systems. Other shell interpreters may be installed and used, but are not necessary. To execute the script given by the hash-bang or she-bang (pronounced shih-bang) #! , use ./script.sh in the folder containing the script. Executable permissions must be set to use a script this way, with chmod u+x <script-path> . To execute the script using a specific interpreter use bash <script-path> , or replace bash with your preferred interpreter. Beware that not all interpreters behave the same way. Executable permissions do not need to be set to use a script this way. Space-separated arguments may be passed to a script when executed in the same way as any other command. Script Arguments \u00b6 Arguments or parameters are passed to a command or script as a space separated list. Arguments may be referred to using numeric variables. The following list contains examples of variable references to arguments. \"${0}\" is the execution path. If you use ./script.sh then \"${0}\" will be ./script.sh . \"${1}\" , \"${2}\" , etc., are the first, second, etc., space separated variables. Calling ./script.sh hello world will have $1 = hello and $2 = world . Many arguments may be passed this way. ${@} is all arguments except \"${0}\" . Important!! Note that double quotes \" are not used! Double quoting would bind all the arguments together. If you need to pass a group of arguments to another script, be cautious about using quotes. ${@: 2} is all arguments starting with \"${2}\" . ${@: 2:2} is the second and third argument. ${@: -2} is the last two arguments. ${@: -2:1} is the second to last argument only. Below is an example script file, hash-bang not shown, demonstrating how each argument variable works, and its interpreted output. The text editor nano is used to display the file and chmod is used to modify file permissions. Tip Using the shell requires some defensive techniques. Never use the space character Space in variables, directory names, file names, etc. Instead, only use letters, numbers and the underscore character _ . In bash it is also allowed to use the hyphen character - , but this may not be portable to other shell interpreters. Names should only start with one or more letters or numbers. Do protect yourself from others who might use the space character Space by always double quoting your variables like \"${var}\" instead of ${var} or $var . Commands for Solutions to Common Problems \u00b6 Below is a reference guide to various commands through the lens of problems to be solved. Note When you see words surrounded by angle brackets like <name> , you should not take that as a literal part of the command. In the case of <name> you would replace it with whatever name is appropriate. Important If you are using Cheaha and working with more than a few files or directories, or the files are large, please run your shell commands in a Job Context . Danger It is safest to assume that any command run at the shell cannot be undone. Be especially aware of the rm command, which is destructive. We do not maintain backups of any files, so once those files are removed or deleted they cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Show working directory ( pwd ) \u00b6 Use pwd , which stands for present working directory. List files and directories ( ls ) \u00b6 Below are common uses of ls , short for \"list\", used to display directory contents and examine details of files and directories. It may be used to check file permissions when using chmod . Visible files only, list: ls . Multiple entries per line. Visible files only, table: ls -l . One entry per line and shows permissions , size in bytes on disk, and timestamp. Visible and hidden files, list ls -a . Same as ls , but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Visible and hidden files, table ls -al . Same as ls -l but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Examine disk usage ( du ) \u00b6 Use du to examine disk usage of files and directories. By default all values are given in bytes, use the flag -h to give values in K , M , G and T for kilobytes, megabytes, gigabytes and terabytes, respectively. Use the flag -s to summarize space used by directories. Below is an example of du -sh . Note that only directories with read permissions can be examined by du . When culling files to conserve storage space, it helps to find the largest files and directories. To find the ten largest, use the command du -sh .[^.]* * | sort -hr | head -n10 in the top-level directory of your data. To better understand this command chain, see also sort , head and piping . Change working directory ( cd ) \u00b6 To change to a different directory, use cd <directory-path> . The variable <path> may be relative, like my/path . This will move to the subdirectory my/path within the working directory. Relative directory paths may contain multiple .. shortcuts to indicate parent directories. Or <path> may be absolute, like /my/path which will move to the directory /my/path , starting at the root of the filesystem. Recall the root directory is just / . To move up to the parent of the working directory, use cd .. . Below are examples of cd usage, tested with ls . Copy files and directories ( cp ) \u00b6 Below are use cases with associated commands, and examples tested using ls . Single file, change name: cp -a <source-file-path> <destination-file-path> . Single file, keep name: cp -a <source-file-path> <destination-directory-path> . Directory: cp -a <source-directory-path> <destination-directory-path> . Glob: cp -a <source-path-glob> <destination-directory-path> . Move files and directories mv \u00b6 Below are use cases with associated commands, and examples tested using ls . Single file, change name: mv <source-file-path> <destination-file-path> . Single file, keep name: mv <source-file-path> <destination-directory-path> . Directory: mv <source-directory-path> <destination-directory-path> . Glob: mv <source-path-glob> <destination-directory-path> . Delete files and directories ( rm , rmdir ) \u00b6 Danger The rm command is destructive and cannot be undone. We do not maintain backups of any files, so files that are removed or deleted cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Below are use cases with associated commands, and examples tested using ls . Single file: rm <file-path> . Empty Directory: rmdir <directory-path> . Directory with Contents: rm -r <directory-path> . Glob: rm <file-path-glob> . Warning Careless use of the directory and glob forms of rm can lead to unwanted data loss. Be sure to double check your commands before executing. Download files from internet sources (curl) \u00b6 Use curl to download files and webpages from internet sources. By default curl writes to STDOUT . If you wish to save the output to a file, use the -o <file-path> command. Note that curl does not transform, encode or decode the data in any way, and it is saved exactly as received from the supplied url. By far the most common usage is to download a file. To do so use curl -o <file-path> <url> , where <file-path> is the desired local path to save and <url> is the web address of the source data. Create directories ( mkdir ) \u00b6 Use mkdir <directory-name> . Example below is tested using ls . Create files ( touch ) \u00b6 Use touch <file-path> . Example below is tested using ls . Edit plain-text files ( nano ) \u00b6 Use nano <file-path> . If the file exists, it will be opened. If the file does not exist, it will be stored in memory until saved. If the contents are not saved they will be discarded and cannot be recovered. You may also use just nano , without a file path, to create a new empty file. The character ^ is the Ctrl key, so ^y would require pressing Ctrl + Y . To save use Ctrl + X to open the exit prompt. Press Y for yes to bring up the save prompt. If you opened nano using an existing file, or passed in a file path, then the File Name to Write prompt will be autofilled with the given file name. If not, you will need to enter it here. When you have the file name entered as desired, press Enter to save. Below is a demonstration of the shell with the new file listed using ls . Example below is tested using ls . Count lines, words and characters ( wc ) \u00b6 Lines: wc -l <file-path> . Words: wc -w <file-path> . Characters: wc -m <file-path> . All: wc <file-path> . Below are example outputs of a file named newfile containing the text hello world followed by a blank line. Note the file is redirected to wc from STDIN . Display file contents ( cat , less ) \u00b6 Use cat <file-path> to display the contents of the file at <file-path> . If the contents are too long to be displayed on one screen, you can pipe the output to less to add scrolling functionality by using cat <file-path> | less . Use the keyboard key Q to stop using less . Below are two images showing what less looks like. Note the last line of each image, which indicates you are using the less environment. The first image is an example where there is more text below the visible text. The second image is an example at the bottom of the text. Examine start and end of file ( head , tail ) \u00b6 To display only the first two lines use head -n 2 <file-path> . Use tail instead of head for the last two lines. Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. Sort file contents ( sort ) \u00b6 Alphabetical: sort <file-path> . Numeric: sort -n <file-path> . Ignore case: sort -i <file-path> . Lines in the input file are returned in sorted order. The results are displayed in stdout. To write the sorted result to a file use sort <file-path> > <new-file-path> . Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. Test a command ( echo ) \u00b6 Use echo \"<command>\" to see the expanded command without executing it. For example echo \"cp -a $USER_DATA /mydir $HOME \" # prints cp -a /data/user/<username>/mydir /home/<username> Use the -e flag if you need to interpret escaped characters such as \\t for tab or \\n for new line. The command being examined in the example below is cp Search for text ( grep ) \u00b6 Use grep \"<pattern>\" \"<file-path>\" to search for <pattern> in the file at <file-path> . Use the -n flag to display line numbers with results. Below is an example of grep -n on a file. The number at the start of the result line is the line number of the pattern match. The matched portion is shown in red while other text on the same line is shown in white. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. We are looking for the literal text \"echo\" within the file. Close the session ( exit ) \u00b6 Use exit . Clear the shell display ( clear ) \u00b6 Use clear . Where is a command located? ( which ) \u00b6 Use which <command-name> . The command being searched for is ls . What does a command do? ( whatis , man ) \u00b6 For builtin command and aliases there are two distinct options for learning more. The command being examined is ls . Use whatis <command-name> to get a brief summary of the command. Use man <command-name> to get the full help file in a less environment. Use q on your keyboard to exit less . Remotely access shell on other machines ( ssh ) \u00b6 See our SSH Section for more detailed information. Remotely access or transfer files between machines ( sftp ) \u00b6 See our Remote Access - Data Transfer Section for more detailed information. Submit and manage jobs on Cheaha \u00b6 See our SLURM Section for more detailed information. Manage permissions of files and directores ( chmod ) \u00b6 Use chmod with the least permissions needed to accomplish a task. Permission management is an important part of managing access and control of files and directories. Danger Please carefully consider security when working in shared spaces like Cheaha. Setting private directories or files as readable by other users can inadvertently expose sensitive or protected information and may violate IT policy, FERPA, HIPAA or some combination. There are legitimate use cases for truly shared spaces. Please Contact Us if you need to share information with other users or collaborators and aren't sure of how to do so securely. What Permissions Do \u00b6 Setting the permissions of a file affect the contents of only that file. A read-only file can still be deleted by users with write permissions in its parent directory. Read permissions allow viewing and copying contents of a file. Write permissions allow changing the contents of a file, including deleting all of the contents. Execute permissions allow using the file as an executable. Helpful for scripts and compiled programs. Scripts and interpreted language files, like Python, must also have read permission set. Setting the permissions of directories affects what can be done with contained files and directories. Read permission allows the use of ls within the directory. cp may be used to copy files from the directory to somewhere else. Write permission allows creation of files and directories, as well as the use of touch , mv and rm on files and directories, within the directory. cp may be used to copy files into the directory. Executable permission allows setting the directory as working directory and the use of cd into the directory. Permissions are not inherited from their parent directory. How to Check Permissions \u00b6 Use ls -ald <path> to see the permissions on the file or directory at <path> . The -d flag lists directories instead of their contents. Patterns for Setting Permissions \u00b6 Two separate patterns can be used to set or change permissions on files and directories. Either may be used, but they cannot be combined in a single use of chmod . In the example images below, the command ls is used to check permissions and the command cat is used to display the contents of the script . Symbols The letter and symbol pattern is in the form a=r . There are three parts. A collection of letters denoting who, e.g. a in this case. Multiple letters may be used. u the owner of the file or directory g the owner's group members o users outside the owner's group a all users (same as ugo ). A symbol indicating how to change the permissions = set permissions - remove permissions + add permissions A collection of letters denoting which permissions to change. Multiple letters may be used. r read w write (change the contents) x execute To add executable permission for only the owner chmod u+x <file-path> . Useful for custom scripts and compiled executables you will use directly from the command line. To set read-only permission for everyone use chmod a=r <file-path> . Note Using = to set permissions will both add and remove permissions. Using a=r will take away existing write and execute permissions. Below is an example of chmod used symbolically to set user execute permissions on a script. Note the error before permissions are set. Numerals Bit mask patterns are in the form 755 . Each digit is the sum of three binary bits. The bits are 4 read 2 write 1 execute The left digit is the owner's permissions. Middle digit is the owner's group. Right digit is users outside the owner's group. Setting chmod 755 means the following: For the owner, set 4 read, 2 write and 1 execute. 4+2+1=7 . For the owner's group, set 4 read, 1 execute. 4+1=5 . For other users, set 4 read, 1 execute. 4+1=5 . Setting 755 is a common pattern for system-wide scripts. Because the 4 read and 1 executable bits are set for all users, it can be called from anywhere by any other script, and not just the owner. However, the 2 write bit is set only for the owner, so other users cannot modify the contents of the script. Below is an example of chmod used numerically to add execution permissions to all users. Note the error before permissions are set. Examples \u00b6 chmod u+x script.sh adds execute permission for you, to a script. chmod 755 script.sh makes a script readable and executable by all, but only writeable by you. This is a common permission for non-sensitive files and directories. chmod 740 sensitive_directory make a directory readable by you and your group, and writeable and executable by only you. Other users cannot delete files in this folder. chmod ug=r notes.txt followed by chmod o-rwx notes.txt makes a file read-only for you and your group and removes all permissions for other users. Manage researcher access to files and directories ( getfacl , setfacl ) \u00b6 Construction Under construction.","title":"Using the Shell"},{"location":"workflow_solutions/shell/#shell-reference","text":"","title":"Shell Reference"},{"location":"workflow_solutions/shell/#introductory-guides","text":"The shell is a powerful tool, and with great power comes great responsibility. The following warnings are not intended to frighten, but to give a sense of respect for the power of shell commands. Most commands are perfectly safe, and often when they do something unexpected it can be fixed with some work. We will do our best to warn you of commands with greater potential for destruction, but no documentation is perfect. We are not responsible for accidental deletions or overwrites caused inadvertently, or otherwise, by any commands run by researchers. Be warned that directories, files and file contents that are deleted or overwritten cannot be restored by us under any circumstances. Researchers are responsible for maintaining backups of their files. If in doubt about a command please contact Contact Us for guidance.","title":"Introductory Guides"},{"location":"workflow_solutions/shell/#educational-resources","text":"The internet has thousands of guides for using the shell. Rather than devise our own complete lesson plan for using the shell, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the shell lesson, see https://swcarpentry.github.io/shell-novice/ . At the shell prompt, you can also use the command curl cheat.sh/<command> to get a simple-to-understand explanation of what the command does and how to use it (see curl ). Below is an example for the pwd command .","title":"Educational Resources"},{"location":"workflow_solutions/shell/#reference","text":"","title":"Reference"},{"location":"workflow_solutions/shell/#command-concepts","text":"Commands are entered at the prompt. The prompt can take many forms, typically something like one of the following. Common features are: (1) a prompt character, often the dollar sign $ ; (2) a caret to indicate where characters will be inserted when you type, typically a blinking underscore _ or rectangle; (3) color to enhance meaning of various components. Bash on Cheaha... Git Bash on Windows Desktop... Oh My Zsh on Debian... Commands take the form command [optional] <required> . The word command should be replaced with the literal name of the command, such as pwd , ls and cd , among many others. The text [optional] is for flags and inputs that are not required to run the command or that have default values. These flags can be useful for modifying the behavior or output of the command. The text [required] is for flags and inputs that are required to run the command. These must be supplied by the user or the command will not function or produce an error. Flags start with the character - as with the -l flag in ls -l (see ls ). Flags that do not require input can be combined as ls -al . Flags that require input may not be combined as with the flags -n and -m 2 in grep -n -m 2 pattern textfile.txt (see grep ). All inputs are separated by the space character Space . If you wish to or must use a space character in an input, that input must be surrounded by quotation marks. Note that single quotes and double quotes have different behavior. Single quotes '' interpret all characters between them literally. Double quotes \"\" interprets special characters . In most cases, especially with variable contents, double quotes \"\" are preferred. All commands are run in a process. By default, commands run at the shell prompt are run in the shell process, and wait for execution to stop before returning control to you. It is possible to regain control earlier in a number of ways. Warning Copying commands from rich-text sources, such as .pdf , Microsoft Office and webpages, can result in copying special or invisible unicode characters. These characters can cause commands to behave unexpectedly and can be difficult to diagnose. Instead, please try pasting your command into a plain-text editor, like notepad, before copying to the shell prompt.","title":"Command Concepts"},{"location":"workflow_solutions/shell/#how-do-i-regain-control-of-the-prompt-while-a-command-is-running","text":"Running commands may be terminated using Ctrl + C . Pressing it once will request a graceful termination of the running command. Pressing it more than one will attempt to immediately kill the program. Open a new shell terminal and use that instead. Start the command as command [optional] <required> & . Note the trailing ampersand character & , which causes the command to be run asynchronously in the background.","title":"How do I regain control of the prompt while a command is running?"},{"location":"workflow_solutions/shell/#how-do-i-terminate-a-process-running-asynchronously","text":"Danger The commands listed here can cause loss of work by termination of incorrect processes if not used carefully. To kill a process running in another shell terminal or running in the background, use either kill or pkill together with an appropriate signal flag. The flag -15 sends SIGTERM which will allow the program to terminate itself gracefully. The flag -9 sends SIGKILL and will immediately terminate the process, in case -15 is not working. kill <signal> <pid> if you know the process id <pid> of the process. Use ps -u <username> to see your running processes. On Cheaha you can use ps -u $USER as a shortcut. pkill <signal> <name-pattern> if you know the name of the process. Warning Using pkill requires carefully thinking of an appropriate name pattern. An incorrect name pattern can cause unwanted termination of processes that may be important to you. Process termination cannot be stopped or undone.","title":"How do I terminate a process running asynchronously?"},{"location":"workflow_solutions/shell/#special-escaped-characters","text":"Backslash \\ is used to write literal versions of certain special characters. Backslash is also called the escape character, and the special characters are also called escape sequences. Character escape sequences are useful in situations where you need a representation of a character, instead of the result of pressing the corresponding key on your keyboard. For example, if you want to store a newline character in a string then you can't just press Enter . If you did, you would immediately execute the command before you finished the string. Instead you can type \\n , which is the escape sequence for a newline character. \\t is interpreted as a tab character. The plain-text equivalent of pressing Tab . \\n is interpreted as a newline character. The plain-text equivalent of pressing Enter . \\\\ is interpreted as a single backslash. At the shell prompt, double quotes \"\" interpret newline characters, while single quotes '' do not. Below is an example file containing escaped characters (shown in nano ), and its interpreted output (shown with echo ).","title":"Special (Escaped) Characters"},{"location":"workflow_solutions/shell/#piping-and-command-chains","text":"Commands may be composed into chains using pipes with the pipe character | . For example, ls -l | wc -l counts the number of lines returned by ls -l (see ls and wc ). Warning This construct does not accurately count the number of items in a directory, and is only for demonstration purposes. Do not use this exact command chain in practice.","title":"Piping and Command Chains"},{"location":"workflow_solutions/shell/#redirects","text":"Command inputs and outputs may be redirected with the characters < for input and > for output. Output redirects using > overwrite the contents of existing files and are destructive. Using >> in place of > appends contents to a file, rather than overwriting the contents. Inputs come from STDIN or 0 . Typical output is written to STDOUT or 1 and errors are written to STDERR or 2 . ls -l 1> dirlist stores the directory listing to the file dirlist . In this case using > is the same as 1> . ls -l doesnotexist 1> error 2>&1 stores the error message to the file error . The text 2>&1 means write STDERR to STDOUT . wc -l 0< lines reads the contents of file lines and counts the number of lines. Note that for wc this is not needed, but may be needed for other commands. In this case using < is the same as 0< . For more information on the commands used in the examples, see ls , wc and cat . Danger Output redirects using > are destructive. The contents of the target file are immediately overwritten when the command is executed. It is not possible to recover the contents of the file under any circumstances. Researchers are responsible for maintaining backups of their files.","title":"Redirects"},{"location":"workflow_solutions/shell/#path-concepts","text":"The working directory is the directory you are currently in and may be identified using the command pwd . Dot . is a shortcut for the working directory. This is only used in some contexts. Double dot .. is a shortcut for the immediate parent directory of whatever comes before it. Twiddle ~ is a shortcut for your home directory. Forwardslash / is the path to the root directory of the filesystem, which has no parent. Files and directory names starting with . are hidden. Paths are formed of text-based directory names separated by / Absolute paths start at the root directory, e.g. /home/user/documents/ . Relative paths start at the working directory, e.g. bin . Below are examples of constructed paths tested with ls .","title":"Path Concepts"},{"location":"workflow_solutions/shell/#glob-syntax","text":"Glob is a shorthand syntax for dealing with many files and directories matching simple patterns. Question mark ? matches a single character. c?t matches both cat and cut . Star * matches any string. c* matches cat , cut , and clatter . *.png matches all png files. Double star ** matches any number of directories with any names. **/*.png matches all png files within any subdirectory of the working directory. This is not commonly used, but extremely useful for some applications. Below are examples of glob usage tested with ls .","title":"Glob Syntax"},{"location":"workflow_solutions/shell/#environment-concepts","text":"Environment variables may be assigned by using var=value where var is the variable name and value is its value. Below is an example tested with echo . Environment variable values may be expanded by using \"${var}\" where var is the variable name. On Cheaha... \"${HOME}\" expands to the path to your home directory. \"${USER_DATA}\" expands to the path to your data/user/<username> directory. \"${USER}\" expands to your user name. Always use double quotes around variables. DO use \"${HOME}\" , do NOT use ${HOME} . Double quotes ensure that space characters in expanded values are handled appropriately. Below, note the error occurring without double quotes due to the spaces in the directory name. Double quoting the variable fixes the error. The commands used to test are ls , cd and pwd . Expanding a variable that isn't defined returns an empty string and does not produce an error, but may cause unexpected behavior. Environment variables may be expanded in paths and command arguments to save effort and time. \"${USER_DATA}/project/inputs\" command ${VARIABLE} Warning Modifying, changing or overwriting existing environment variables while in a shell session can result in unexpected behavior. The environment can be reset to its default starting state by exiting the shell session and starting a new session.","title":"Environment Concepts"},{"location":"workflow_solutions/shell/#script-concepts","text":"Scripts are a way to bundle many commands together and execute them in sequence. Scripts should start with the intended interpreter using a hash-bang like #!/bin/bash . Most commonly bash is the intended interpreter on our systems. Other shell interpreters may be installed and used, but are not necessary. To execute the script given by the hash-bang or she-bang (pronounced shih-bang) #! , use ./script.sh in the folder containing the script. Executable permissions must be set to use a script this way, with chmod u+x <script-path> . To execute the script using a specific interpreter use bash <script-path> , or replace bash with your preferred interpreter. Beware that not all interpreters behave the same way. Executable permissions do not need to be set to use a script this way. Space-separated arguments may be passed to a script when executed in the same way as any other command.","title":"Script Concepts"},{"location":"workflow_solutions/shell/#script-arguments","text":"Arguments or parameters are passed to a command or script as a space separated list. Arguments may be referred to using numeric variables. The following list contains examples of variable references to arguments. \"${0}\" is the execution path. If you use ./script.sh then \"${0}\" will be ./script.sh . \"${1}\" , \"${2}\" , etc., are the first, second, etc., space separated variables. Calling ./script.sh hello world will have $1 = hello and $2 = world . Many arguments may be passed this way. ${@} is all arguments except \"${0}\" . Important!! Note that double quotes \" are not used! Double quoting would bind all the arguments together. If you need to pass a group of arguments to another script, be cautious about using quotes. ${@: 2} is all arguments starting with \"${2}\" . ${@: 2:2} is the second and third argument. ${@: -2} is the last two arguments. ${@: -2:1} is the second to last argument only. Below is an example script file, hash-bang not shown, demonstrating how each argument variable works, and its interpreted output. The text editor nano is used to display the file and chmod is used to modify file permissions. Tip Using the shell requires some defensive techniques. Never use the space character Space in variables, directory names, file names, etc. Instead, only use letters, numbers and the underscore character _ . In bash it is also allowed to use the hyphen character - , but this may not be portable to other shell interpreters. Names should only start with one or more letters or numbers. Do protect yourself from others who might use the space character Space by always double quoting your variables like \"${var}\" instead of ${var} or $var .","title":"Script Arguments"},{"location":"workflow_solutions/shell/#commands-for-solutions-to-common-problems","text":"Below is a reference guide to various commands through the lens of problems to be solved. Note When you see words surrounded by angle brackets like <name> , you should not take that as a literal part of the command. In the case of <name> you would replace it with whatever name is appropriate. Important If you are using Cheaha and working with more than a few files or directories, or the files are large, please run your shell commands in a Job Context . Danger It is safest to assume that any command run at the shell cannot be undone. Be especially aware of the rm command, which is destructive. We do not maintain backups of any files, so once those files are removed or deleted they cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files.","title":"Commands for Solutions to Common Problems"},{"location":"workflow_solutions/shell/#show-working-directory-pwd","text":"Use pwd , which stands for present working directory.","title":"Show working directory (pwd)"},{"location":"workflow_solutions/shell/#list-files-and-directories-ls","text":"Below are common uses of ls , short for \"list\", used to display directory contents and examine details of files and directories. It may be used to check file permissions when using chmod . Visible files only, list: ls . Multiple entries per line. Visible files only, table: ls -l . One entry per line and shows permissions , size in bytes on disk, and timestamp. Visible and hidden files, list ls -a . Same as ls , but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Visible and hidden files, table ls -al . Same as ls -l but has hidden files and directories and . and .. . Example below is truncated to conserve page space.","title":"List files and directories (ls)"},{"location":"workflow_solutions/shell/#examine-disk-usage-du","text":"Use du to examine disk usage of files and directories. By default all values are given in bytes, use the flag -h to give values in K , M , G and T for kilobytes, megabytes, gigabytes and terabytes, respectively. Use the flag -s to summarize space used by directories. Below is an example of du -sh . Note that only directories with read permissions can be examined by du . When culling files to conserve storage space, it helps to find the largest files and directories. To find the ten largest, use the command du -sh .[^.]* * | sort -hr | head -n10 in the top-level directory of your data. To better understand this command chain, see also sort , head and piping .","title":"Examine disk usage (du)"},{"location":"workflow_solutions/shell/#change-working-directory-cd","text":"To change to a different directory, use cd <directory-path> . The variable <path> may be relative, like my/path . This will move to the subdirectory my/path within the working directory. Relative directory paths may contain multiple .. shortcuts to indicate parent directories. Or <path> may be absolute, like /my/path which will move to the directory /my/path , starting at the root of the filesystem. Recall the root directory is just / . To move up to the parent of the working directory, use cd .. . Below are examples of cd usage, tested with ls .","title":"Change working directory (cd)"},{"location":"workflow_solutions/shell/#copy-files-and-directories-cp","text":"Below are use cases with associated commands, and examples tested using ls . Single file, change name: cp -a <source-file-path> <destination-file-path> . Single file, keep name: cp -a <source-file-path> <destination-directory-path> . Directory: cp -a <source-directory-path> <destination-directory-path> . Glob: cp -a <source-path-glob> <destination-directory-path> .","title":"Copy files and directories (cp)"},{"location":"workflow_solutions/shell/#move-files-and-directories-mv","text":"Below are use cases with associated commands, and examples tested using ls . Single file, change name: mv <source-file-path> <destination-file-path> . Single file, keep name: mv <source-file-path> <destination-directory-path> . Directory: mv <source-directory-path> <destination-directory-path> . Glob: mv <source-path-glob> <destination-directory-path> .","title":"Move files and directories mv"},{"location":"workflow_solutions/shell/#delete-files-and-directories-rm-rmdir","text":"Danger The rm command is destructive and cannot be undone. We do not maintain backups of any files, so files that are removed or deleted cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Below are use cases with associated commands, and examples tested using ls . Single file: rm <file-path> . Empty Directory: rmdir <directory-path> . Directory with Contents: rm -r <directory-path> . Glob: rm <file-path-glob> . Warning Careless use of the directory and glob forms of rm can lead to unwanted data loss. Be sure to double check your commands before executing.","title":"Delete files and directories (rm, rmdir)"},{"location":"workflow_solutions/shell/#download-files-from-internet-sources-curl","text":"Use curl to download files and webpages from internet sources. By default curl writes to STDOUT . If you wish to save the output to a file, use the -o <file-path> command. Note that curl does not transform, encode or decode the data in any way, and it is saved exactly as received from the supplied url. By far the most common usage is to download a file. To do so use curl -o <file-path> <url> , where <file-path> is the desired local path to save and <url> is the web address of the source data.","title":"Download files from internet sources (curl)"},{"location":"workflow_solutions/shell/#create-directories-mkdir","text":"Use mkdir <directory-name> . Example below is tested using ls .","title":"Create directories (mkdir)"},{"location":"workflow_solutions/shell/#create-files-touch","text":"Use touch <file-path> . Example below is tested using ls .","title":"Create files (touch)"},{"location":"workflow_solutions/shell/#edit-plain-text-files-nano","text":"Use nano <file-path> . If the file exists, it will be opened. If the file does not exist, it will be stored in memory until saved. If the contents are not saved they will be discarded and cannot be recovered. You may also use just nano , without a file path, to create a new empty file. The character ^ is the Ctrl key, so ^y would require pressing Ctrl + Y . To save use Ctrl + X to open the exit prompt. Press Y for yes to bring up the save prompt. If you opened nano using an existing file, or passed in a file path, then the File Name to Write prompt will be autofilled with the given file name. If not, you will need to enter it here. When you have the file name entered as desired, press Enter to save. Below is a demonstration of the shell with the new file listed using ls . Example below is tested using ls .","title":"Edit plain-text files (nano)"},{"location":"workflow_solutions/shell/#count-lines-words-and-characters-wc","text":"Lines: wc -l <file-path> . Words: wc -w <file-path> . Characters: wc -m <file-path> . All: wc <file-path> . Below are example outputs of a file named newfile containing the text hello world followed by a blank line. Note the file is redirected to wc from STDIN .","title":"Count lines, words and characters (wc)"},{"location":"workflow_solutions/shell/#display-file-contents-cat-less","text":"Use cat <file-path> to display the contents of the file at <file-path> . If the contents are too long to be displayed on one screen, you can pipe the output to less to add scrolling functionality by using cat <file-path> | less . Use the keyboard key Q to stop using less . Below are two images showing what less looks like. Note the last line of each image, which indicates you are using the less environment. The first image is an example where there is more text below the visible text. The second image is an example at the bottom of the text.","title":"Display file contents (cat, less)"},{"location":"workflow_solutions/shell/#examine-start-and-end-of-file-head-tail","text":"To display only the first two lines use head -n 2 <file-path> . Use tail instead of head for the last two lines. Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.","title":"Examine start and end of file (head, tail)"},{"location":"workflow_solutions/shell/#sort-file-contents-sort","text":"Alphabetical: sort <file-path> . Numeric: sort -n <file-path> . Ignore case: sort -i <file-path> . Lines in the input file are returned in sorted order. The results are displayed in stdout. To write the sorted result to a file use sort <file-path> > <new-file-path> . Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.","title":"Sort file contents (sort)"},{"location":"workflow_solutions/shell/#test-a-command-echo","text":"Use echo \"<command>\" to see the expanded command without executing it. For example echo \"cp -a $USER_DATA /mydir $HOME \" # prints cp -a /data/user/<username>/mydir /home/<username> Use the -e flag if you need to interpret escaped characters such as \\t for tab or \\n for new line. The command being examined in the example below is cp","title":"Test a command (echo)"},{"location":"workflow_solutions/shell/#search-for-text-grep","text":"Use grep \"<pattern>\" \"<file-path>\" to search for <pattern> in the file at <file-path> . Use the -n flag to display line numbers with results. Below is an example of grep -n on a file. The number at the start of the result line is the line number of the pattern match. The matched portion is shown in red while other text on the same line is shown in white. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. We are looking for the literal text \"echo\" within the file.","title":"Search for text (grep)"},{"location":"workflow_solutions/shell/#close-the-session-exit","text":"Use exit .","title":"Close the session (exit)"},{"location":"workflow_solutions/shell/#clear-the-shell-display-clear","text":"Use clear .","title":"Clear the shell display (clear)"},{"location":"workflow_solutions/shell/#where-is-a-command-located-which","text":"Use which <command-name> . The command being searched for is ls .","title":"Where is a command located? (which)"},{"location":"workflow_solutions/shell/#what-does-a-command-do-whatis-man","text":"For builtin command and aliases there are two distinct options for learning more. The command being examined is ls . Use whatis <command-name> to get a brief summary of the command. Use man <command-name> to get the full help file in a less environment. Use q on your keyboard to exit less .","title":"What does a command do? (whatis, man)"},{"location":"workflow_solutions/shell/#remotely-access-shell-on-other-machines-ssh","text":"See our SSH Section for more detailed information.","title":"Remotely access shell on other machines (ssh)"},{"location":"workflow_solutions/shell/#remotely-access-or-transfer-files-between-machines-sftp","text":"See our Remote Access - Data Transfer Section for more detailed information.","title":"Remotely access or transfer files between machines (sftp)"},{"location":"workflow_solutions/shell/#submit-and-manage-jobs-on-cheaha","text":"See our SLURM Section for more detailed information.","title":"Submit and manage jobs on Cheaha"},{"location":"workflow_solutions/shell/#manage-permissions-of-files-and-directores-chmod","text":"Use chmod with the least permissions needed to accomplish a task. Permission management is an important part of managing access and control of files and directories. Danger Please carefully consider security when working in shared spaces like Cheaha. Setting private directories or files as readable by other users can inadvertently expose sensitive or protected information and may violate IT policy, FERPA, HIPAA or some combination. There are legitimate use cases for truly shared spaces. Please Contact Us if you need to share information with other users or collaborators and aren't sure of how to do so securely.","title":"Manage permissions of files and directores (chmod)"},{"location":"workflow_solutions/shell/#what-permissions-do","text":"Setting the permissions of a file affect the contents of only that file. A read-only file can still be deleted by users with write permissions in its parent directory. Read permissions allow viewing and copying contents of a file. Write permissions allow changing the contents of a file, including deleting all of the contents. Execute permissions allow using the file as an executable. Helpful for scripts and compiled programs. Scripts and interpreted language files, like Python, must also have read permission set. Setting the permissions of directories affects what can be done with contained files and directories. Read permission allows the use of ls within the directory. cp may be used to copy files from the directory to somewhere else. Write permission allows creation of files and directories, as well as the use of touch , mv and rm on files and directories, within the directory. cp may be used to copy files into the directory. Executable permission allows setting the directory as working directory and the use of cd into the directory. Permissions are not inherited from their parent directory.","title":"What Permissions Do"},{"location":"workflow_solutions/shell/#how-to-check-permissions","text":"Use ls -ald <path> to see the permissions on the file or directory at <path> . The -d flag lists directories instead of their contents.","title":"How to Check Permissions"},{"location":"workflow_solutions/shell/#patterns-for-setting-permissions","text":"Two separate patterns can be used to set or change permissions on files and directories. Either may be used, but they cannot be combined in a single use of chmod . In the example images below, the command ls is used to check permissions and the command cat is used to display the contents of the script . Symbols The letter and symbol pattern is in the form a=r . There are three parts. A collection of letters denoting who, e.g. a in this case. Multiple letters may be used. u the owner of the file or directory g the owner's group members o users outside the owner's group a all users (same as ugo ). A symbol indicating how to change the permissions = set permissions - remove permissions + add permissions A collection of letters denoting which permissions to change. Multiple letters may be used. r read w write (change the contents) x execute To add executable permission for only the owner chmod u+x <file-path> . Useful for custom scripts and compiled executables you will use directly from the command line. To set read-only permission for everyone use chmod a=r <file-path> . Note Using = to set permissions will both add and remove permissions. Using a=r will take away existing write and execute permissions. Below is an example of chmod used symbolically to set user execute permissions on a script. Note the error before permissions are set. Numerals Bit mask patterns are in the form 755 . Each digit is the sum of three binary bits. The bits are 4 read 2 write 1 execute The left digit is the owner's permissions. Middle digit is the owner's group. Right digit is users outside the owner's group. Setting chmod 755 means the following: For the owner, set 4 read, 2 write and 1 execute. 4+2+1=7 . For the owner's group, set 4 read, 1 execute. 4+1=5 . For other users, set 4 read, 1 execute. 4+1=5 . Setting 755 is a common pattern for system-wide scripts. Because the 4 read and 1 executable bits are set for all users, it can be called from anywhere by any other script, and not just the owner. However, the 2 write bit is set only for the owner, so other users cannot modify the contents of the script. Below is an example of chmod used numerically to add execution permissions to all users. Note the error before permissions are set.","title":"Patterns for Setting Permissions"},{"location":"workflow_solutions/shell/#examples","text":"chmod u+x script.sh adds execute permission for you, to a script. chmod 755 script.sh makes a script readable and executable by all, but only writeable by you. This is a common permission for non-sensitive files and directories. chmod 740 sensitive_directory make a directory readable by you and your group, and writeable and executable by only you. Other users cannot delete files in this folder. chmod ug=r notes.txt followed by chmod o-rwx notes.txt makes a file read-only for you and your group and removes all permissions for other users.","title":"Examples"},{"location":"workflow_solutions/shell/#manage-researcher-access-to-files-and-directories-getfacl-setfacl","text":"Construction Under construction.","title":"Manage researcher access to files and directories (getfacl, setfacl)"},{"location":"workflow_solutions/using_anaconda/","text":"Anaconda \u00b6 Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is the standard in Python package management for scientific research. Benefits of Anaconda: Shareability: environments can be shared via human-readable text-based YAML files. Maintainability: the same YAML files can be version controlled using git. Repeatability: environments can be rebuilt using those same YAML files. Simplicity: dependency matrices are computed and solved by Anaconda, and libraries are pre-built and stored on remote servers for download instead of being built on your local machine. Ubiquity: nearly all Python developers are aware of the usage of Anaconda, especially in scientific research, so there are many resources available for learning how to use it, and what to do if something goes wrong. Anaconda can also install Pip and record which Pip packages are installed, so Anaconda can do everything Pip can, and more. What is my best solution for installing Anaconda? \u00b6 If you are using a local machine or doing general purpose software development, or have a particular package in mind, go here to install Anaconda. If you are using a virtual machine or container, go here to install Miniconda. If you are using Cheaha, go here for how to use Anaconda on Cheaha. Installing Anaconda \u00b6 The full Anaconda install is a good choice if you are using a local machine, or doing general Python development work, or have a particular scientific package in mind. Anaconda installation instructions are located here: https://docs.anaconda.com/anaconda/install/index.html . Installing Miniconda \u00b6 Miniconda is a lightweight version of Anaconda. While Anaconda's base environment comes with Python, the Scipy stack, and other common packages pre-installed, Miniconda comes with no packages installed. This is an excellent alternative to the full Anaconda installation for environments where minimal space is available or where setup time is important, like virtual machines and containers . Miniconda installation instructions are located here: https://docs.conda.io/en/latest/miniconda.html . Using Anaconda \u00b6 Anaconda is a package manager, meaning it handles all of the difficult mathematics and logistics of figuring out exactly what versions of which packages should be downloaded to meet your needs, or inform you if there is a conflict. Anaconda is structured around environments. Environments are self-contained collections of researcher-selected packages. Environments can be changed out using a simple package without requiring tedious installing and uninstalling of packages or software, and avoiding dependency conflicts with each other. Environments allow researchers to work and collaborate on multiple projects, each with different requirements, all on the same computer. Environments can be installed from the command line, from pre-designed or shared YAML files, and can be modified or updated as needed. The following subsections detail some of the more common commands and use cases for Anaconda usage. More complete information on this process can be found at the Anaconda documentation . Create an Environment \u00b6 In order to create a basic environment with the default packages, use the conda create command: # create a base environment. Replace <env> with an environment name conda create -n <env> If you are trying to replicate a pipeline or analysis from another person, you can also recreate an environment using a YAML file, if they have provided one. To replicate an environment using a YAML file, use: # replicate an environment from a YAML file named env.yml conda create -n <env> -f <path/to/env.yml> By default, all of your conda environments are stored in /home/<user>/.conda/envs . Activate an Environment \u00b6 From here, you can activate the environment using either source or conda : # activate the virtual environment using source source activate <env> # or using conda conda activate <env> To know your environment has loaded, the command line should look like: (<env>) [blazerid@c0XXX ~]$ Once the environment is activated, you are allowed to install whichever python libraries you need for your analysis. Install Packages \u00b6 To install packages using Anaconda, use the conda install command. The -c or --channel command can be used to select a specific package channel to install from. The anaconda channel is a curated collection of high-quality packages, but the very latest versions may not be available on this channel. The conda-forge channel is more open, less carefully curated, and has more recent versions. # install most recent version of a package conda install <package> # install a specific version conda install <package> = version # install from a specific conda channel conda install -c <channel> <package>< = version> Generally, if a package needs to be downloaded from a specific conda channel, it will mention that in its installation instructions. Installing Packages with Pip \u00b6 Some packages are not available through Anaconda. Often these packages are available via PyPi and thus using the Python built-in Pip package manager. Pip may also be used to install locally-available packages as well. # install most recent version of a package pip install \\< package \\> # install a specific version, note the double equals sign pip install \\< package \\> == version # install a list of packages from a text file pip install -r packages.txt Finding Packages \u00b6 You may use the Anaconda page to search for packages on Anaconda, or use Google with something like <package name> conda . To find packages in PyPi, either use the PyPi page to search, or use Google with something like <package name> pip . Packages for Jupyter \u00b6 If you are using Anaconda with Jupyter, you will need to be sure to install the ipykernel package for your environment to be recognized by the Jupyter Server . If you are using Jupyter in Open OnDemand then you do not need to install the jupyter package. Deactivating an Environment \u00b6 An environment can be deactivated using the following command. # Using conda conda deactivate Anaconda may say that using source deactivate is deprecated, but environment will still be deactivated. Closing the terminal will also close out the environment. Working with Environment YAML Files \u00b6 Exporting an Environment \u00b6 To easily share environments with other researchers or replicate it on a new machine, it is useful to create an environment YAML file. You can do this using: # activate the environment if it is not active already conda activate <env> # export the environment to a YAML file conda env export > env.yml Creating an Environment from a YAML File \u00b6 To create an environment from a YAML file env.yml , use the following command. conda env create --file env.yml Replicability versus Portability \u00b6 An environment with only python 3.10.4 , numpy 1.21.5 and jinja2 2.11.2 installed will output something like the following file when conda env export is used. This file may be used to precisely replicate the environment as it exists on the machine where conda env export was run. Note that the versioning for each package contains two = signs. The code like he774522_0 after the second = sign contains hyper-specific build information for the compiled libraries for that package. Sharing this exact file with collaborators may result in frustration if they do not have the exact same operating system and hardware as you, and they would not be able to build this environment. We would say that this environment file is not very portable. There are other portability issues: The prefix: C:\\... line is not used by conda in any way and is deprecated. It also shares system information about file locations which is potentially sensitive information. The channels: group uses - defaults , which may vary depending on how you or your collaborator has customized their Anaconda installation. It may result in packages not being found, resulting in environment creation failure. name : test-env channels : - defaults dependencies : - blas=1.0=mkl - bzip2=1.0.8=he774522_0 - ca-certificates=2022.4.26=haa95532_0 - certifi=2021.5.30=py310haa95532_0 - intel-openmp=2021.4.0=haa95532_3556 - jinja2=2.11.2=pyhd3eb1b0_0 - libffi=3.4.2=h604cdb4_1 - markupsafe=2.1.1=py310h2bbff1b_0 - mkl=2021.4.0=haa95532_640 - mkl-service=2.4.0=py310h2bbff1b_0 - mkl_fft=1.3.1=py310ha0764ea_0 - mkl_random=1.2.2=py310h4ed8f06_0 - numpy=1.21.5=py310h6d2d95c_2 - numpy-base=1.21.5=py310h206c741_2 - openssl=1.1.1o=h2bbff1b_0 - pip=21.2.4=py310haa95532_0 - python=3.10.4=hbb2ffb3_0 - setuptools=61.2.0=py310haa95532_0 - six=1.16.0=pyhd3eb1b0_1 - sqlite=3.38.3=h2bbff1b_0 - tk=8.6.11=h2bbff1b_1 - tzdata=2022a=hda174b7_0 - vc=14.2=h21ff451_1 - vs2015_runtime=14.27.29016=h5e58377_2 - wheel=0.37.1=pyhd3eb1b0_0 - wincertstore=0.2=py310haa95532_2 - xz=5.2.5=h8cc25b3_1 - zlib=1.2.12=h8cc25b3_2 prefix : C:\\Users\\user\\Anaconda3\\envs\\test-env To make this a more portable file, suitable for collaboration, some planning is required. Instead of using conda env export we can build our own file. Create a new file called env.yml using your favorite text editor and add the following. Note we've only listed exactly the packages we installed, and their version numbers, only. This allows Anaconda the flexibility to choose dependencies which do not conflict and do not contain unusable hyper-specific library build information. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - numpy=1.21.5 - python=3.10.4 This is a much more readable and portable file suitable for sharing with collaborators. We aren't quite finished though! Some scientific packages on the conda-forge channel, and on other channels, can contain dependency errors. Those packages may accidentally pull a version of a dependency that breaks their code. For example, the package markupsafe made a not-backward-compatible change (a breaking change) to their code between 2.0.1 and 2.1.1 . Dependent packages expected 2.1.1 to be backward compatible, so their packages allowed 2.1.1 as a substitute for 2.0.1 . Since Anaconda chooses the most recent version allowable, package installs broke. To work around this for our environment, we would need to modify the environment to \"pin\" that package at a specific version, even though we didn't explicitly install it. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - markupsafe=2.0.1 - numpy=1.21.5 - python=3.10.4 Now we can be sure that the correct versions of the software will be installed on our collaborator's machines. Note The example above is provided only for illustration purposes. The error has since been fixed, but the example above really happened and is helpful to explain version pinning. Good Software Development Practice \u00b6 Building on the example above, we can bring in good software development practices to ensure we don't lose track of how our environment is changing as we develop our software or our workflows. If you've ever lost a lot of hard work by accidentally deleting an important file, or forgetting what changes you've made that need to be rolled back, this section is for you. Efficient software developers live the mantra \"Don't repeat yourself\". Part of not repeating yourself is keeping a detailed and meticulous record of changes made as your software grows over time. Git is a way to have the computer keep track of those changes digitally. Git can be used to save changes to environment files as they change over time. Remember that each time your environment changes to commit the output of Exporting your Environment to a repository for your project. Speeding Things up with Mamba \u00b6 Mamba is an alternative to Anaconda that uses libsolv and parallel processing to install environments more quickly, sometimes by an order of magnitude. Mamba will also discover conflicts very quickly. Mamba is available as a package via Anaconda. Currently Mamba cannot be installed on Cheaha, only on self-maanged systems like cloud.rc instances. To install use the following. conda activate base conda update --all conda install -n base -c conda-forge mamba Warning Mamba must be installed in the base environment to function correctly! If you are using Cheaha, and cannot install in the base environment, see our workaround here","title":"Using Anaconda"},{"location":"workflow_solutions/using_anaconda/#anaconda","text":"Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is the standard in Python package management for scientific research. Benefits of Anaconda: Shareability: environments can be shared via human-readable text-based YAML files. Maintainability: the same YAML files can be version controlled using git. Repeatability: environments can be rebuilt using those same YAML files. Simplicity: dependency matrices are computed and solved by Anaconda, and libraries are pre-built and stored on remote servers for download instead of being built on your local machine. Ubiquity: nearly all Python developers are aware of the usage of Anaconda, especially in scientific research, so there are many resources available for learning how to use it, and what to do if something goes wrong. Anaconda can also install Pip and record which Pip packages are installed, so Anaconda can do everything Pip can, and more.","title":"Anaconda"},{"location":"workflow_solutions/using_anaconda/#what-is-my-best-solution-for-installing-anaconda","text":"If you are using a local machine or doing general purpose software development, or have a particular package in mind, go here to install Anaconda. If you are using a virtual machine or container, go here to install Miniconda. If you are using Cheaha, go here for how to use Anaconda on Cheaha.","title":"What is my best solution for installing Anaconda?"},{"location":"workflow_solutions/using_anaconda/#installing-anaconda","text":"The full Anaconda install is a good choice if you are using a local machine, or doing general Python development work, or have a particular scientific package in mind. Anaconda installation instructions are located here: https://docs.anaconda.com/anaconda/install/index.html .","title":"Installing Anaconda"},{"location":"workflow_solutions/using_anaconda/#installing-miniconda","text":"Miniconda is a lightweight version of Anaconda. While Anaconda's base environment comes with Python, the Scipy stack, and other common packages pre-installed, Miniconda comes with no packages installed. This is an excellent alternative to the full Anaconda installation for environments where minimal space is available or where setup time is important, like virtual machines and containers . Miniconda installation instructions are located here: https://docs.conda.io/en/latest/miniconda.html .","title":"Installing Miniconda"},{"location":"workflow_solutions/using_anaconda/#using-anaconda","text":"Anaconda is a package manager, meaning it handles all of the difficult mathematics and logistics of figuring out exactly what versions of which packages should be downloaded to meet your needs, or inform you if there is a conflict. Anaconda is structured around environments. Environments are self-contained collections of researcher-selected packages. Environments can be changed out using a simple package without requiring tedious installing and uninstalling of packages or software, and avoiding dependency conflicts with each other. Environments allow researchers to work and collaborate on multiple projects, each with different requirements, all on the same computer. Environments can be installed from the command line, from pre-designed or shared YAML files, and can be modified or updated as needed. The following subsections detail some of the more common commands and use cases for Anaconda usage. More complete information on this process can be found at the Anaconda documentation .","title":"Using Anaconda"},{"location":"workflow_solutions/using_anaconda/#create-an-environment","text":"In order to create a basic environment with the default packages, use the conda create command: # create a base environment. Replace <env> with an environment name conda create -n <env> If you are trying to replicate a pipeline or analysis from another person, you can also recreate an environment using a YAML file, if they have provided one. To replicate an environment using a YAML file, use: # replicate an environment from a YAML file named env.yml conda create -n <env> -f <path/to/env.yml> By default, all of your conda environments are stored in /home/<user>/.conda/envs .","title":"Create an Environment"},{"location":"workflow_solutions/using_anaconda/#activate-an-environment","text":"From here, you can activate the environment using either source or conda : # activate the virtual environment using source source activate <env> # or using conda conda activate <env> To know your environment has loaded, the command line should look like: (<env>) [blazerid@c0XXX ~]$ Once the environment is activated, you are allowed to install whichever python libraries you need for your analysis.","title":"Activate an Environment"},{"location":"workflow_solutions/using_anaconda/#install-packages","text":"To install packages using Anaconda, use the conda install command. The -c or --channel command can be used to select a specific package channel to install from. The anaconda channel is a curated collection of high-quality packages, but the very latest versions may not be available on this channel. The conda-forge channel is more open, less carefully curated, and has more recent versions. # install most recent version of a package conda install <package> # install a specific version conda install <package> = version # install from a specific conda channel conda install -c <channel> <package>< = version> Generally, if a package needs to be downloaded from a specific conda channel, it will mention that in its installation instructions.","title":"Install Packages"},{"location":"workflow_solutions/using_anaconda/#installing-packages-with-pip","text":"Some packages are not available through Anaconda. Often these packages are available via PyPi and thus using the Python built-in Pip package manager. Pip may also be used to install locally-available packages as well. # install most recent version of a package pip install \\< package \\> # install a specific version, note the double equals sign pip install \\< package \\> == version # install a list of packages from a text file pip install -r packages.txt","title":"Installing Packages with Pip"},{"location":"workflow_solutions/using_anaconda/#finding-packages","text":"You may use the Anaconda page to search for packages on Anaconda, or use Google with something like <package name> conda . To find packages in PyPi, either use the PyPi page to search, or use Google with something like <package name> pip .","title":"Finding Packages"},{"location":"workflow_solutions/using_anaconda/#packages-for-jupyter","text":"If you are using Anaconda with Jupyter, you will need to be sure to install the ipykernel package for your environment to be recognized by the Jupyter Server . If you are using Jupyter in Open OnDemand then you do not need to install the jupyter package.","title":"Packages for Jupyter"},{"location":"workflow_solutions/using_anaconda/#deactivating-an-environment","text":"An environment can be deactivated using the following command. # Using conda conda deactivate Anaconda may say that using source deactivate is deprecated, but environment will still be deactivated. Closing the terminal will also close out the environment.","title":"Deactivating an Environment"},{"location":"workflow_solutions/using_anaconda/#working-with-environment-yaml-files","text":"","title":"Working with Environment YAML Files"},{"location":"workflow_solutions/using_anaconda/#exporting-an-environment","text":"To easily share environments with other researchers or replicate it on a new machine, it is useful to create an environment YAML file. You can do this using: # activate the environment if it is not active already conda activate <env> # export the environment to a YAML file conda env export > env.yml","title":"Exporting an Environment"},{"location":"workflow_solutions/using_anaconda/#creating-an-environment-from-a-yaml-file","text":"To create an environment from a YAML file env.yml , use the following command. conda env create --file env.yml","title":"Creating an Environment from a YAML File"},{"location":"workflow_solutions/using_anaconda/#replicability-versus-portability","text":"An environment with only python 3.10.4 , numpy 1.21.5 and jinja2 2.11.2 installed will output something like the following file when conda env export is used. This file may be used to precisely replicate the environment as it exists on the machine where conda env export was run. Note that the versioning for each package contains two = signs. The code like he774522_0 after the second = sign contains hyper-specific build information for the compiled libraries for that package. Sharing this exact file with collaborators may result in frustration if they do not have the exact same operating system and hardware as you, and they would not be able to build this environment. We would say that this environment file is not very portable. There are other portability issues: The prefix: C:\\... line is not used by conda in any way and is deprecated. It also shares system information about file locations which is potentially sensitive information. The channels: group uses - defaults , which may vary depending on how you or your collaborator has customized their Anaconda installation. It may result in packages not being found, resulting in environment creation failure. name : test-env channels : - defaults dependencies : - blas=1.0=mkl - bzip2=1.0.8=he774522_0 - ca-certificates=2022.4.26=haa95532_0 - certifi=2021.5.30=py310haa95532_0 - intel-openmp=2021.4.0=haa95532_3556 - jinja2=2.11.2=pyhd3eb1b0_0 - libffi=3.4.2=h604cdb4_1 - markupsafe=2.1.1=py310h2bbff1b_0 - mkl=2021.4.0=haa95532_640 - mkl-service=2.4.0=py310h2bbff1b_0 - mkl_fft=1.3.1=py310ha0764ea_0 - mkl_random=1.2.2=py310h4ed8f06_0 - numpy=1.21.5=py310h6d2d95c_2 - numpy-base=1.21.5=py310h206c741_2 - openssl=1.1.1o=h2bbff1b_0 - pip=21.2.4=py310haa95532_0 - python=3.10.4=hbb2ffb3_0 - setuptools=61.2.0=py310haa95532_0 - six=1.16.0=pyhd3eb1b0_1 - sqlite=3.38.3=h2bbff1b_0 - tk=8.6.11=h2bbff1b_1 - tzdata=2022a=hda174b7_0 - vc=14.2=h21ff451_1 - vs2015_runtime=14.27.29016=h5e58377_2 - wheel=0.37.1=pyhd3eb1b0_0 - wincertstore=0.2=py310haa95532_2 - xz=5.2.5=h8cc25b3_1 - zlib=1.2.12=h8cc25b3_2 prefix : C:\\Users\\user\\Anaconda3\\envs\\test-env To make this a more portable file, suitable for collaboration, some planning is required. Instead of using conda env export we can build our own file. Create a new file called env.yml using your favorite text editor and add the following. Note we've only listed exactly the packages we installed, and their version numbers, only. This allows Anaconda the flexibility to choose dependencies which do not conflict and do not contain unusable hyper-specific library build information. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - numpy=1.21.5 - python=3.10.4 This is a much more readable and portable file suitable for sharing with collaborators. We aren't quite finished though! Some scientific packages on the conda-forge channel, and on other channels, can contain dependency errors. Those packages may accidentally pull a version of a dependency that breaks their code. For example, the package markupsafe made a not-backward-compatible change (a breaking change) to their code between 2.0.1 and 2.1.1 . Dependent packages expected 2.1.1 to be backward compatible, so their packages allowed 2.1.1 as a substitute for 2.0.1 . Since Anaconda chooses the most recent version allowable, package installs broke. To work around this for our environment, we would need to modify the environment to \"pin\" that package at a specific version, even though we didn't explicitly install it. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - markupsafe=2.0.1 - numpy=1.21.5 - python=3.10.4 Now we can be sure that the correct versions of the software will be installed on our collaborator's machines. Note The example above is provided only for illustration purposes. The error has since been fixed, but the example above really happened and is helpful to explain version pinning.","title":"Replicability versus Portability"},{"location":"workflow_solutions/using_anaconda/#good-software-development-practice","text":"Building on the example above, we can bring in good software development practices to ensure we don't lose track of how our environment is changing as we develop our software or our workflows. If you've ever lost a lot of hard work by accidentally deleting an important file, or forgetting what changes you've made that need to be rolled back, this section is for you. Efficient software developers live the mantra \"Don't repeat yourself\". Part of not repeating yourself is keeping a detailed and meticulous record of changes made as your software grows over time. Git is a way to have the computer keep track of those changes digitally. Git can be used to save changes to environment files as they change over time. Remember that each time your environment changes to commit the output of Exporting your Environment to a repository for your project.","title":"Good Software Development Practice"},{"location":"workflow_solutions/using_anaconda/#speeding-things-up-with-mamba","text":"Mamba is an alternative to Anaconda that uses libsolv and parallel processing to install environments more quickly, sometimes by an order of magnitude. Mamba will also discover conflicts very quickly. Mamba is available as a package via Anaconda. Currently Mamba cannot be installed on Cheaha, only on self-maanged systems like cloud.rc instances. To install use the following. conda activate base conda update --all conda install -n base -c conda-forge mamba Warning Mamba must be installed in the base environment to function correctly! If you are using Cheaha, and cannot install in the base environment, see our workaround here","title":"Speeding Things up with Mamba"},{"location":"workflow_solutions/using_workflow_managers/","text":"Workflow Managers \u00b6 Construction This page is a stub and is under construction. Snakemake \u00b6 Pegasus \u00b6 Nextflow \u00b6","title":"Using Workflow Managers"},{"location":"workflow_solutions/using_workflow_managers/#workflow-managers","text":"Construction This page is a stub and is under construction.","title":"Workflow Managers"},{"location":"workflow_solutions/using_workflow_managers/#snakemake","text":"","title":"Snakemake"},{"location":"workflow_solutions/using_workflow_managers/#pegasus","text":"","title":"Pegasus"},{"location":"workflow_solutions/using_workflow_managers/#nextflow","text":"","title":"Nextflow"}]}