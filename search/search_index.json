{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to UAB Research Computing \u00b6 The Research Computing System (RCS) provides a framework for sharing research data, accessing computing power, and collaborating with peers on campus and around the globe. Our goal is to construct a dynamic \"network of services\" that you can use to organize, study and share your research data. Research Computing provides services to researchers in these core areas: Data Analysis : using the High Performance Computing (HPC) fabric Cheaha for batch data processing. Data Sharing : supporting trusted information exchange to spark new ideas via our Storage system. Application Development : providing virtual machines and web-hosted development tools empowering researcher via our cloud.rc fabric. Announcement We have released new A100 gpus on Cheaha! For more information please see GPUs . We have released new CUDA and cuDNN modules! For more information please see CUDA Modules . Also see our A100 GPU Frequently Asked Questions (FAQ) How to Contact Us \u00b6 Please reach out to us via email at support@listserv.uab.edu to create a support ticket. For face-to-face support please visit us in our Zoom office hours held weekly: Mondays 10:00 AM to 12:00 PM: Zoom Thursdays 10:00 AM to 12:00 PM: Zoom For additional information please see Support . Support and Development \u00b6 RCS is developed and supported by UAB IT's Research Computing Group. We are developing a core set of applications to help you to easily incorporate our services into your research processes and this documentation collection to help you leverage the resources already available. We follow the best practices of the Open Source community and develop our software in an open-source fashion. RCS is an out growth of the UABgrid pilot, launched in September 2007 which has focused on demonstrating the utility of unlimited analysis, storage, and application for research. RCS is built on the same technology foundations used by major cloud vendors and decades of distributed systems computing research, technology that powered the last ten years of large scale systems serving prominent national and international initiatives like the Open Science Grid , XSEDE , the LHC Computing Grid , and NCIP . Outreach \u00b6 The UAB IT Research Computing Group has collaborated with a number of prominent research projects at UAB to identify use cases and develop requirements. Our collaborators include, but are not limited to, the Center for Clinical and Translational Science (CCTS), Heflin Genomics Center, the Comprehensive Cancer Center (CCC), the Department of Computer and Information Sciences (CIS), the Department of Mechanical Engineering (ME), Lister Hill Library, the School of Optometry's Center for the Development of Functional Imaging, and Health System Information Services (HSIS). As part of the process of building RC, the UAB IT Research Computing Group has hosted an annual campus symposium on research computing and cyber-infrastructure (CI) developments and accomplishments. Starting as CyberInfrastructure (CI) Days in 2007, the name was changed to UAB Research Computing Day in 2011 to reflect the broader mission to support research. IT Research Computing also participates in other campus wide symposiums including UAB Research Core Day.","title":"Home"},{"location":"#welcome-to-uab-research-computing","text":"The Research Computing System (RCS) provides a framework for sharing research data, accessing computing power, and collaborating with peers on campus and around the globe. Our goal is to construct a dynamic \"network of services\" that you can use to organize, study and share your research data. Research Computing provides services to researchers in these core areas: Data Analysis : using the High Performance Computing (HPC) fabric Cheaha for batch data processing. Data Sharing : supporting trusted information exchange to spark new ideas via our Storage system. Application Development : providing virtual machines and web-hosted development tools empowering researcher via our cloud.rc fabric. Announcement We have released new A100 gpus on Cheaha! For more information please see GPUs . We have released new CUDA and cuDNN modules! For more information please see CUDA Modules . Also see our A100 GPU Frequently Asked Questions (FAQ)","title":"Welcome to UAB Research Computing"},{"location":"#how-to-contact-us","text":"Please reach out to us via email at support@listserv.uab.edu to create a support ticket. For face-to-face support please visit us in our Zoom office hours held weekly: Mondays 10:00 AM to 12:00 PM: Zoom Thursdays 10:00 AM to 12:00 PM: Zoom For additional information please see Support .","title":"How to Contact Us"},{"location":"#support-and-development","text":"RCS is developed and supported by UAB IT's Research Computing Group. We are developing a core set of applications to help you to easily incorporate our services into your research processes and this documentation collection to help you leverage the resources already available. We follow the best practices of the Open Source community and develop our software in an open-source fashion. RCS is an out growth of the UABgrid pilot, launched in September 2007 which has focused on demonstrating the utility of unlimited analysis, storage, and application for research. RCS is built on the same technology foundations used by major cloud vendors and decades of distributed systems computing research, technology that powered the last ten years of large scale systems serving prominent national and international initiatives like the Open Science Grid , XSEDE , the LHC Computing Grid , and NCIP .","title":"Support and Development"},{"location":"#outreach","text":"The UAB IT Research Computing Group has collaborated with a number of prominent research projects at UAB to identify use cases and develop requirements. Our collaborators include, but are not limited to, the Center for Clinical and Translational Science (CCTS), Heflin Genomics Center, the Comprehensive Cancer Center (CCC), the Department of Computer and Information Sciences (CIS), the Department of Mechanical Engineering (ME), Lister Hill Library, the School of Optometry's Center for the Development of Functional Imaging, and Health System Information Services (HSIS). As part of the process of building RC, the UAB IT Research Computing Group has hosted an annual campus symposium on research computing and cyber-infrastructure (CI) developments and accomplishments. Starting as CyberInfrastructure (CI) Days in 2007, the name was changed to UAB Research Computing Day in 2011 to reflect the broader mission to support research. IT Research Computing also participates in other campus wide symposiums including UAB Research Core Day.","title":"Outreach"},{"location":"policies/","text":"UAB Research Computing Policies \u00b6 Construction This page is a stub and is under construction. UAB IT Policies \u00b6 All users of UAB IT systems, including UAB Research Computing systems, must agree to all UAB relevant UAB IT policies. UAB IT policies may be found at https://www.uab.edu/it/home/policies . The list of policies below are those that may be most relevant to typical use cases on UAB Research Computing systems. The list below is not an exhaustive list, and it is the responsibility of each individual using the system to be aware of and follow all relevant policies, whether listed here or not. Acceptable Use Policy (AUP) \u00b6 Acceptable Use Policy Data Classification \u00b6 Data Classification","title":"Policies"},{"location":"policies/#uab-research-computing-policies","text":"Construction This page is a stub and is under construction.","title":"UAB Research Computing Policies"},{"location":"policies/#uab-it-policies","text":"All users of UAB IT systems, including UAB Research Computing systems, must agree to all UAB relevant UAB IT policies. UAB IT policies may be found at https://www.uab.edu/it/home/policies . The list of policies below are those that may be most relevant to typical use cases on UAB Research Computing systems. The list below is not an exhaustive list, and it is the responsibility of each individual using the system to be aware of and follow all relevant policies, whether listed here or not.","title":"UAB IT Policies"},{"location":"policies/#acceptable-use-policy-aup","text":"Acceptable Use Policy","title":"Acceptable Use Policy (AUP)"},{"location":"policies/#data-classification","text":"Data Classification","title":"Data Classification"},{"location":"account_management/cheaha_account/","text":"Cheaha Account Management \u00b6 These instructions are intended to guide researchers on creating new accounts and managing existing accounts. Creating a New Account \u00b6 Creating a new account is a simple, automated, self-service process. To start, navigate to https://rc.uab.edu , our Open OnDemand web portal, and authenticate. The authentication process differs depending on your affiliation. Accounts are available to researchers with the following situations. If you are affiliated with UAB and have a BlazerID, please authenticate using Single Sign-On (SSO). If you are affiliated with UAB Medicine, you will need to use your BlazerID to authenticate via Single Sign-On (SSO) instead of your UABMC authentication process. If you are an external collaborator and have a XIAS account with access to Cheaha, please authenticate using your XIAS email address as the ID, not automatically generated xias-XXXXXX-1 ID. If you are an external collaborator and do not have a XIAS account, you will need a UAB-affiliated sponsor and will need to follow our XIAS Guest Account Instructions . Your sponsor will need to follow our XIAS Site Management and XIAS Guest Management documentation pages. Once you have authenticated, you should see a page that looks like the following. The form should be prefilled with your BlazerID or XIAS ID, full name, and email address. If any of these details are incorrect please Contact Support . Please also fill out the reason you wish to create an account. To create a Cheaha account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Create Account\" to start the account creation process. You should see a popup notification that looks like the following. After a few moments you should be redirected to our Open OnDemand web portal. If not, please Contact Support . Welcome to Cheaha and to Research Computing! Managing an Existing Account \u00b6 If you already have an account and wish to check it's status, please visit your account status page at https://rc.uab.edu/account . Account in Good Standing \u00b6 If your account is in good standing you should see a page like the following. Account Requires Certification \u00b6 We review accounts periodically to ensure the system is being used fairly and as intended. Part of this process is to certify that researchers with accounts still wish to make use of Cheaha. Once per year every researcher will be required to certify their account before making use of Cheaha. If you account requires certification, when logging in to our Open OnDemand web portal you will see the following notification page. . To certify your account, click the button to be taken to the certification form, which should look like the following. While the certification form looks similar to the new account creation form, please be sure to review all of the information carefully. To certify your account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Certify Account\" to start the account creation process. You should see a popup notification confirming the process is working. After a few moments your account should be certified and you will be free to use Cheaha again. If not, please Contact Support . Account on Hold \u00b6 Mistakes happen, and sometimes what we thought we programmed wasn't quite what we actually programmed. When these kinds of mistakes occur, excess resources may get used. If this impacts performance or other users excessively, we may put a hold on your account. We may also put a hold on your account if you do not complete Account Certification when required. Other reasons for holds include, but are not limited to: Misuse (intentional or not) of Research Computing resources IT Policy violations HIPAA or FERPA violations related to use of Cheaha As part of a required investigation In rare circumstances, we may also place a hold on your account if you possess the sole copy of data not owned by you. If your account is on hold, you will see a page like the following. If you SSH into the cluster while your account is on hold you will see the following text in your terminal. If your account is on hold and we have not already contacted you, or you believe the hold to be in error, please Contact Support . Authorization Error \u00b6 Periodically, we review all researcher accounts to ensure they are authorized to use Cheaha based on affiliation status. If we find a researcher is no longer affiliated with UAB, we may disable the account. If you are not authorized to use Cheaha, you will see a page like the following. If you believe this to be in error, please Contact Support .","title":"Cheaha Account"},{"location":"account_management/cheaha_account/#cheaha-account-management","text":"These instructions are intended to guide researchers on creating new accounts and managing existing accounts.","title":"Cheaha Account Management"},{"location":"account_management/cheaha_account/#creating-a-new-account","text":"Creating a new account is a simple, automated, self-service process. To start, navigate to https://rc.uab.edu , our Open OnDemand web portal, and authenticate. The authentication process differs depending on your affiliation. Accounts are available to researchers with the following situations. If you are affiliated with UAB and have a BlazerID, please authenticate using Single Sign-On (SSO). If you are affiliated with UAB Medicine, you will need to use your BlazerID to authenticate via Single Sign-On (SSO) instead of your UABMC authentication process. If you are an external collaborator and have a XIAS account with access to Cheaha, please authenticate using your XIAS email address as the ID, not automatically generated xias-XXXXXX-1 ID. If you are an external collaborator and do not have a XIAS account, you will need a UAB-affiliated sponsor and will need to follow our XIAS Guest Account Instructions . Your sponsor will need to follow our XIAS Site Management and XIAS Guest Management documentation pages. Once you have authenticated, you should see a page that looks like the following. The form should be prefilled with your BlazerID or XIAS ID, full name, and email address. If any of these details are incorrect please Contact Support . Please also fill out the reason you wish to create an account. To create a Cheaha account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Create Account\" to start the account creation process. You should see a popup notification that looks like the following. After a few moments you should be redirected to our Open OnDemand web portal. If not, please Contact Support . Welcome to Cheaha and to Research Computing!","title":"Creating a New Account"},{"location":"account_management/cheaha_account/#managing-an-existing-account","text":"If you already have an account and wish to check it's status, please visit your account status page at https://rc.uab.edu/account .","title":"Managing an Existing Account"},{"location":"account_management/cheaha_account/#account-in-good-standing","text":"If your account is in good standing you should see a page like the following.","title":"Account in Good Standing"},{"location":"account_management/cheaha_account/#account-requires-certification","text":"We review accounts periodically to ensure the system is being used fairly and as intended. Part of this process is to certify that researchers with accounts still wish to make use of Cheaha. Once per year every researcher will be required to certify their account before making use of Cheaha. If you account requires certification, when logging in to our Open OnDemand web portal you will see the following notification page. . To certify your account, click the button to be taken to the certification form, which should look like the following. While the certification form looks similar to the new account creation form, please be sure to review all of the information carefully. To certify your account, you must check both boxes affirming your acceptance of relevant UAB IT policies. Until both boxes are checked, the \"Create Account\" button will not be usable. When you are ready, click \"Certify Account\" to start the account creation process. You should see a popup notification confirming the process is working. After a few moments your account should be certified and you will be free to use Cheaha again. If not, please Contact Support .","title":"Account Requires Certification"},{"location":"account_management/cheaha_account/#account-on-hold","text":"Mistakes happen, and sometimes what we thought we programmed wasn't quite what we actually programmed. When these kinds of mistakes occur, excess resources may get used. If this impacts performance or other users excessively, we may put a hold on your account. We may also put a hold on your account if you do not complete Account Certification when required. Other reasons for holds include, but are not limited to: Misuse (intentional or not) of Research Computing resources IT Policy violations HIPAA or FERPA violations related to use of Cheaha As part of a required investigation In rare circumstances, we may also place a hold on your account if you possess the sole copy of data not owned by you. If your account is on hold, you will see a page like the following. If you SSH into the cluster while your account is on hold you will see the following text in your terminal. If your account is on hold and we have not already contacted you, or you believe the hold to be in error, please Contact Support .","title":"Account on Hold"},{"location":"account_management/cheaha_account/#authorization-error","text":"Periodically, we review all researcher accounts to ensure they are authorized to use Cheaha based on affiliation status. If we find a researcher is no longer affiliated with UAB, we may disable the account. If you are not authorized to use Cheaha, you will see a page like the following. If you believe this to be in error, please Contact Support .","title":"Authorization Error"},{"location":"account_management/gitlab_account/","text":"UAB GitLab Overview and Registration \u00b6 Welcome to UAB GitLab! This is a UAB-specific GitLab . GitLab is similar to GitHub , but hosted here at UAB on secure servers. GitLab Use Cases \u00b6 For Researchers \u00b6 GitLab can be used: For reproducibility Analysis and software code can be kept in one, central repository everyone can use instead of spread across multiple computers/places. Code can be versioned and tracked as it changes over time. Software versions can be recorded, virtual environments can be documented, and containers can be recorded to help future-proof analyses. Collaboration GitLab is a central place to create code, edit, and track needed code changes (issues) with your lab and collaborators. Multiple people can use, modify, and merge changes in code while communicating with the broader team all along the way. Security Unlimited private repositories for internal code projects. Set behind UAB authentication. For Software Developers (and Researchers!) \u00b6 UAB GitLab is useful for software developers. It is a single application for the entire software development lifecycle. From project planning and source code management to continuous integration (CI) and continuous deployment (CD), monitoring, and security. Our GitLab instance may be found at https://gitlab.rc.uab.edu . UAB GitLab Registration \u00b6 UAB-Affiliated Researcher Registration \u00b6 If you are a UAB affiliated researcher and have a BlazerID, you may create an account by logging in at the site above using the ldap tab. Please use your single sign-on (SSO) credentials. Please use BlazerID and password instead of UABMC credentials Please use your BlazerID and BlazerID password for UAB GitLab. UABMC credentials are a different sign in system and will likely not work. Central IT groups like Research Computing do not have a way to access UABMC credentials. UABMC Researcher Registration \u00b6 Please use your BlazerID and BlazerID credentials to sign in following the directions for UAB-Affiliated Researchers. UABMC credentials should not be used for UAB GitLab. XIAS External Collaborator Registration \u00b6 If you are a collaborator with a XIAS account you'll need to follow a different procedure. Ensure that your sponsor has included https://gitlab.rc.uab.edu in the list of approved URIs on the XIAS configuration page. Email support@listserv.uab.edu providing your full name, XIAS account email address, and sponsor. UAB Research Computing will create the account. You will recieve an email from gitlab.rc.uab.edu with a link to create a password. Navigate to https://gitlab.rc.uab.edu . Click the Standard tab. In the Username or email field type the part of your XIAS email address before the @ symbol. Do not include the @ symbol or anything after it. Fill out the Password field with the GitLab password you created in Step #4. Click Sign in . Warning XIAS account researchers can only be granted access if their sponsor adds the GitLab URL to the list of approved URIs. Please see XIAS Sites for more information.","title":"Gitlab Account"},{"location":"account_management/gitlab_account/#uab-gitlab-overview-and-registration","text":"Welcome to UAB GitLab! This is a UAB-specific GitLab . GitLab is similar to GitHub , but hosted here at UAB on secure servers.","title":"UAB GitLab Overview and Registration"},{"location":"account_management/gitlab_account/#gitlab-use-cases","text":"","title":"GitLab Use Cases"},{"location":"account_management/gitlab_account/#for-researchers","text":"GitLab can be used: For reproducibility Analysis and software code can be kept in one, central repository everyone can use instead of spread across multiple computers/places. Code can be versioned and tracked as it changes over time. Software versions can be recorded, virtual environments can be documented, and containers can be recorded to help future-proof analyses. Collaboration GitLab is a central place to create code, edit, and track needed code changes (issues) with your lab and collaborators. Multiple people can use, modify, and merge changes in code while communicating with the broader team all along the way. Security Unlimited private repositories for internal code projects. Set behind UAB authentication.","title":"For Researchers"},{"location":"account_management/gitlab_account/#for-software-developers-and-researchers","text":"UAB GitLab is useful for software developers. It is a single application for the entire software development lifecycle. From project planning and source code management to continuous integration (CI) and continuous deployment (CD), monitoring, and security. Our GitLab instance may be found at https://gitlab.rc.uab.edu .","title":"For Software Developers (and Researchers!)"},{"location":"account_management/gitlab_account/#uab-gitlab-registration","text":"","title":"UAB GitLab Registration"},{"location":"account_management/gitlab_account/#uab-affiliated-researcher-registration","text":"If you are a UAB affiliated researcher and have a BlazerID, you may create an account by logging in at the site above using the ldap tab. Please use your single sign-on (SSO) credentials. Please use BlazerID and password instead of UABMC credentials Please use your BlazerID and BlazerID password for UAB GitLab. UABMC credentials are a different sign in system and will likely not work. Central IT groups like Research Computing do not have a way to access UABMC credentials.","title":"UAB-Affiliated Researcher Registration"},{"location":"account_management/gitlab_account/#uabmc-researcher-registration","text":"Please use your BlazerID and BlazerID credentials to sign in following the directions for UAB-Affiliated Researchers. UABMC credentials should not be used for UAB GitLab.","title":"UABMC Researcher Registration"},{"location":"account_management/gitlab_account/#xias-external-collaborator-registration","text":"If you are a collaborator with a XIAS account you'll need to follow a different procedure. Ensure that your sponsor has included https://gitlab.rc.uab.edu in the list of approved URIs on the XIAS configuration page. Email support@listserv.uab.edu providing your full name, XIAS account email address, and sponsor. UAB Research Computing will create the account. You will recieve an email from gitlab.rc.uab.edu with a link to create a password. Navigate to https://gitlab.rc.uab.edu . Click the Standard tab. In the Username or email field type the part of your XIAS email address before the @ symbol. Do not include the @ symbol or anything after it. Fill out the Password field with the GitLab password you created in Step #4. Click Sign in . Warning XIAS account researchers can only be granted access if their sponsor adds the GitLab URL to the list of approved URIs. Please see XIAS Sites for more information.","title":"XIAS External Collaborator Registration"},{"location":"account_management/xias/","text":"External Collaborator (XIAS) Accounts \u00b6 This segment of the docs has instructions for managing and creating XIAS accounts for external collaborators. External collaborators require XIAS accounts to access Cheaha. Obtaining an account requires a UAB-employed sponsor, typically a research PI, who will claim responsibility for the external collaborator. The XIAS account creation process is initiated by the sponsor, not by the external collaborator. An overview of creating an external collaborator account: The sponsor must first: Create and manage XIAS Sites Create and manage XIAS Guests The guest must then Create a XIAS Guest Account We recommend the sponsor and guest stay in close contact during the process in case anything unexpected occurs. If you encounter difficulties with any part of the XIAS website process, please contact AskIT . If you encounter difficulties with the Cheaha account creation portion at the Open OnDemand web portal, please contact Support","title":"External Collaborator (XIAS) Accounts"},{"location":"account_management/xias/#external-collaborator-xias-accounts","text":"This segment of the docs has instructions for managing and creating XIAS accounts for external collaborators. External collaborators require XIAS accounts to access Cheaha. Obtaining an account requires a UAB-employed sponsor, typically a research PI, who will claim responsibility for the external collaborator. The XIAS account creation process is initiated by the sponsor, not by the external collaborator. An overview of creating an external collaborator account: The sponsor must first: Create and manage XIAS Sites Create and manage XIAS Guests The guest must then Create a XIAS Guest Account We recommend the sponsor and guest stay in close contact during the process in case anything unexpected occurs. If you encounter difficulties with any part of the XIAS website process, please contact AskIT . If you encounter difficulties with the Cheaha account creation portion at the Open OnDemand web portal, please contact Support","title":"External Collaborator (XIAS) Accounts"},{"location":"account_management/xias/guest_instructions/","text":"Guest Instructions \u00b6 These instructions are for guests who have been registered by UAB faculty and staff to use internal UAB resources. Once a request for a XIAS account has been made by your UAB sponsor, you will need to follow these instructions to complete the XIAS registration and obtain access to UAB resources. All of the links used on this page are available at the UAB XIAS Guest Users page. Create Account \u00b6 The first email you receive should be a notification that a request has been made to add you as a XIAS user. This email will include the project(s)/site(s) you're being added to. The next email you receive should contain instructions on how to register your account. This email may take an hour or so to arrive after the first. It will contain an invite code that you must enter at the XIAS website, along with the email address used to register you. Navigate to the link in the email. Please practice good internet hygiene and copy the link text, instead of clicking the link! As of the time of writing the link will be to the UAB XIAS Guest Users page. Once at the main page, click the \"Enter Invite or Reset Code\" link. You will be taken to the \"Register XIAS Account\" page. Enter the email address used to register you for a XIAS account, and the code from the email you received with registration instructions. Then click proceed. Enter your first and last names, then click proceed. Enter a password that will be used with your XIAS account. This password can be changed later, and your account can be recovered if the password is lost. Click proceed. You will be taken to a confirmation page. If everything is acceptable, click proceed. Otherwise click edit next to the incorrect field. Your XIAS email cannot be changed. If the email is not correct you will need to communicate with your sponsor to start the entire process over from the beginning. You should be taken to a page indicating success. Please carefully read the page and follow any instructions. If you do not see a success page, please contact your sponsor about next steps. Following this step, your account registration is complete and you should be able to access the resources you have been granted permission to use. Most internal UAB systems use a Single Sign-On (SSO) to simplify and standardize logging in. For those sites that don't you will need to activate your account manually. To manually activate accounts for resources that do not use SSO click the \"Activate (Sync) Accounts\" link on the left hand navigation pane. Fill out the form using the email used to register the XIAS account and the current password. Required Software for Research Computing Access \u00b6 Research computing software requires security software be installed on your devices in order to login. Duo two-factor authentication (2FA) software is required on your mobile to device to access any Single Sign-on services. VPN access software is required for some services when connected from outside the UAB internal network. Accessing the VPN also requires Duo 2FA. Below is a list of Research Computing services and their required software. Cheaha: 2FA Cloud.rc: VPN and 2FA if off-campus Change Password and Recover From Lost Password \u00b6 To change your password, or recover your account in case of a lost password, please click the \"Change XIAS Password\" link in the left hand panel of the main page. Once there, follow the instructions on the form. Resend Invite Code \u00b6 If your invite code has expired, you can have a new invite code sent to you by clicking the \"Resend Invite Code\" link in the left hand panel of the main page. Once there, follow the instructions on the form. Guest IT Info \u00b6 For more information on UAB IT policies and other useful and helpful information, please click the \"UABIT Guest User info\" link.","title":"For Guests - Account Creation"},{"location":"account_management/xias/guest_instructions/#guest-instructions","text":"These instructions are for guests who have been registered by UAB faculty and staff to use internal UAB resources. Once a request for a XIAS account has been made by your UAB sponsor, you will need to follow these instructions to complete the XIAS registration and obtain access to UAB resources. All of the links used on this page are available at the UAB XIAS Guest Users page.","title":"Guest Instructions"},{"location":"account_management/xias/guest_instructions/#create-account","text":"The first email you receive should be a notification that a request has been made to add you as a XIAS user. This email will include the project(s)/site(s) you're being added to. The next email you receive should contain instructions on how to register your account. This email may take an hour or so to arrive after the first. It will contain an invite code that you must enter at the XIAS website, along with the email address used to register you. Navigate to the link in the email. Please practice good internet hygiene and copy the link text, instead of clicking the link! As of the time of writing the link will be to the UAB XIAS Guest Users page. Once at the main page, click the \"Enter Invite or Reset Code\" link. You will be taken to the \"Register XIAS Account\" page. Enter the email address used to register you for a XIAS account, and the code from the email you received with registration instructions. Then click proceed. Enter your first and last names, then click proceed. Enter a password that will be used with your XIAS account. This password can be changed later, and your account can be recovered if the password is lost. Click proceed. You will be taken to a confirmation page. If everything is acceptable, click proceed. Otherwise click edit next to the incorrect field. Your XIAS email cannot be changed. If the email is not correct you will need to communicate with your sponsor to start the entire process over from the beginning. You should be taken to a page indicating success. Please carefully read the page and follow any instructions. If you do not see a success page, please contact your sponsor about next steps. Following this step, your account registration is complete and you should be able to access the resources you have been granted permission to use. Most internal UAB systems use a Single Sign-On (SSO) to simplify and standardize logging in. For those sites that don't you will need to activate your account manually. To manually activate accounts for resources that do not use SSO click the \"Activate (Sync) Accounts\" link on the left hand navigation pane. Fill out the form using the email used to register the XIAS account and the current password.","title":"Create Account"},{"location":"account_management/xias/guest_instructions/#required-software-for-research-computing-access","text":"Research computing software requires security software be installed on your devices in order to login. Duo two-factor authentication (2FA) software is required on your mobile to device to access any Single Sign-on services. VPN access software is required for some services when connected from outside the UAB internal network. Accessing the VPN also requires Duo 2FA. Below is a list of Research Computing services and their required software. Cheaha: 2FA Cloud.rc: VPN and 2FA if off-campus","title":"Required Software for Research Computing Access"},{"location":"account_management/xias/guest_instructions/#change-password-and-recover-from-lost-password","text":"To change your password, or recover your account in case of a lost password, please click the \"Change XIAS Password\" link in the left hand panel of the main page. Once there, follow the instructions on the form.","title":"Change Password and Recover From Lost Password"},{"location":"account_management/xias/guest_instructions/#resend-invite-code","text":"If your invite code has expired, you can have a new invite code sent to you by clicking the \"Resend Invite Code\" link in the left hand panel of the main page. Once there, follow the instructions on the form.","title":"Resend Invite Code"},{"location":"account_management/xias/guest_instructions/#guest-it-info","text":"For more information on UAB IT policies and other useful and helpful information, please click the \"UABIT Guest User info\" link.","title":"Guest IT Info"},{"location":"account_management/xias/pi_guest_management/","text":"Managing UAB XIAS Users \u00b6 Note These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts. Important To complete these steps you will first need to createa a Project/Site UAB XIAS User management allow UAB faculty and staff to grant external collaborators access to specific resources on the internal UAB Campus Network. All XIAS users must be connected with at least one site, so you'll need to create one at the UAB XIAS User Management Webpage . All XIAS Users must also have an expiration date. Adding Users \u00b6 Before adding users, have a list of user emails handy for the site you wish to add users to, as well as expiration dates for each user. You will need to create a Project/Site before you can add external collaborators. To start go to the UAB XIAS User Management Webpage and click Manage Users in the left menu. Select the Project/Site you wish to add users to from the drop down box. Click \"Register\" to open a form for adding new users. Fill in the form. All fields are required. Checkbox list - Leave the site checked. End date - An expiration date for the users being added. Cannot be longer than the end date for the selected Project/Site. Text box - Enter a list of e-mail addresses for users to add. Click \"Submit\" to move to a confirmation page. Check the emails are correct and click \"Add\" to submit the information Emails will be sent to all email addresses for next steps. You will be redirected to the UAB XIAS User Management Webpage, which should now have the text \"Registration successful.\" near the top. To complete their registration, please direct your external collaborators to the UAB XIAS Guest Users page . When they have completed their registration, you should receive an email like the following. Once the guest XIAS account has been created, the guest will need to login at https://rc.uab.edu and follow the automated Cheaha Account Creation Process to create a Cheaha account. They will need to use the same email and password they used when creating their XIAS account. Discovering and Managing Users \u00b6 There are two ways to discover XIAS users you are currently sponsoring. The first is to search by email address. The second is to list all users associated with a site. Discovering Users \u00b6 To locate users by e-mail address: type their email into the \"Locate specific user(s) by e-mail address\" text field on the \"Manage Users\" page. To manage users by site: select the site from the drop-down box and click the \"List\" button. The page will reload with a table containing name, email, and start and end dates. The end date is when the XIAS user registration expires. To change the end date for user(s), click the \"Sel\" checkbox next to their names, enter a date in the \"Change end date for selected users to\" text field, and click \"Update\". Revoking User Privileges \u00b6 Warning THIS INFORMATION IS PENDING TESTING Users cannot have their XIAS account deleted. However, privileges may be revoked. To revoke user privileges, follow the instructions for managing users by site. Update the desired user(s)' end date to a date earlier than the current date. Important If you need to urgently revoke privileges, please also notify UAB IT by emailing AskIT@uab.edu as soon as possible. Please be clear about what is needed and when.","title":"For PIs - (2) Managing Guests"},{"location":"account_management/xias/pi_guest_management/#managing-uab-xias-users","text":"Note These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts. Important To complete these steps you will first need to createa a Project/Site UAB XIAS User management allow UAB faculty and staff to grant external collaborators access to specific resources on the internal UAB Campus Network. All XIAS users must be connected with at least one site, so you'll need to create one at the UAB XIAS User Management Webpage . All XIAS Users must also have an expiration date.","title":"Managing UAB XIAS Users"},{"location":"account_management/xias/pi_guest_management/#adding-users","text":"Before adding users, have a list of user emails handy for the site you wish to add users to, as well as expiration dates for each user. You will need to create a Project/Site before you can add external collaborators. To start go to the UAB XIAS User Management Webpage and click Manage Users in the left menu. Select the Project/Site you wish to add users to from the drop down box. Click \"Register\" to open a form for adding new users. Fill in the form. All fields are required. Checkbox list - Leave the site checked. End date - An expiration date for the users being added. Cannot be longer than the end date for the selected Project/Site. Text box - Enter a list of e-mail addresses for users to add. Click \"Submit\" to move to a confirmation page. Check the emails are correct and click \"Add\" to submit the information Emails will be sent to all email addresses for next steps. You will be redirected to the UAB XIAS User Management Webpage, which should now have the text \"Registration successful.\" near the top. To complete their registration, please direct your external collaborators to the UAB XIAS Guest Users page . When they have completed their registration, you should receive an email like the following. Once the guest XIAS account has been created, the guest will need to login at https://rc.uab.edu and follow the automated Cheaha Account Creation Process to create a Cheaha account. They will need to use the same email and password they used when creating their XIAS account.","title":"Adding Users"},{"location":"account_management/xias/pi_guest_management/#discovering-and-managing-users","text":"There are two ways to discover XIAS users you are currently sponsoring. The first is to search by email address. The second is to list all users associated with a site.","title":"Discovering and Managing Users"},{"location":"account_management/xias/pi_guest_management/#discovering-users","text":"To locate users by e-mail address: type their email into the \"Locate specific user(s) by e-mail address\" text field on the \"Manage Users\" page. To manage users by site: select the site from the drop-down box and click the \"List\" button. The page will reload with a table containing name, email, and start and end dates. The end date is when the XIAS user registration expires. To change the end date for user(s), click the \"Sel\" checkbox next to their names, enter a date in the \"Change end date for selected users to\" text field, and click \"Update\".","title":"Discovering Users"},{"location":"account_management/xias/pi_guest_management/#revoking-user-privileges","text":"Warning THIS INFORMATION IS PENDING TESTING Users cannot have their XIAS account deleted. However, privileges may be revoked. To revoke user privileges, follow the instructions for managing users by site. Update the desired user(s)' end date to a date earlier than the current date. Important If you need to urgently revoke privileges, please also notify UAB IT by emailing AskIT@uab.edu as soon as possible. Please be clear about what is needed and when.","title":"Revoking User Privileges"},{"location":"account_management/xias/pi_site_management/","text":"Creating a UAB XIAS Project/Site \u00b6 Note These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts. XIAS Project/Sites, or simply sites, tie external users to specific resources at UAB. By connecting people to the resource they use, UAB can maintain security and accountability. Creating a site is the first step to giving access to external collaborators, and the process can be thought of as \"create once, use many times\". All sites must have an expiration date for security reasons. To create a site you'll need at least one Uniform Resource Identifier (URI) relating to resources used by the site. If you aren't sure what URI(s) to list for your site, please contact UserServices@uab.edu . To start go to the UAB XIAS Project/Site Management Webpage . Click \"New\" to open a form for creating a new Project/Site. Fill in the form. All fields are required. Short name for project/site - A memorable name for your project or site. Longer description - A complete yet concise description of the project or site and its resources. Start date - The start date, can be today. End date - An expiration date for the project or site. URIs - One or more uniform resource locators (URIs) associated with the site, to increase accountability. Cheaha URI: https://rc.uab.edu Cloud URI: https://cloud.rc.uab.edu GitLab URI: https://gitlab.rc.uab.edu Click \"Add\" to submit the form. You should be taken to a page summarizing the created Project/Site. When you visit the \"Manage Projects/Sites\" page in the future, you will see a table with the newly created Project/Site listed. Click \"View\" to return to the page seen in the previous step. Click \"Edit\" to return to the form from [link]. Click \"Users\" to manage users for this site.","title":"For PIs - (1) Managing Projects & Sites"},{"location":"account_management/xias/pi_site_management/#creating-a-uab-xias-projectsite","text":"Note These instructions are intended for use by UAB-employed PIs to organize external collaborators, also known as guests. UAB PIs: Please direct guests here for instructions on creating their accounts. XIAS Project/Sites, or simply sites, tie external users to specific resources at UAB. By connecting people to the resource they use, UAB can maintain security and accountability. Creating a site is the first step to giving access to external collaborators, and the process can be thought of as \"create once, use many times\". All sites must have an expiration date for security reasons. To create a site you'll need at least one Uniform Resource Identifier (URI) relating to resources used by the site. If you aren't sure what URI(s) to list for your site, please contact UserServices@uab.edu . To start go to the UAB XIAS Project/Site Management Webpage . Click \"New\" to open a form for creating a new Project/Site. Fill in the form. All fields are required. Short name for project/site - A memorable name for your project or site. Longer description - A complete yet concise description of the project or site and its resources. Start date - The start date, can be today. End date - An expiration date for the project or site. URIs - One or more uniform resource locators (URIs) associated with the site, to increase accountability. Cheaha URI: https://rc.uab.edu Cloud URI: https://cloud.rc.uab.edu GitLab URI: https://gitlab.rc.uab.edu Click \"Add\" to submit the form. You should be taken to a page summarizing the created Project/Site. When you visit the \"Manage Projects/Sites\" page in the future, you will see a table with the newly created Project/Site listed. Click \"View\" to return to the page seen in the previous step. Click \"Edit\" to return to the form from [link]. Click \"Users\" to manage users for this site.","title":"Creating a UAB XIAS Project/Site"},{"location":"cheaha/getting_started/","text":"Getting Started \u00b6 Cheaha is a High Performance Computing (HPC) resource intended primarily for batch processing of research computing software. We offer a user-friendly portal website Open OnDemand with graphical interfaces to the most common features, all in one place. Read on to learn more about our resources and how to access them. Getting Help \u00b6 Please Contact Us with requests for support. Tips on getting effective support are here , and our frequently asked questions are here . Account Creation \u00b6 Please visit our Account Creation page for detailed instructions on creating a Cheaha account. Accessing Cheaha \u00b6 The primary method for accessing Cheaha is through our online portal website, Open OnDemand. To login to our portal, navigate to our https://rc.uab.edu , which does not require an on-campus connection nor the UAB Campus VPN. You should be presented with UAB's Single Sign-on page, which will require use of Duo 2FA . Login using the appropriate credentials laid out at our Account Creation page . SSH may be used to access Cheaha. Connect to host cheaha.rc.uab.edu on port 22 . Open OnDemand Features \u00b6 The Open OnDemand portal features a file browser , job composer and various interactive applications including a remote desktop, Jupyter, RStudio and MATLAB, among others. There is also a terminal usable directly in the browser for very basic functions such as file management. More detailed documentation may be found on our Open OnDemand page . Hardware \u00b6 A full list of the available hardware can be found on our hardware page . Storage \u00b6 All researchers are granted 5 TB of individual storage when they create their Research Computing account . Shared storage is available to all Lab Groups and Core Facilities on campus. Shared storage is also available to UAB Administration groups. Please visit our Storage page for detailed information about our individual and shared storage options. Etiquette \u00b6 Quality-of-Service (QoS) limits are in place to ensure any one user can't monopolize all resources. Running Tasks on Compute Nodes \u00b6 There two main node types on Cheaha for researchers, the login node and many compute nodes. All expensive compute tasks must be run on compute nodes. Tasks running on the login node slow down processes for everyone, and in extreme cases can cause service outages affecting your work and the work of many of your colleagues. We will contact you if we find processes on the login node to help move your tasks to compute nodes. You are on compute nodes if: using Open OnDemand Interactive Apps using Open OnDemand Job Composer terminal prompt looks like [<blazerid>@c0001 ~]$ You are on the login node if: terminal prompt looks like [<blazerid>@login001 ~]$ Important If you are doing more than minor file management, you will need to use a compute node. Please request an interactive session at https://rc.uab.edu or through a job submitted using Slurm . Slurm \u00b6 Slurm is our job queueing software used for submitting any number of job scripts to run on the cluster. We have documentation on how to set up job scripts and submit them further in . More complete documentation is available at https://slurm.schedmd.com/ . Software \u00b6 A large variety of software is available on Cheaha as modules. To view and use these modules see the following documentation . For new software installation, please try searching Anaconda for packages first. If you still need help, please send a support ticket Conda Packages \u00b6 A significant amount of open-source software is distributed as Anaconda or Python libraries. These libraries can be installed by the user without permission from Research Computing using Anaconda environments. To read more about using Anaconda virtual environments see our Anaconda page . If the software installation instructions tell you to use either conda install or pip install commands, the software and its dependencies can be installed using a virtual environment.","title":"Getting Started"},{"location":"cheaha/getting_started/#getting-started","text":"Cheaha is a High Performance Computing (HPC) resource intended primarily for batch processing of research computing software. We offer a user-friendly portal website Open OnDemand with graphical interfaces to the most common features, all in one place. Read on to learn more about our resources and how to access them.","title":"Getting Started"},{"location":"cheaha/getting_started/#getting-help","text":"Please Contact Us with requests for support. Tips on getting effective support are here , and our frequently asked questions are here .","title":"Getting Help"},{"location":"cheaha/getting_started/#account-creation","text":"Please visit our Account Creation page for detailed instructions on creating a Cheaha account.","title":"Account Creation"},{"location":"cheaha/getting_started/#accessing-cheaha","text":"The primary method for accessing Cheaha is through our online portal website, Open OnDemand. To login to our portal, navigate to our https://rc.uab.edu , which does not require an on-campus connection nor the UAB Campus VPN. You should be presented with UAB's Single Sign-on page, which will require use of Duo 2FA . Login using the appropriate credentials laid out at our Account Creation page . SSH may be used to access Cheaha. Connect to host cheaha.rc.uab.edu on port 22 .","title":"Accessing Cheaha"},{"location":"cheaha/getting_started/#open-ondemand-features","text":"The Open OnDemand portal features a file browser , job composer and various interactive applications including a remote desktop, Jupyter, RStudio and MATLAB, among others. There is also a terminal usable directly in the browser for very basic functions such as file management. More detailed documentation may be found on our Open OnDemand page .","title":"Open OnDemand Features"},{"location":"cheaha/getting_started/#hardware","text":"A full list of the available hardware can be found on our hardware page .","title":"Hardware"},{"location":"cheaha/getting_started/#storage","text":"All researchers are granted 5 TB of individual storage when they create their Research Computing account . Shared storage is available to all Lab Groups and Core Facilities on campus. Shared storage is also available to UAB Administration groups. Please visit our Storage page for detailed information about our individual and shared storage options.","title":"Storage"},{"location":"cheaha/getting_started/#etiquette","text":"Quality-of-Service (QoS) limits are in place to ensure any one user can't monopolize all resources.","title":"Etiquette"},{"location":"cheaha/getting_started/#running-tasks-on-compute-nodes","text":"There two main node types on Cheaha for researchers, the login node and many compute nodes. All expensive compute tasks must be run on compute nodes. Tasks running on the login node slow down processes for everyone, and in extreme cases can cause service outages affecting your work and the work of many of your colleagues. We will contact you if we find processes on the login node to help move your tasks to compute nodes. You are on compute nodes if: using Open OnDemand Interactive Apps using Open OnDemand Job Composer terminal prompt looks like [<blazerid>@c0001 ~]$ You are on the login node if: terminal prompt looks like [<blazerid>@login001 ~]$ Important If you are doing more than minor file management, you will need to use a compute node. Please request an interactive session at https://rc.uab.edu or through a job submitted using Slurm .","title":"Running Tasks on Compute Nodes"},{"location":"cheaha/getting_started/#slurm","text":"Slurm is our job queueing software used for submitting any number of job scripts to run on the cluster. We have documentation on how to set up job scripts and submit them further in . More complete documentation is available at https://slurm.schedmd.com/ .","title":"Slurm"},{"location":"cheaha/getting_started/#software","text":"A large variety of software is available on Cheaha as modules. To view and use these modules see the following documentation . For new software installation, please try searching Anaconda for packages first. If you still need help, please send a support ticket","title":"Software"},{"location":"cheaha/getting_started/#conda-packages","text":"A significant amount of open-source software is distributed as Anaconda or Python libraries. These libraries can be installed by the user without permission from Research Computing using Anaconda environments. To read more about using Anaconda virtual environments see our Anaconda page . If the software installation instructions tell you to use either conda install or pip install commands, the software and its dependencies can be installed using a virtual environment.","title":"Conda Packages"},{"location":"cheaha/hardware/","text":"Hardware Information \u00b6 The following hardware summaries may be useful for selecting partitions for workflows and for grant proposal writing. If any information is missing that would be helpful to you, please be sure to contact us or create an issue on our tracker . Tip The tables in this section are wide and can be scrolled horizontally to display more information. Cheaha HPC Cluster \u00b6 Summary \u00b6 The table below contains a summary of the computational resources available on Cheaha and relevant Quality of Service (QoS) Limits. QoS limits allow us to balance usage and ensure fairness for all researchers using the cluster. QoS limits are not a guarantee of resource availability. In the table, Slurm partitions are grouped by shared QoS limits on cores, memory, and GPUs. Node limits are applied to partitions independently. All limits are applied to researchers independently. Examples of how to make use of the table: Suppose you submit 30 jobs to the \"express\" partition, and suppose each job needs 10 cores each. Hypothetically, in order for all of the jobs to start at once, 300 cores would be required. The QoS limit on cores is 264 on the \"express\" partition, so at most 26 jobs (260 cores) can start at once. The remaining 4 jobs will be held in queue, because starting one more would go beyond the QoS limit (270 > 264). Suppose you submit 5 jobs to the \"medium\" partition and 5 to the \"long\" partition, each requiring 1 node. Then, 10 total nodes would be needed. In this case, it is possible that all 10 jobs can start at once because partition node limits are separate. If all 5 jobs start, jobs on the \"medium\" partition. Suppose you submit 5 jobs to the \"amperenodes\" partition and 5 to \"amperenodes-medium\", for a total of 10 A100 GPUs. Additionally, you also submit 4 jobs to the \"pascalnodes\" partition totaling 8 P100 GPUs. Then 4 of the \"gpu: ampere\" group jobs can start at once, because the QoS limit is 4 GPUs there. Additionally, all 4 of the \"gpu: pascal\" group jobs, because the QoS limit is 8 GPUs there. In this case, the QoS for each group is separate. Partition Time Limit in Hours Nodes (Limit/Partition) Cores/Node (Limit/Person) Mem GB/Node (Limit/Person) GPU/Node (Limit/Person) cpu: amd amd-hdr100 150 34 (5) 128 (264) 504 (3072) cpu: intel express 2 51 (~) 48 (264) 754 (3072) short 12 51 (44) 48 (264) 754 (3072) medium 50 51 (44) 48 (264) 754 (3072) long 150 51 (5) 48 (264) 754 (3072) gpu: ampere amperenodes 12 20 (TBD) 32 (64) 189 (384) 2 (4) amperenodes-medium 48 20 (TBD) 32 (64) 189 (384) 2 (4) gpu: pascal pascalnodes 12 18 (~) 28 (56) 252 (500) 4 (8) pascalnodes-medium 48 7 (~) 28 (56) 252 (500) 4 (8) mem: large largemem 50 13 (10) 24 (290) 755 (7168) largemem-long 150 5 (10) 24 (290) 755 (7168) The full table can be downloaded here . Details \u00b6 Detailed hardware information, including processor and GPU makes and models, core clock frequencies, and other information for current hardware are in the table below. Generation Compute Type Total Cores Total Memory Gb Total Gpus Cores Per Node Cores Per Die Dies Per Node Die Brand Die Name Die Frequency Ghz Memory Per Node Gb Gpu Per Node Gpu Brand Gpu Name Gpu Memory Gb Nodes 1 cpu: amd 128 1024 2 1 2 AMD Opteron 242 1.6 16 64 2 cpu: intel 192 1152 8 4 2 Intel Xeon E5450 3 48 24 3 cpu: intel 384 1536 12 6 2 Intel Xeon X5650 2.66 48 32 3 cpu: intel 192 1536 12 6 2 Intel Xeon X5650 2.66 96 16 4 cpu: intel 48 1152 16 8 2 Intel Xeon X5650 2.7 384 3 5 cpu: intel 192 1152 16 8 2 Intel Xeon E2650 2 96 12 6 cpu: intel 336 5376 24 12 2 Intel Xeon E5-2680 v3 2.5 384 14 6 cpu: intel 912 9728 24 12 2 Intel Xeon E5-2680 v3 2.5 256 38 6 cpu: intel 1056 5632 24 12 2 Intel Xeon E5-2680 v3 2.5 128 44 7 gpu: pascal 504 4608 72 28 14 2 Intel Xeon E5-2680 v4 2.4 256 4 NVIDIA Tesla P100 16 18 8 cpu: intel 504 4032 24 12 2 Intel Xeon E5-2680 v4 2.5 192 21 8 mem: large 240 7680 24 12 2 Intel Xeon E5-2680 v4 2.5 768 10 8 mem: large 96 6144 24 12 2 Intel Xeon E5-2680 v4 2.5 1536 4 9 cpu: intel 2496 39936 48 24 2 Intel Xeon Gold 6248R 3 768 52 10 cpu: amd 4352 17408 128 64 2 AMD Epyc 7713 Milan 2 512 34 11 gpu: ampere 2560 10240 40 128 64 2 AMD Epyc 7763 Milan 2.45 512 2 NVIDIA A100 80 20 1 cpu: intel 240 960 48 12 4 Intel Xeon Gold 6248R 3 192 5 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 1 cpu: intel 144 576 48 12 4 Intel Xeon Gold 6248R 3 192 3 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 7 1.08 17.06 18.14 18 326.43 8 0.96 0.96 21 20.16 8 0.96 0.96 10 9.60 8 0.96 0.96 4 3.84 9 2.30 2.30 52 119.81 10 4.10 4.10 34 139.26 11 5.02 15.14 20.15 20 403.10 Total 1,022.20 The full table can be downloaded here . For information on using Cheaha, see our dedicated section . Cloud Service at cloud.rc \u00b6 The Cloud service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info cloud 1 cpu 240 960 48 192 5 Intel Xeon Gold 6248R 3.00 GHz cloud 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 752 5056 32 9 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 5 11.52 1 4.61 77.97 82.58 4 330.3 Total 341.82 The full table can be downloaded here . For information on using our Cloud service at cloud.rc, see our dedicated section . Kubernetes Container Service \u00b6 Important The Kubernetes fabric is still in deployment and not ready for researcher use. We will be sure to inform you when the service is ready. The following information is planned hardware. The Kubernetes container service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info container 1 cpu 144 576 48 192 3 Intel Xeon Gold 6248R 3.00 GHz container 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 656 4672 32 7 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 3 6.91 1 4.61 77.97 82.58 4 330.3 Total 337.21 The full table can be downloaded here .","title":"Hardware"},{"location":"cheaha/hardware/#hardware-information","text":"The following hardware summaries may be useful for selecting partitions for workflows and for grant proposal writing. If any information is missing that would be helpful to you, please be sure to contact us or create an issue on our tracker . Tip The tables in this section are wide and can be scrolled horizontally to display more information.","title":"Hardware Information"},{"location":"cheaha/hardware/#cheaha-hpc-cluster","text":"","title":"Cheaha HPC Cluster"},{"location":"cheaha/hardware/#summary","text":"The table below contains a summary of the computational resources available on Cheaha and relevant Quality of Service (QoS) Limits. QoS limits allow us to balance usage and ensure fairness for all researchers using the cluster. QoS limits are not a guarantee of resource availability. In the table, Slurm partitions are grouped by shared QoS limits on cores, memory, and GPUs. Node limits are applied to partitions independently. All limits are applied to researchers independently. Examples of how to make use of the table: Suppose you submit 30 jobs to the \"express\" partition, and suppose each job needs 10 cores each. Hypothetically, in order for all of the jobs to start at once, 300 cores would be required. The QoS limit on cores is 264 on the \"express\" partition, so at most 26 jobs (260 cores) can start at once. The remaining 4 jobs will be held in queue, because starting one more would go beyond the QoS limit (270 > 264). Suppose you submit 5 jobs to the \"medium\" partition and 5 to the \"long\" partition, each requiring 1 node. Then, 10 total nodes would be needed. In this case, it is possible that all 10 jobs can start at once because partition node limits are separate. If all 5 jobs start, jobs on the \"medium\" partition. Suppose you submit 5 jobs to the \"amperenodes\" partition and 5 to \"amperenodes-medium\", for a total of 10 A100 GPUs. Additionally, you also submit 4 jobs to the \"pascalnodes\" partition totaling 8 P100 GPUs. Then 4 of the \"gpu: ampere\" group jobs can start at once, because the QoS limit is 4 GPUs there. Additionally, all 4 of the \"gpu: pascal\" group jobs, because the QoS limit is 8 GPUs there. In this case, the QoS for each group is separate. Partition Time Limit in Hours Nodes (Limit/Partition) Cores/Node (Limit/Person) Mem GB/Node (Limit/Person) GPU/Node (Limit/Person) cpu: amd amd-hdr100 150 34 (5) 128 (264) 504 (3072) cpu: intel express 2 51 (~) 48 (264) 754 (3072) short 12 51 (44) 48 (264) 754 (3072) medium 50 51 (44) 48 (264) 754 (3072) long 150 51 (5) 48 (264) 754 (3072) gpu: ampere amperenodes 12 20 (TBD) 32 (64) 189 (384) 2 (4) amperenodes-medium 48 20 (TBD) 32 (64) 189 (384) 2 (4) gpu: pascal pascalnodes 12 18 (~) 28 (56) 252 (500) 4 (8) pascalnodes-medium 48 7 (~) 28 (56) 252 (500) 4 (8) mem: large largemem 50 13 (10) 24 (290) 755 (7168) largemem-long 150 5 (10) 24 (290) 755 (7168) The full table can be downloaded here .","title":"Summary"},{"location":"cheaha/hardware/#details","text":"Detailed hardware information, including processor and GPU makes and models, core clock frequencies, and other information for current hardware are in the table below. Generation Compute Type Total Cores Total Memory Gb Total Gpus Cores Per Node Cores Per Die Dies Per Node Die Brand Die Name Die Frequency Ghz Memory Per Node Gb Gpu Per Node Gpu Brand Gpu Name Gpu Memory Gb Nodes 1 cpu: amd 128 1024 2 1 2 AMD Opteron 242 1.6 16 64 2 cpu: intel 192 1152 8 4 2 Intel Xeon E5450 3 48 24 3 cpu: intel 384 1536 12 6 2 Intel Xeon X5650 2.66 48 32 3 cpu: intel 192 1536 12 6 2 Intel Xeon X5650 2.66 96 16 4 cpu: intel 48 1152 16 8 2 Intel Xeon X5650 2.7 384 3 5 cpu: intel 192 1152 16 8 2 Intel Xeon E2650 2 96 12 6 cpu: intel 336 5376 24 12 2 Intel Xeon E5-2680 v3 2.5 384 14 6 cpu: intel 912 9728 24 12 2 Intel Xeon E5-2680 v3 2.5 256 38 6 cpu: intel 1056 5632 24 12 2 Intel Xeon E5-2680 v3 2.5 128 44 7 gpu: pascal 504 4608 72 28 14 2 Intel Xeon E5-2680 v4 2.4 256 4 NVIDIA Tesla P100 16 18 8 cpu: intel 504 4032 24 12 2 Intel Xeon E5-2680 v4 2.5 192 21 8 mem: large 240 7680 24 12 2 Intel Xeon E5-2680 v4 2.5 768 10 8 mem: large 96 6144 24 12 2 Intel Xeon E5-2680 v4 2.5 1536 4 9 cpu: intel 2496 39936 48 24 2 Intel Xeon Gold 6248R 3 768 52 10 cpu: amd 4352 17408 128 64 2 AMD Epyc 7713 Milan 2 512 34 11 gpu: ampere 2560 10240 40 128 64 2 AMD Epyc 7763 Milan 2.45 512 2 NVIDIA A100 80 20 1 cpu: intel 240 960 48 12 4 Intel Xeon Gold 6248R 3 192 5 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 1 cpu: intel 144 576 48 12 4 Intel Xeon Gold 6248R 3 192 3 1 gpu: ampere 512 4096 32 128 64 2 AMD Epyc 7742 Rome 2.25 1024 8 NVIDIA A100 40 4 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 7 1.08 17.06 18.14 18 326.43 8 0.96 0.96 21 20.16 8 0.96 0.96 10 9.60 8 0.96 0.96 4 3.84 9 2.30 2.30 52 119.81 10 4.10 4.10 34 139.26 11 5.02 15.14 20.15 20 403.10 Total 1,022.20 The full table can be downloaded here . For information on using Cheaha, see our dedicated section .","title":"Details"},{"location":"cheaha/hardware/#cloud-service-at-cloudrc","text":"The Cloud service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info cloud 1 cpu 240 960 48 192 5 Intel Xeon Gold 6248R 3.00 GHz cloud 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 752 5056 32 9 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 5 11.52 1 4.61 77.97 82.58 4 330.3 Total 341.82 The full table can be downloaded here . For information on using our Cloud service at cloud.rc, see our dedicated section .","title":"Cloud Service at cloud.rc"},{"location":"cheaha/hardware/#kubernetes-container-service","text":"Important The Kubernetes fabric is still in deployment and not ready for researcher use. We will be sure to inform you when the service is ready. The following information is planned hardware. The Kubernetes container service hardware consists of 5 Intel nodes and 4 DGX-A100 nodes. A description of the available hardware are summarized in the following table. Fabric Generation Compute Type Partition Total Cores Total Memory Gb Total Gpus Cores Per Node Memory Per Node Gb Nodes Cpu Info Gpu Info container 1 cpu 144 576 48 192 3 Intel Xeon Gold 6248R 3.00 GHz container 1 gpu 512 4096 32 128 1024 4 AMD Epyc 7742 Rome 2.25 GHz NVIDIA A100 40 GB Total 656 4672 32 7 The full table can be downloaded here . The table below is a theoretical analysis of FLOPS (floating point operations per second) based on processor instructions and core counts, and is not a reflection of efficiency in practice. Generation Cpu Tflops Per Node Gpu Tflops Per Node Tflops Per Node Nodes Tflops 1 2.30 2.30 3 6.91 1 4.61 77.97 82.58 4 330.3 Total 337.21 The full table can be downloaded here .","title":"Kubernetes Container Service"},{"location":"cheaha/job_efficiency/","text":"Job Efficiency \u00b6 Efficient jobs save you time. Many factors go into queue wait time, but you can control your job requests. Optimizing queue wait times relies on getting resource requests close to actual resource usage. For example, if your task runs as fast with 2 cores as with 4 cores, requesting 4 cores will increase your wait time for no benefit. Other researchers time will be wasted due to locked up, unused resources. So please read this page to learn how to increase your efficiency and save time. As with any new skill, developing an intuition for efficiency and resource estimation requires experimentation, practice, and feedback. Please DO: run subsets of your data with varying resource requests to develop intuition make use of seff to validate contact us for advice if you're lost Estimating Compute Resources \u00b6 Being able to estimate the resources a job will need is critical. Requesting substantially more resources than necessary bottlenecks the cluster by preventing jobs from using resources that are reserved, but going unused. Of course requesting too few resources may cause the tasks to perform unacceptably slowly, or to fail altogether. Questions to ask yourself before requesting resources: Can my scripts take advantage of multiple cores? If yes, then request more cores. If no, then request only one core. Example: RStudio generally runs on a single thread. Any cores beyond the first will go unused and unusable. How large is the data I'm working with? Start by requesting memory equal to double the size of one file, no less than 2 GB per core. If that isn't enough, increase the request by 50% until there are no more memory errors. Example: If your data file is 4 GB, try starting out by requesting 8 GB of memory, then 12 GB, 16 GB, etc. Do my pipelines keep large amounts of data in memory? If yes, you may need to request even more memory than above. Example: Without careful programming, MATLAB code will often make and retain copies of data until finished. How long should my job take? Example: If my laptop is able to run the code on one data file in 2 hours, it will take about that long on Cheaha. Example: Requesting 50 hours of time for a 15 hour process will lengthen the queue time. Don't request too little! Include a buffer to account for scheduler and network issues. How is the software I'm using programmed? Can it use a GPU? Request one. Can it use multiple cores? Request more than one core. Is it single-threaded? Request only one core. Does it use MPI? Request multiple nodes. Be sure to check all of the flags, configuration, and options for the software, or these changes may not work. Which partition is most appropriate? More than 40 GB memory and queue wait times are long? Try largemem* . Need a GPU? Use pascalnodes* or amperenodes* . Software works with AMD? Try amd-hdr100 . Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. We are happy to help guide you to an efficient usage of the cluster. Use seff to verify that your code is as efficient as possible. Verifying Job Efficiency \u00b6 It's important to evaluate the efficiency of your job in terms of resource usage after it completes. Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. As well, because each researcher has a maximum amount of resources they can use at a given time, having inefficient jobs can increase analysis runtime across many jobs, and increase queue wait times. In order to look at job efficieny, use the seff command. seff <jobid> The output will look like: The job had poor CPU efficiency, requesting 2 CPUs which were only busy for 30% of runtime. Requesting only a single core may have made more sense here. The job also had poor memory efficiency, using less than 1 GB total memory of the requested 16 GB (5.73%). For subsequent jobs using a similar analysis and dataset size, decreasing the requested memory to about 1200 MB and a single CPU would be more efficient, and get the job queued faster. Tip Aim for between 75% and 90% memory efficiency. Lower than that is a waste of resources, but too close to 100% could result in job failure due to an unexpected out-of-memory issue.","title":"Job Efficiency"},{"location":"cheaha/job_efficiency/#job-efficiency","text":"Efficient jobs save you time. Many factors go into queue wait time, but you can control your job requests. Optimizing queue wait times relies on getting resource requests close to actual resource usage. For example, if your task runs as fast with 2 cores as with 4 cores, requesting 4 cores will increase your wait time for no benefit. Other researchers time will be wasted due to locked up, unused resources. So please read this page to learn how to increase your efficiency and save time. As with any new skill, developing an intuition for efficiency and resource estimation requires experimentation, practice, and feedback. Please DO: run subsets of your data with varying resource requests to develop intuition make use of seff to validate contact us for advice if you're lost","title":"Job Efficiency"},{"location":"cheaha/job_efficiency/#estimating-compute-resources","text":"Being able to estimate the resources a job will need is critical. Requesting substantially more resources than necessary bottlenecks the cluster by preventing jobs from using resources that are reserved, but going unused. Of course requesting too few resources may cause the tasks to perform unacceptably slowly, or to fail altogether. Questions to ask yourself before requesting resources: Can my scripts take advantage of multiple cores? If yes, then request more cores. If no, then request only one core. Example: RStudio generally runs on a single thread. Any cores beyond the first will go unused and unusable. How large is the data I'm working with? Start by requesting memory equal to double the size of one file, no less than 2 GB per core. If that isn't enough, increase the request by 50% until there are no more memory errors. Example: If your data file is 4 GB, try starting out by requesting 8 GB of memory, then 12 GB, 16 GB, etc. Do my pipelines keep large amounts of data in memory? If yes, you may need to request even more memory than above. Example: Without careful programming, MATLAB code will often make and retain copies of data until finished. How long should my job take? Example: If my laptop is able to run the code on one data file in 2 hours, it will take about that long on Cheaha. Example: Requesting 50 hours of time for a 15 hour process will lengthen the queue time. Don't request too little! Include a buffer to account for scheduler and network issues. How is the software I'm using programmed? Can it use a GPU? Request one. Can it use multiple cores? Request more than one core. Is it single-threaded? Request only one core. Does it use MPI? Request multiple nodes. Be sure to check all of the flags, configuration, and options for the software, or these changes may not work. Which partition is most appropriate? More than 40 GB memory and queue wait times are long? Try largemem* . Need a GPU? Use pascalnodes* or amperenodes* . Software works with AMD? Try amd-hdr100 . Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. We are happy to help guide you to an efficient usage of the cluster. Use seff to verify that your code is as efficient as possible.","title":"Estimating Compute Resources"},{"location":"cheaha/job_efficiency/#verifying-job-efficiency","text":"It's important to evaluate the efficiency of your job in terms of resource usage after it completes. Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. As well, because each researcher has a maximum amount of resources they can use at a given time, having inefficient jobs can increase analysis runtime across many jobs, and increase queue wait times. In order to look at job efficieny, use the seff command. seff <jobid> The output will look like: The job had poor CPU efficiency, requesting 2 CPUs which were only busy for 30% of runtime. Requesting only a single core may have made more sense here. The job also had poor memory efficiency, using less than 1 GB total memory of the requested 16 GB (5.73%). For subsequent jobs using a similar analysis and dataset size, decreasing the requested memory to about 1200 MB and a single CPU would be more efficient, and get the job queued faster. Tip Aim for between 75% and 90% memory efficiency. Lower than that is a waste of resources, but too close to 100% could result in job failure due to an unexpected out-of-memory issue.","title":"Verifying Job Efficiency"},{"location":"cheaha/open_ondemand/ood_files/","text":"File Browser \u00b6 OOD provides an web-based file browser for your files on Cheaha. In order to access it, in the toolbar click Files --> <dir> , where <dir> is one of the choices of your $HOME , $USER_SCRATCH , or $USER_DATA directories. This will open up the following page: This page has a few parts to it: Home directory (red): A link to your $HOME directory and all of its subdirectories for easier navigation. This is always $HOME even if you chose $USER_DATA or $USER_SCRATCH to open. Working directory (green): The absolute path for the current directory you are in along with hyperlinks to the parent directories for easier navigation. File List (black): A list of all file and folders in the working directory along with select information. Hidden files and file permissions can be shown using the radio button in Command Bar at the top right (orange). File Command Menu (blue): A list of commands to perform to a file or directory. Danger Be careful deleting files here. They will be gone forever! OOD Command Menu (orange): A list of commands for navigating in the file browser, file or folder creation, and opening a terminal. OOD Command Menu \u00b6 Uploading Data \u00b6 Data can be uploaded from your local machine using this interface. Use the Upload button in the OOD Command Menu at the top right to select files from your local browser. Additionally, you can drag and drop files from your machine into the File List window as well. This should be limited to small files only. For large files or datasets, please use Globus instead. Opening a Terminal \u00b6 You can also open a bash terminal in the current directory using the >_Open in Terminal command. This should only be used for small tasks when fine-grain researcher control is necessary because the terminal is running on the login node. For compute-intensive tasks, either request an interactive session in the terminal or request an HPC Desktop session through the Interactive Apps and use the terminal there.","title":"File Browser"},{"location":"cheaha/open_ondemand/ood_files/#file-browser","text":"OOD provides an web-based file browser for your files on Cheaha. In order to access it, in the toolbar click Files --> <dir> , where <dir> is one of the choices of your $HOME , $USER_SCRATCH , or $USER_DATA directories. This will open up the following page: This page has a few parts to it: Home directory (red): A link to your $HOME directory and all of its subdirectories for easier navigation. This is always $HOME even if you chose $USER_DATA or $USER_SCRATCH to open. Working directory (green): The absolute path for the current directory you are in along with hyperlinks to the parent directories for easier navigation. File List (black): A list of all file and folders in the working directory along with select information. Hidden files and file permissions can be shown using the radio button in Command Bar at the top right (orange). File Command Menu (blue): A list of commands to perform to a file or directory. Danger Be careful deleting files here. They will be gone forever! OOD Command Menu (orange): A list of commands for navigating in the file browser, file or folder creation, and opening a terminal.","title":"File Browser"},{"location":"cheaha/open_ondemand/ood_files/#ood-command-menu","text":"","title":"OOD Command Menu"},{"location":"cheaha/open_ondemand/ood_files/#uploading-data","text":"Data can be uploaded from your local machine using this interface. Use the Upload button in the OOD Command Menu at the top right to select files from your local browser. Additionally, you can drag and drop files from your machine into the File List window as well. This should be limited to small files only. For large files or datasets, please use Globus instead.","title":"Uploading Data"},{"location":"cheaha/open_ondemand/ood_files/#opening-a-terminal","text":"You can also open a bash terminal in the current directory using the >_Open in Terminal command. This should only be used for small tasks when fine-grain researcher control is necessary because the terminal is running on the login node. For compute-intensive tasks, either request an interactive session in the terminal or request an HPC Desktop session through the Interactive Apps and use the terminal there.","title":"Opening a Terminal"},{"location":"cheaha/open_ondemand/ood_interactive/","text":"Interactive Apps \u00b6 The Interactive Apps dropdown from the toolbar will list a few standalone programs you are able to launch directly from the browser as well as an HPC Desktop that will allow you access all of the other software on Cheaha. Currently, the available standalone programs are IGV, Matlab, RStudio, SAS, and Jupyter. All of the interactive apps have similar setup pages. For instance, if we click HPC Desktop, the following screen will appear: This will allow to choose the number of hours, partition, number of cpus, and memory per cpu needed for the job. These fields are common to all interactive apps and are required. Not all partitions are available when creating an interactive job in OOD. For instance, if you need to use the largemem partition, request those resources in a terminal session for an interactive job or submit a batch job. Tip You can decrease wait time in the queue by choosing resources carefully. The closer your request is to actual usage, the more optimal your wait time will be. Please see our section on Job Efficiency for more information. Once you've selected the compute resources you need, Launch the job. This will bring you to the My Interactive Sessions page, and read on. My Interactive Sessions \u00b6 The My Interactive Sessions page looks like: For each job running via Open OnDemand, there will be a card listed on this page. Each card has basic information about the number of cores, nodes, and time remaining for the job. Also shown are the Job ID in the top-left part of the job card, and a Session ID link near the center of the card. The Job ID and Session ID are important for diagnosing issues you may encounter on Cheaha while using Open OnDemand. The Session ID link points to a folder in your home directory with information we may ask for. Click the Launch Desktop in new tab button to open your interactive VNC session. You may have to wait for the card to change from blue to green to launch the job. Bug If your job fails to launch, please see our FAQ for possible solutions, or contact us . Note For HPC Desktop, you do not need to request resources after you open the Desktop. You are already on a compute node. Any tasks you run will use the resources you requested when initializing the job. Note You can request another interactive session in a terminal in HPC Desktop. Only the terminal you requested the other interactive session in will have access to the new resources. Everything else in the HPC Desktop will run with the resources you requested when creating the initial job. These interactive jobs can be stopped early by clicking Delete on the right side of the job card. Standalone Programs \u00b6 As shown earlier, some software can be run outside of the VNC session. Setup for most of these follow the same rules as creation of an HPC Desktop job in terms of requesting resources. You will also need to select the version of software to use for the job. Note Versions shown in the OOD form may not line up with versions available in modules. If you need a version not available in OOD, please feel free to send in a ticket . RStudio Server \u00b6 RStudio is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. You'll also need to select both the version of RStudio you wish to use, and the version of R you wish to use. To adjust the environment, please use the Environment Setup field to load modules besides R and RStudio as seen below. All other modules and paths should be loaded here as it is difficult to load and consistently use modules once RStudio starts. Important Unless an older version of R is absolutely necessary, it is highly suggested to always use the newest version of R and RStudio for both updated functionality within those software as well as updated compilers for package installation. Using the newest version of R solves most known package installation errors. RStudio and Python \u00b6 If you have a workflow that uses both R and Python, it is strongly recommended to use the reticulate package along with Anaconda environments. Reticulate allows researchers to load Python packages into a native R session as objects. For instance, if someone prefer some functionality of the pandas package but has other code already written in R, they can import pandas to R and use both simultaneously. This also allows researchers to download precompiled command line binaries into an Anaconda environment and easliy use them in their R scripts. For setup, use the following steps: In a terminal on a compute node, either in an HPC Desktop job or by clicking the blue Host button on any job card: Load the Anaconda3 module Create an Anaconda environment. More information about how to create Anaconda environments can be found in our documentation . Activate your environment and install your requuired python packages using either pip install or conda install depending on the package source. Note The preceding steps should only need to be run once. If other Python packages need to be installed in the same environment, repeat steps 1 and 3. You will not need to recreate your environment. In RStudio: Add the command module load Anaconda3 to the Environment Setup window when requesting the RStudio job. If not already installed, install the reticulate package using either install.packages or the renv package. Use reticulate::use_condaenv('env_name') to load your conda environment. From here, you will be able to interact with all of the python packages and non-python precompiled binaries in your Anaconda environment using R and RStudio. Please read more about how to do that in reticulate's documentation . For cases where your R code only needs access to precompiled binaries or libraries and does not need to import any Python libraries, you can instead create your Anaconda environment and add the following lines into the Environment Setup window: module load Anaconda3 conda activate <env_name> This will add those binaries and libraries to your environment $PATH which RStudio will inherit. Important If you're wanting to directly use any Python package in R, DO NOT include the conda activate command in the Environment Setup. Use reticulate instead as described above. RStudio Projects and renv \u00b6 The most recent versions of RStudio installed on Cheaha support R Projects as well as package management through the renv package. Please read more about improving analysis reproducibility using both of these tools in our workflow solutions Using Pandoc and knitr within RStudio \u00b6 If you want to use RMarkdown to create reports in RStudio, R modules using version 4.2.0 and later include knitr compatibility. Please use the latest versions of both R and Rstudio for fully integrated knitr functionality. Starting With a Clean Session to Avoid Errors \u00b6 By default, RStudio loads the most recently opened project at startup and restores the .RData file into the workspace. If you only work on a single project, this may be helpful. If you frequently change projects then these default settings can create difficult-to-diagnose errors, or you may inadvertently alter a project by adding incorrect packages, for example. To reduce the risk of these kinds of errors, uncheck the highlighted fields below in the RStudio Options menu under the \"General\" selection. Restore most recently opened project at startup Restore .RData into workspace at startup Jupyter Notebook \u00b6 Jupyter Notebooks are available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. Jupyter Notebooks are commonly used with Anaconda environments. If you are unfamiliar with Anaconda environments please see the Working with Anaconda Environments section below before continuing here. To modify the Operating System (OS) environment that Anaconda and Jupyter will run in, please use the Environment Setup field to load modules. For GPU applications you'll need to load a CUDA/* module. If working with deep learning workflows, you will also possibly need to load the cuDNN/*-CUDA-* module corresponding to your choice of CUDA/* module version. These are required for popular ML/DL/AI libraries like TensorFlow, Keras, and PyTorch. Use module spider cuda and module spider cudnn to view the list of appropriate modules. An example of what to put in the Environment Setup field, when using Tensorflow in a Jupyter notebook, is shown below. # ENVIRONMENT SETUP module load CUDA/12.2.0 module load cuDNN/8.9.2.26-CUDA-12.2.0 For information on which versions of CUDA to load for Tensorflow and PyTorch, please see Tensorflow Compatibility and PyTorch Compatibility . Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. Important If you are not able to see your environment, you may need to install the ipykernel package. It is required for Jupyter to recognize your environment. See Packages for Jupyter for more information. Important Do not load module load Anaconda3 in the Environment Setup field, as it is loaded automatically. Loading any versions of Anaconda3 would affect the Python executable, which is used by default. These results in hard-to-diagnose errors in the OOD Jupyter notebook. Warning Having conda/mamba activate and source activate statements in the Environment Setup field can cause unexpected and silent job failure. Avoid using conda activate in the Environment Setup field. Working with Anaconda Environments \u00b6 For information on working with Anaconda environments please see our Using Anaconda page . The please review our Cheaha-specific Anaconda page for important tips and how to avoid common pitfalls. Extra Jupyter Arguments \u00b6 The Extra Jupyter Arguments field allows you to pass additional arguments to the Jupyter Server as it is being started. It can be helpful to point the server to the folder containing your notebook. To do this, assuming your notebooks are stored in /data/user/$USER , also known as $USER_DATA , put --notebook-dir=$USER_DATA in this field. You will be able to navigate to the notebook if it is in a subdirectory of notebook-dir , but you won't be able to navigate to any other directories. An example is shown below. Submitting the Jupyter Notebook Job \u00b6 Submitting the job will bring you to the My Interactive Jobs window while the Jupyter job is initialized. Click Connect to Jupyter to open the Jupyter Home Page. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. The Jupyter Server Home Page \u00b6 The Jupyter Server Home Page will look like the following From here, you can navigate to and select an existing notebook, or you can create a new one using one of your existing virtual environments or the base environment. Once inside a Jupyter notebook, you can use the Kernel --> Change kernel menu to select your preferred Anaconda environment. Important See Anaconda Environments for information on Jupyter related packages. Warning Files deleted within using the Jupyter server interface are not truly deleted. Instead they are moved to $HOME/.local/share/Trash . This may cause inflation of personal storage usage on Cheaha. Python Libraries and Virtual Environments \u00b6 To run Jupyter with specific libraries and packages outside of the base install, you will need to create a virtual environment first. You can do this either in an HPC Desktop job or in the Conda tab of the Jupyter homepage. The Conda has the following layout: Current environments (red): a listing of the current existing environments in your $HOME/.conda/envs folder. Available packages (green): a list of all packages available to install from conda sources. Installed packages (blue): a list of the packages installed in the currently selected environment. To create a new environment, click the + button at the top of the Current environments pane and enter the name of the environment. After it has been created, you can select packages to install by searching for the package name at the top right of the Available packages pane. After selecting the package, click the -> button, and the package and all its dependencies will be installed. Note If a package is not available using the conda command directly, it will not be listed as an available package. Use a terminal window to install the package as necessary. Note In order to use an environment with Jupyter, the ipykernel library is necessary. Creating an environment in the Conda tab will autoinstall this library. If using the terminal, use conda install ipykernel to install it. After successfully creating your environment, navigate to the Files tab. You can create a new notebook using the New dropdown menu in the top right. Select your virtual environment of choice, and a notebook will be created and opened. Help GPU is not Available with TensorFlow or PyTorch \u00b6 If you are using Jupyter with TensorFlow or PyTorch and no GPU is found, please see our Slurm GPU page sections on TensorFlow Compatibility and PyTorch Compatibility . For MATLAB, please see MATLAB Compatibility . Matlab \u00b6 Matlab is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. Warning Matlab tends to consume substantial memory at startup. You may experience difficulty with job errors below 20 GB of total memory. Using Anaconda Python from within Matlab \u00b6 Matlab has the ability to interoperate with Python from within Matlab. The official documentation for this featuer may be found at https://www.mathworks.com/help/matlab/call-python-libraries.html . This section is dedicated to using this feature with Anaconda on Cheaha. To use Python contained in an Anaconda Environment within Matlab, please use the following steps. Create an HPC Interactive Desktop Job using Open OnDemand . Open a terminal in that job. The following steps should all be run in this terminal unless otherwise specified. Load the Anaconda Module . Create an Environment in Anaconda with the packages needed. Activate the Environment , Load the Matlab Module . Start Matlab by entering the command matlab . Verify success by entering pyenv at the Matlab prompt (not the terminal window). Multiple lines of text will be returned at the prompt. Among them you should see a line like the following, with your environment name in place of <env_name> . Executable: /home/$USER/.conda/envs/<env_name>/bin/python You may optionally verify that Python works correctly by entering py.list([\"hello\", \"world\"]) . A python list object should appear in the workspace. Using a GPU with MATLAB \u00b6 Please see the MATLAB Section on our GPU Page .","title":"Interactive Apps"},{"location":"cheaha/open_ondemand/ood_interactive/#interactive-apps","text":"The Interactive Apps dropdown from the toolbar will list a few standalone programs you are able to launch directly from the browser as well as an HPC Desktop that will allow you access all of the other software on Cheaha. Currently, the available standalone programs are IGV, Matlab, RStudio, SAS, and Jupyter. All of the interactive apps have similar setup pages. For instance, if we click HPC Desktop, the following screen will appear: This will allow to choose the number of hours, partition, number of cpus, and memory per cpu needed for the job. These fields are common to all interactive apps and are required. Not all partitions are available when creating an interactive job in OOD. For instance, if you need to use the largemem partition, request those resources in a terminal session for an interactive job or submit a batch job. Tip You can decrease wait time in the queue by choosing resources carefully. The closer your request is to actual usage, the more optimal your wait time will be. Please see our section on Job Efficiency for more information. Once you've selected the compute resources you need, Launch the job. This will bring you to the My Interactive Sessions page, and read on.","title":"Interactive Apps"},{"location":"cheaha/open_ondemand/ood_interactive/#my-interactive-sessions","text":"The My Interactive Sessions page looks like: For each job running via Open OnDemand, there will be a card listed on this page. Each card has basic information about the number of cores, nodes, and time remaining for the job. Also shown are the Job ID in the top-left part of the job card, and a Session ID link near the center of the card. The Job ID and Session ID are important for diagnosing issues you may encounter on Cheaha while using Open OnDemand. The Session ID link points to a folder in your home directory with information we may ask for. Click the Launch Desktop in new tab button to open your interactive VNC session. You may have to wait for the card to change from blue to green to launch the job. Bug If your job fails to launch, please see our FAQ for possible solutions, or contact us . Note For HPC Desktop, you do not need to request resources after you open the Desktop. You are already on a compute node. Any tasks you run will use the resources you requested when initializing the job. Note You can request another interactive session in a terminal in HPC Desktop. Only the terminal you requested the other interactive session in will have access to the new resources. Everything else in the HPC Desktop will run with the resources you requested when creating the initial job. These interactive jobs can be stopped early by clicking Delete on the right side of the job card.","title":"My Interactive Sessions"},{"location":"cheaha/open_ondemand/ood_interactive/#standalone-programs","text":"As shown earlier, some software can be run outside of the VNC session. Setup for most of these follow the same rules as creation of an HPC Desktop job in terms of requesting resources. You will also need to select the version of software to use for the job. Note Versions shown in the OOD form may not line up with versions available in modules. If you need a version not available in OOD, please feel free to send in a ticket .","title":"Standalone Programs"},{"location":"cheaha/open_ondemand/ood_interactive/#rstudio-server","text":"RStudio is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. You'll also need to select both the version of RStudio you wish to use, and the version of R you wish to use. To adjust the environment, please use the Environment Setup field to load modules besides R and RStudio as seen below. All other modules and paths should be loaded here as it is difficult to load and consistently use modules once RStudio starts. Important Unless an older version of R is absolutely necessary, it is highly suggested to always use the newest version of R and RStudio for both updated functionality within those software as well as updated compilers for package installation. Using the newest version of R solves most known package installation errors.","title":"RStudio Server"},{"location":"cheaha/open_ondemand/ood_interactive/#rstudio-and-python","text":"If you have a workflow that uses both R and Python, it is strongly recommended to use the reticulate package along with Anaconda environments. Reticulate allows researchers to load Python packages into a native R session as objects. For instance, if someone prefer some functionality of the pandas package but has other code already written in R, they can import pandas to R and use both simultaneously. This also allows researchers to download precompiled command line binaries into an Anaconda environment and easliy use them in their R scripts. For setup, use the following steps: In a terminal on a compute node, either in an HPC Desktop job or by clicking the blue Host button on any job card: Load the Anaconda3 module Create an Anaconda environment. More information about how to create Anaconda environments can be found in our documentation . Activate your environment and install your requuired python packages using either pip install or conda install depending on the package source. Note The preceding steps should only need to be run once. If other Python packages need to be installed in the same environment, repeat steps 1 and 3. You will not need to recreate your environment. In RStudio: Add the command module load Anaconda3 to the Environment Setup window when requesting the RStudio job. If not already installed, install the reticulate package using either install.packages or the renv package. Use reticulate::use_condaenv('env_name') to load your conda environment. From here, you will be able to interact with all of the python packages and non-python precompiled binaries in your Anaconda environment using R and RStudio. Please read more about how to do that in reticulate's documentation . For cases where your R code only needs access to precompiled binaries or libraries and does not need to import any Python libraries, you can instead create your Anaconda environment and add the following lines into the Environment Setup window: module load Anaconda3 conda activate <env_name> This will add those binaries and libraries to your environment $PATH which RStudio will inherit. Important If you're wanting to directly use any Python package in R, DO NOT include the conda activate command in the Environment Setup. Use reticulate instead as described above.","title":"RStudio and Python"},{"location":"cheaha/open_ondemand/ood_interactive/#rstudio-projects-and-renv","text":"The most recent versions of RStudio installed on Cheaha support R Projects as well as package management through the renv package. Please read more about improving analysis reproducibility using both of these tools in our workflow solutions","title":"RStudio Projects and renv"},{"location":"cheaha/open_ondemand/ood_interactive/#using-pandoc-and-knitr-within-rstudio","text":"If you want to use RMarkdown to create reports in RStudio, R modules using version 4.2.0 and later include knitr compatibility. Please use the latest versions of both R and Rstudio for fully integrated knitr functionality.","title":"Using Pandoc and knitr within RStudio"},{"location":"cheaha/open_ondemand/ood_interactive/#starting-with-a-clean-session-to-avoid-errors","text":"By default, RStudio loads the most recently opened project at startup and restores the .RData file into the workspace. If you only work on a single project, this may be helpful. If you frequently change projects then these default settings can create difficult-to-diagnose errors, or you may inadvertently alter a project by adding incorrect packages, for example. To reduce the risk of these kinds of errors, uncheck the highlighted fields below in the RStudio Options menu under the \"General\" selection. Restore most recently opened project at startup Restore .RData into workspace at startup","title":"Starting With a Clean Session to Avoid Errors"},{"location":"cheaha/open_ondemand/ood_interactive/#jupyter-notebook","text":"Jupyter Notebooks are available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. Jupyter Notebooks are commonly used with Anaconda environments. If you are unfamiliar with Anaconda environments please see the Working with Anaconda Environments section below before continuing here. To modify the Operating System (OS) environment that Anaconda and Jupyter will run in, please use the Environment Setup field to load modules. For GPU applications you'll need to load a CUDA/* module. If working with deep learning workflows, you will also possibly need to load the cuDNN/*-CUDA-* module corresponding to your choice of CUDA/* module version. These are required for popular ML/DL/AI libraries like TensorFlow, Keras, and PyTorch. Use module spider cuda and module spider cudnn to view the list of appropriate modules. An example of what to put in the Environment Setup field, when using Tensorflow in a Jupyter notebook, is shown below. # ENVIRONMENT SETUP module load CUDA/12.2.0 module load cuDNN/8.9.2.26-CUDA-12.2.0 For information on which versions of CUDA to load for Tensorflow and PyTorch, please see Tensorflow Compatibility and PyTorch Compatibility . Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running. Important If you are not able to see your environment, you may need to install the ipykernel package. It is required for Jupyter to recognize your environment. See Packages for Jupyter for more information. Important Do not load module load Anaconda3 in the Environment Setup field, as it is loaded automatically. Loading any versions of Anaconda3 would affect the Python executable, which is used by default. These results in hard-to-diagnose errors in the OOD Jupyter notebook. Warning Having conda/mamba activate and source activate statements in the Environment Setup field can cause unexpected and silent job failure. Avoid using conda activate in the Environment Setup field.","title":"Jupyter Notebook"},{"location":"cheaha/open_ondemand/ood_interactive/#working-with-anaconda-environments","text":"For information on working with Anaconda environments please see our Using Anaconda page . The please review our Cheaha-specific Anaconda page for important tips and how to avoid common pitfalls.","title":"Working with Anaconda Environments"},{"location":"cheaha/open_ondemand/ood_interactive/#extra-jupyter-arguments","text":"The Extra Jupyter Arguments field allows you to pass additional arguments to the Jupyter Server as it is being started. It can be helpful to point the server to the folder containing your notebook. To do this, assuming your notebooks are stored in /data/user/$USER , also known as $USER_DATA , put --notebook-dir=$USER_DATA in this field. You will be able to navigate to the notebook if it is in a subdirectory of notebook-dir , but you won't be able to navigate to any other directories. An example is shown below.","title":"Extra Jupyter Arguments"},{"location":"cheaha/open_ondemand/ood_interactive/#submitting-the-jupyter-notebook-job","text":"Submitting the job will bring you to the My Interactive Jobs window while the Jupyter job is initialized. Click Connect to Jupyter to open the Jupyter Home Page. Note If you get a Failed to Connect message when opening the job, close the tab and wait a couple of minutes. Jupyter is still initializing and takes some time after the job first begins running.","title":"Submitting the Jupyter Notebook Job"},{"location":"cheaha/open_ondemand/ood_interactive/#the-jupyter-server-home-page","text":"The Jupyter Server Home Page will look like the following From here, you can navigate to and select an existing notebook, or you can create a new one using one of your existing virtual environments or the base environment. Once inside a Jupyter notebook, you can use the Kernel --> Change kernel menu to select your preferred Anaconda environment. Important See Anaconda Environments for information on Jupyter related packages. Warning Files deleted within using the Jupyter server interface are not truly deleted. Instead they are moved to $HOME/.local/share/Trash . This may cause inflation of personal storage usage on Cheaha.","title":"The Jupyter Server Home Page"},{"location":"cheaha/open_ondemand/ood_interactive/#python-libraries-and-virtual-environments","text":"To run Jupyter with specific libraries and packages outside of the base install, you will need to create a virtual environment first. You can do this either in an HPC Desktop job or in the Conda tab of the Jupyter homepage. The Conda has the following layout: Current environments (red): a listing of the current existing environments in your $HOME/.conda/envs folder. Available packages (green): a list of all packages available to install from conda sources. Installed packages (blue): a list of the packages installed in the currently selected environment. To create a new environment, click the + button at the top of the Current environments pane and enter the name of the environment. After it has been created, you can select packages to install by searching for the package name at the top right of the Available packages pane. After selecting the package, click the -> button, and the package and all its dependencies will be installed. Note If a package is not available using the conda command directly, it will not be listed as an available package. Use a terminal window to install the package as necessary. Note In order to use an environment with Jupyter, the ipykernel library is necessary. Creating an environment in the Conda tab will autoinstall this library. If using the terminal, use conda install ipykernel to install it. After successfully creating your environment, navigate to the Files tab. You can create a new notebook using the New dropdown menu in the top right. Select your virtual environment of choice, and a notebook will be created and opened.","title":"Python Libraries and Virtual Environments"},{"location":"cheaha/open_ondemand/ood_interactive/#help-gpu-is-not-available-with-tensorflow-or-pytorch","text":"If you are using Jupyter with TensorFlow or PyTorch and no GPU is found, please see our Slurm GPU page sections on TensorFlow Compatibility and PyTorch Compatibility . For MATLAB, please see MATLAB Compatibility .","title":"Help GPU is not Available with TensorFlow or PyTorch"},{"location":"cheaha/open_ondemand/ood_interactive/#matlab","text":"Matlab is available for use graphically in your browser via OOD. As with other standalone programs, you'll need to select the resources required using the job creation form. The form is shown below. Warning Matlab tends to consume substantial memory at startup. You may experience difficulty with job errors below 20 GB of total memory.","title":"Matlab"},{"location":"cheaha/open_ondemand/ood_interactive/#using-anaconda-python-from-within-matlab","text":"Matlab has the ability to interoperate with Python from within Matlab. The official documentation for this featuer may be found at https://www.mathworks.com/help/matlab/call-python-libraries.html . This section is dedicated to using this feature with Anaconda on Cheaha. To use Python contained in an Anaconda Environment within Matlab, please use the following steps. Create an HPC Interactive Desktop Job using Open OnDemand . Open a terminal in that job. The following steps should all be run in this terminal unless otherwise specified. Load the Anaconda Module . Create an Environment in Anaconda with the packages needed. Activate the Environment , Load the Matlab Module . Start Matlab by entering the command matlab . Verify success by entering pyenv at the Matlab prompt (not the terminal window). Multiple lines of text will be returned at the prompt. Among them you should see a line like the following, with your environment name in place of <env_name> . Executable: /home/$USER/.conda/envs/<env_name>/bin/python You may optionally verify that Python works correctly by entering py.list([\"hello\", \"world\"]) . A python list object should appear in the workspace.","title":"Using Anaconda Python from within Matlab"},{"location":"cheaha/open_ondemand/ood_interactive/#using-a-gpu-with-matlab","text":"Please see the MATLAB Section on our GPU Page .","title":"Using a GPU with MATLAB"},{"location":"cheaha/open_ondemand/ood_jobs/","text":"Job Viewer and Composer \u00b6 Using the Jobs dropdown menu in the toolbar, you can view the status of your current submitted jobs and how long they have been running as well as submit new jobs via the job composer. View Current Jobs \u00b6 Click Jobs > Active Jobs . This will open a new window with you current active jobs that looks like: There are fields such as job ID, job name, time spent active, and the queue or partition. You can sort your job list by any of these fields and can filter for specific jobs using the Filter option at the top right. Additionally, you can view more detailed information about a job by clicking the arrow to the left of the job ID. The resulting table will look like: This table shows extra information such as the total number of nodes, CPUs, time limit, and memory requested for the job. The path to the output file for the job created by Slurm will also be listed. You can open the file location in a file browser or in a terminal here. You can end a currently running job by pressing Delete at the bottom right of these expanded job details. Job Composer \u00b6 The job composer allows you to create Slurm jobs directly in the web interface without having to create a VNC session. Clicking Jobs > Job Composer will bring up a new window: To create a job: Select New Job > From Default Template . This will bring up the job details window pane. Click Open Editor at the bottom of the Job Details. This will open an editable document in another browser window. Paste or type in your script directives including SBATCH options. Once done, click Save in the top left of the editor and close the tab. In the main job window, click Submit , and your job will be submitted to the scheduler. View its status on the Active Jobs page.","title":"Job Composer"},{"location":"cheaha/open_ondemand/ood_jobs/#job-viewer-and-composer","text":"Using the Jobs dropdown menu in the toolbar, you can view the status of your current submitted jobs and how long they have been running as well as submit new jobs via the job composer.","title":"Job Viewer and Composer"},{"location":"cheaha/open_ondemand/ood_jobs/#view-current-jobs","text":"Click Jobs > Active Jobs . This will open a new window with you current active jobs that looks like: There are fields such as job ID, job name, time spent active, and the queue or partition. You can sort your job list by any of these fields and can filter for specific jobs using the Filter option at the top right. Additionally, you can view more detailed information about a job by clicking the arrow to the left of the job ID. The resulting table will look like: This table shows extra information such as the total number of nodes, CPUs, time limit, and memory requested for the job. The path to the output file for the job created by Slurm will also be listed. You can open the file location in a file browser or in a terminal here. You can end a currently running job by pressing Delete at the bottom right of these expanded job details.","title":"View Current Jobs"},{"location":"cheaha/open_ondemand/ood_jobs/#job-composer","text":"The job composer allows you to create Slurm jobs directly in the web interface without having to create a VNC session. Clicking Jobs > Job Composer will bring up a new window: To create a job: Select New Job > From Default Template . This will bring up the job details window pane. Click Open Editor at the bottom of the Job Details. This will open an editable document in another browser window. Paste or type in your script directives including SBATCH options. Once done, click Save in the top left of the editor and close the tab. In the main job window, click Submit , and your job will be submitted to the scheduler. View its status on the Active Jobs page.","title":"Job Composer"},{"location":"cheaha/open_ondemand/ood_main/","text":"Homepage \u00b6 Navigating to rc.uab.edu will take you to the OOD homepage: You will find system-wide messages from admins at the top of the page (red outline). These will always include links to the Research Computing Office Hours on Zoom. This will also be the place to see information about ongoing maintenance. In the middle of the page (green outline), you will see a Message of the Day containing the email address for support if you are having any issues with Cheaha. There are also links to our Acceptable Use Policy as well as links to our documentation. Lastly, there is a table with a list of available Slurm partitions on Cheaha with their max runtime and number of compute nodes per job as well as their priority. Use this table to plan job requests based on your needed computational resources. Toolbar \u00b6 To access all of the features OOD has to offer, use the toolbar at the top of the page that looks like: In it, you will find options to: Directly access your files on Cheaha View currently running jobs Interface with Cheaha via a shell terminal Request interactive sessions To use a shell terminal in Cheaha through OOD, click Clusters >> >_Cheaha Shell Access . You can use this exactly like a standard ssh tunnel. Warning Using the shell terminal in this way puts you on the login node. Do not run any compute tasks on the login node. Request a compute node first!","title":"Homepage"},{"location":"cheaha/open_ondemand/ood_main/#homepage","text":"Navigating to rc.uab.edu will take you to the OOD homepage: You will find system-wide messages from admins at the top of the page (red outline). These will always include links to the Research Computing Office Hours on Zoom. This will also be the place to see information about ongoing maintenance. In the middle of the page (green outline), you will see a Message of the Day containing the email address for support if you are having any issues with Cheaha. There are also links to our Acceptable Use Policy as well as links to our documentation. Lastly, there is a table with a list of available Slurm partitions on Cheaha with their max runtime and number of compute nodes per job as well as their priority. Use this table to plan job requests based on your needed computational resources.","title":"Homepage"},{"location":"cheaha/open_ondemand/ood_main/#toolbar","text":"To access all of the features OOD has to offer, use the toolbar at the top of the page that looks like: In it, you will find options to: Directly access your files on Cheaha View currently running jobs Interface with Cheaha via a shell terminal Request interactive sessions To use a shell terminal in Cheaha through OOD, click Clusters >> >_Cheaha Shell Access . You can use this exactly like a standard ssh tunnel. Warning Using the shell terminal in this way puts you on the login node. Do not run any compute tasks on the login node. Request a compute node first!","title":"Toolbar"},{"location":"cheaha/slurm/gpu/","text":"GPUs \u00b6 Available Devices \u00b6 Cheaha has GPUs available with the following statistics, broken out by Slurm Partition . For more information on all available partitions, see our Hardware Summary . pascalnodes amperenodes Product Name P100 A100 80GB Architecture Pascal Ampere CUDA Compute Capability Version 6.0 8.0 CUDA Cores 3584 6912 Memory (GB) 16 80 Memory Bandwidth (GB/s) 720 2039 NVLink Bandwidth (GB/s) 160 600 FP32 performance (TFLOPs) 10.6 19.5 For more information on these nodes, see Detailed Hardware Information . Scheduling GPUs \u00b6 To submit a job with one or more GPUs, you will need to set the partition to pascalnodes or amperenodes family of partitions for P100 GPUs or amperenodes family for A100 GPUs. When requesting a job using sbatch , you will need to include the Slurm flag --gres=gpu:# . Replace # with the number of GPUs you need. Quotas and constraints are available on our Hardware Summary Note It is suggested that at least 2 CPUs are requested for every GPU to begin with. The user should monitor and adjust the number of cores on subsequent job submissions if necessary. Look at Managing Jobs for more information. Ensuring IO Performance With A100 GPUs \u00b6 If you are using amperenodes and the A100 GPUs, then it is highly recommended to move your input files to /local/$SLURM_JOB_ID prior to running your workflow, to ensure adequate GPU performance. Using $USER_SCRATCH , or other network file locations, will starve the GPU of data, resulting in poor performance. The following script can be used to wrap your existing workflows. It will automatically create a temporary directory $TMPDIR and delete it when your workflow is finished. You'll need to supply the original source of your data as $MY_DATA_DIR . The script is not guaranteed to delete the temporary directory if the job ends before it reaches the final line, so please be mindful and periodically check for any extra temporary directories and delete them as needed. #!/bin/bash #SBATCH ... #SBATCH --partition=amperenodes #SBATCH --gres=gpu:1 # LOAD CUDA MODULES module load CUDA/12.1.1 module load cuDNN/12.1.1 # CREATE TEMPORARY DIRECTORY # WARNING! $TMPDIR will be deleted at the end of the script! # Changing the following line can cause permanent, unintended deletion of important data. TMPDIR = \"/local/ $USER / $SLURM_JOB_ID \" mkdir -p \" $TMPDIR \" # COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY # Replace $MY_DATA_DIR with the path to your data folder cp -r \" $MY_DATA_DIR \" \" $TMPDIR \" # YOUR ORIGINAL WORKFLOW GOES HERE # be sure to load files from \"$TMPDIR\"! # CLEAN UP TEMPORARY DIRECTORY # WARNING! # Changing the following line can cause permanent, unintended deletion of important data. rm -rf \" $TMPDIR \" Open OnDemand \u00b6 When requesting an interactive job through Open OnDemand , selecting the pascalnodes partitions will automatically request access to one GPU as well. There is currently no way to change the number of GPUs for OOD interactive jobs. MATLAB \u00b6 To use GPUs with our Open OnDemand MATLAB app, you may need to take a slightly different route than usual. If you are using MATLAB R2022a or newer, then our pascalnodes P100 GPUs and amperenodes A100 GPUs should work without any additional steps. If you are using R2021b and earlier, then follow the instructions below. Start an HPC Interactive Desktop Job with appropriate resources. Be sure to use one of the pascalnodes* Partitions . Open a terminal. Load the appropriate CUDA Module . Determine which CUDA Modules are compatible with your required version of MATLAB using the table at the MathWorks Site . Check the Pascal (cc6.x) column for the pascalnodes P100 GPUs and Ampere (cc8.x) column for the amperenodes A100 GPUs. As of September, 2023, module load CUDA/11.6.0 and newer should work fine with any version of MATLAB R2021b or older, with possible caveats for some functions. Load the appropriate MATLAB Module . Start MATLAB by entering the command matlab . When MATLAB loads, enter the command gpuDevice in the MATLAB Command Window to verify it can identify the GPU. For more information and official MATLAB documentation please see this page: https://www.mathworks.com/help/parallel-computing/gpu-computing-requirements.html . CUDA Modules \u00b6 You will need to load a CUDA module to make use of GPUs on Cheaha. Depending on which version of software you are using, different versions of CUDA module may be required. For instance, tensorflow version 2.13.0 requires the CUDA/11.8.0 module. To see which versions are available on Cheaha, use the following command at the terminal. module -r spider 'CUDA/*' If a specific version of CUDA is needed but not installed, please send an install request to support@listserv.uab.edu . cuDNN Modules \u00b6 If working with deep neural networks (DNNs, CNNs, LSTMs, LLMs, etc.), you will need to load a cuDNN module as well. The cuDNN modules are built to be compatible with a sibling CUDA module and are named with the corresponding version. For example, if you are loading CUDA/12.2.0 , you will also need to load cuDNN/8.9.2.26-CUDA-12.2.0 . Tensorflow Compatibility \u00b6 To check which CUDA Module version is required for your version of Tensorflow, see the toolkit requirements chart here https://www.tensorflow.org/install/source#gpu . PyTorch Compatibility \u00b6 PyTorch does not maintain a simple compatibility table for CUDA versions. Instead, please manually check their \"get started\" page for the latest PyTorch version compatibility, and their \"previous versions\" page for older PyTorch version compatibility. Assume that a CUDA version is not compatible if it is not listed for a specific PyTorch version. To use GPUs prior to PyTorch version 1.13 you must select a cudatoolkit version from the PyTorch channel when you install PyTorch using Anaconda. It is how PyTorch knows to install a GPU compatible flavor, as opposed to the CPU only flavor. See below for templates of CPU and GPU installs for PyTorch versions prior to 1.13. Be sure to check the compatibility links above for your selected version. Note torchaudio is also available for signal processing. CPU Version: conda install pytorch==... torchvision==... -c pytorch GPU Version: conda install pytorch==... torchvision==... cudatoolkit=... -c pytorch For versions of PyTorch 1.13 and newer, use the following template instead. CPU Version: conda install pytorch==... torchvision==... cpuonly -c pytorch GPU Version: conda install pytorch==... torchvision==... pytorch-cuda=... -c pytorch -c nvidia Reviewing GPU Jobs \u00b6 As with all jobs, use sacct to review GPU jobs. Quantity of GPUs may be reviewed using the reqtres and alloctres fields . Frequently Asked Questions (FAQ) About A100 GPUs \u00b6 I've been using the P100 GPUs on pascalnodes up until now, what is the easiest way to start using the A100 GPUs? If you are using an sbatch script... Change --partition=pascalnodes to --partition=amperenodes , or change --partition=pascalnodes-medium to --partition=amperenodes-medium . Also change --gres=gpu:3 and --gres=gpu:4 to --gres=gpu:2 , as there are only two A100 GPUs per node. If you are using an Open OnDemand Interactive App ... Change the partition from \"pascalnodes\" to \"amperenodes, or change \"pascalnodes-medium\" to \"amperenodes-medium\". In all cases, be sure to read the section on Ensuring IO Performance With A100 GPUs to be sure disk read speed doesn't limit your performance gains. How do I access the A100 GPUs? You can access the A100 GPUs by request jobs in the appropriate partitions. Use amperenodes partition for up to 12 hours or amperenodes-medium partition for up to 48 hours. How many GPUs can I request at once? Up to four GPUs may be requested by any one researcher at once. However, there are only two GPUs per node, so requesting four GPUs will allocate two nodes. To make use of multiple nodes, your workflow software must know how to communicate between nodes using software like Horovod or OpenMPI. If you are new to GPUs and aren't sure you need multiple nodes, please limit your request to one or two gpus. What performance improvements can I expect over the P100 GPUs? Performance improvements depend on the software and algorithms being used. Determining optimal configuration will take some experimenting. Swapping a single P100 to a single A100, you can generally expect 3x to 20x improvement. For more information about possible performance improvements, please see the Official NVIDIA A100 page . How can I make the most efficient use of the A100 GPUs? A100s process data very rapidly compared with previous technology. Ideally, we want the A100 to be the bottleneck during processing, rather than CPUs or I/O operations. Here are two initial possibilities to consider for optimizing efficiency: All researchers should copy their input data onto /local/$SLURM_JOB_ID (node-specific NVMe drives) before processing to avoid I/O bottlenecks reducing performance. See Ensuring IO Performance With A100 GPUs . Some researchers may benefit from using a larger number of CPU cores for data loading and preprocessing, compared with pascalnodes . Please consider experimenting with different numbers of CPU cores using the same dataset to find what is optimal for you. If you feel that performance should be higher, please contact Support so we can guide you toward an optimal CPU-to-GPU ratio for your application and workflow. Where are the A100 nodes physically located, and will this impact my workflows? The A100 nodes are located in the DC BLOX Data Center, west of UAB Campus. Because Cheaha storage (GPFS) is located on campus, there may be slightly higher latency when transferring data between the A100 nodes and GPFS. Impacts will only occur if very small amounts of data are transferred very frequently, which is unusual for most GPU workflows. We strongly recommend copying your input data onto /local/$SLURM_JOB_ID prior to processing, see Ensuring IO Performance With A100 GPUs . What will happen to the P100 GPUs? We intend to retain all of the 18 existing P100 GPU nodes, of which 9 nodes are available now. The remaining 9 nodes have been temporarily taken offline as we reconfigure hardware, and will be reallocated based on demand and other factors. What else should I be aware of? Please be sure to clean your data off of /local/$SLURM_JOB_ID as soon as you no longer need it, before the job finishes. We have updated the CUDA and cuDNN modules to improve reliability and ease of use. Please see the section on CUDA Modules for more information.","title":"GPUs"},{"location":"cheaha/slurm/gpu/#gpus","text":"","title":"GPUs"},{"location":"cheaha/slurm/gpu/#available-devices","text":"Cheaha has GPUs available with the following statistics, broken out by Slurm Partition . For more information on all available partitions, see our Hardware Summary . pascalnodes amperenodes Product Name P100 A100 80GB Architecture Pascal Ampere CUDA Compute Capability Version 6.0 8.0 CUDA Cores 3584 6912 Memory (GB) 16 80 Memory Bandwidth (GB/s) 720 2039 NVLink Bandwidth (GB/s) 160 600 FP32 performance (TFLOPs) 10.6 19.5 For more information on these nodes, see Detailed Hardware Information .","title":"Available Devices"},{"location":"cheaha/slurm/gpu/#scheduling-gpus","text":"To submit a job with one or more GPUs, you will need to set the partition to pascalnodes or amperenodes family of partitions for P100 GPUs or amperenodes family for A100 GPUs. When requesting a job using sbatch , you will need to include the Slurm flag --gres=gpu:# . Replace # with the number of GPUs you need. Quotas and constraints are available on our Hardware Summary Note It is suggested that at least 2 CPUs are requested for every GPU to begin with. The user should monitor and adjust the number of cores on subsequent job submissions if necessary. Look at Managing Jobs for more information.","title":"Scheduling GPUs"},{"location":"cheaha/slurm/gpu/#ensuring-io-performance-with-a100-gpus","text":"If you are using amperenodes and the A100 GPUs, then it is highly recommended to move your input files to /local/$SLURM_JOB_ID prior to running your workflow, to ensure adequate GPU performance. Using $USER_SCRATCH , or other network file locations, will starve the GPU of data, resulting in poor performance. The following script can be used to wrap your existing workflows. It will automatically create a temporary directory $TMPDIR and delete it when your workflow is finished. You'll need to supply the original source of your data as $MY_DATA_DIR . The script is not guaranteed to delete the temporary directory if the job ends before it reaches the final line, so please be mindful and periodically check for any extra temporary directories and delete them as needed. #!/bin/bash #SBATCH ... #SBATCH --partition=amperenodes #SBATCH --gres=gpu:1 # LOAD CUDA MODULES module load CUDA/12.1.1 module load cuDNN/12.1.1 # CREATE TEMPORARY DIRECTORY # WARNING! $TMPDIR will be deleted at the end of the script! # Changing the following line can cause permanent, unintended deletion of important data. TMPDIR = \"/local/ $USER / $SLURM_JOB_ID \" mkdir -p \" $TMPDIR \" # COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY # Replace $MY_DATA_DIR with the path to your data folder cp -r \" $MY_DATA_DIR \" \" $TMPDIR \" # YOUR ORIGINAL WORKFLOW GOES HERE # be sure to load files from \"$TMPDIR\"! # CLEAN UP TEMPORARY DIRECTORY # WARNING! # Changing the following line can cause permanent, unintended deletion of important data. rm -rf \" $TMPDIR \"","title":"Ensuring IO Performance With A100 GPUs"},{"location":"cheaha/slurm/gpu/#open-ondemand","text":"When requesting an interactive job through Open OnDemand , selecting the pascalnodes partitions will automatically request access to one GPU as well. There is currently no way to change the number of GPUs for OOD interactive jobs.","title":"Open OnDemand"},{"location":"cheaha/slurm/gpu/#matlab","text":"To use GPUs with our Open OnDemand MATLAB app, you may need to take a slightly different route than usual. If you are using MATLAB R2022a or newer, then our pascalnodes P100 GPUs and amperenodes A100 GPUs should work without any additional steps. If you are using R2021b and earlier, then follow the instructions below. Start an HPC Interactive Desktop Job with appropriate resources. Be sure to use one of the pascalnodes* Partitions . Open a terminal. Load the appropriate CUDA Module . Determine which CUDA Modules are compatible with your required version of MATLAB using the table at the MathWorks Site . Check the Pascal (cc6.x) column for the pascalnodes P100 GPUs and Ampere (cc8.x) column for the amperenodes A100 GPUs. As of September, 2023, module load CUDA/11.6.0 and newer should work fine with any version of MATLAB R2021b or older, with possible caveats for some functions. Load the appropriate MATLAB Module . Start MATLAB by entering the command matlab . When MATLAB loads, enter the command gpuDevice in the MATLAB Command Window to verify it can identify the GPU. For more information and official MATLAB documentation please see this page: https://www.mathworks.com/help/parallel-computing/gpu-computing-requirements.html .","title":"MATLAB"},{"location":"cheaha/slurm/gpu/#cuda-modules","text":"You will need to load a CUDA module to make use of GPUs on Cheaha. Depending on which version of software you are using, different versions of CUDA module may be required. For instance, tensorflow version 2.13.0 requires the CUDA/11.8.0 module. To see which versions are available on Cheaha, use the following command at the terminal. module -r spider 'CUDA/*' If a specific version of CUDA is needed but not installed, please send an install request to support@listserv.uab.edu .","title":"CUDA Modules"},{"location":"cheaha/slurm/gpu/#cudnn-modules","text":"If working with deep neural networks (DNNs, CNNs, LSTMs, LLMs, etc.), you will need to load a cuDNN module as well. The cuDNN modules are built to be compatible with a sibling CUDA module and are named with the corresponding version. For example, if you are loading CUDA/12.2.0 , you will also need to load cuDNN/8.9.2.26-CUDA-12.2.0 .","title":"cuDNN Modules"},{"location":"cheaha/slurm/gpu/#tensorflow-compatibility","text":"To check which CUDA Module version is required for your version of Tensorflow, see the toolkit requirements chart here https://www.tensorflow.org/install/source#gpu .","title":"Tensorflow Compatibility"},{"location":"cheaha/slurm/gpu/#pytorch-compatibility","text":"PyTorch does not maintain a simple compatibility table for CUDA versions. Instead, please manually check their \"get started\" page for the latest PyTorch version compatibility, and their \"previous versions\" page for older PyTorch version compatibility. Assume that a CUDA version is not compatible if it is not listed for a specific PyTorch version. To use GPUs prior to PyTorch version 1.13 you must select a cudatoolkit version from the PyTorch channel when you install PyTorch using Anaconda. It is how PyTorch knows to install a GPU compatible flavor, as opposed to the CPU only flavor. See below for templates of CPU and GPU installs for PyTorch versions prior to 1.13. Be sure to check the compatibility links above for your selected version. Note torchaudio is also available for signal processing. CPU Version: conda install pytorch==... torchvision==... -c pytorch GPU Version: conda install pytorch==... torchvision==... cudatoolkit=... -c pytorch For versions of PyTorch 1.13 and newer, use the following template instead. CPU Version: conda install pytorch==... torchvision==... cpuonly -c pytorch GPU Version: conda install pytorch==... torchvision==... pytorch-cuda=... -c pytorch -c nvidia","title":"PyTorch Compatibility"},{"location":"cheaha/slurm/gpu/#reviewing-gpu-jobs","text":"As with all jobs, use sacct to review GPU jobs. Quantity of GPUs may be reviewed using the reqtres and alloctres fields .","title":"Reviewing GPU Jobs"},{"location":"cheaha/slurm/gpu/#frequently-asked-questions-faq-about-a100-gpus","text":"I've been using the P100 GPUs on pascalnodes up until now, what is the easiest way to start using the A100 GPUs? If you are using an sbatch script... Change --partition=pascalnodes to --partition=amperenodes , or change --partition=pascalnodes-medium to --partition=amperenodes-medium . Also change --gres=gpu:3 and --gres=gpu:4 to --gres=gpu:2 , as there are only two A100 GPUs per node. If you are using an Open OnDemand Interactive App ... Change the partition from \"pascalnodes\" to \"amperenodes, or change \"pascalnodes-medium\" to \"amperenodes-medium\". In all cases, be sure to read the section on Ensuring IO Performance With A100 GPUs to be sure disk read speed doesn't limit your performance gains. How do I access the A100 GPUs? You can access the A100 GPUs by request jobs in the appropriate partitions. Use amperenodes partition for up to 12 hours or amperenodes-medium partition for up to 48 hours. How many GPUs can I request at once? Up to four GPUs may be requested by any one researcher at once. However, there are only two GPUs per node, so requesting four GPUs will allocate two nodes. To make use of multiple nodes, your workflow software must know how to communicate between nodes using software like Horovod or OpenMPI. If you are new to GPUs and aren't sure you need multiple nodes, please limit your request to one or two gpus. What performance improvements can I expect over the P100 GPUs? Performance improvements depend on the software and algorithms being used. Determining optimal configuration will take some experimenting. Swapping a single P100 to a single A100, you can generally expect 3x to 20x improvement. For more information about possible performance improvements, please see the Official NVIDIA A100 page . How can I make the most efficient use of the A100 GPUs? A100s process data very rapidly compared with previous technology. Ideally, we want the A100 to be the bottleneck during processing, rather than CPUs or I/O operations. Here are two initial possibilities to consider for optimizing efficiency: All researchers should copy their input data onto /local/$SLURM_JOB_ID (node-specific NVMe drives) before processing to avoid I/O bottlenecks reducing performance. See Ensuring IO Performance With A100 GPUs . Some researchers may benefit from using a larger number of CPU cores for data loading and preprocessing, compared with pascalnodes . Please consider experimenting with different numbers of CPU cores using the same dataset to find what is optimal for you. If you feel that performance should be higher, please contact Support so we can guide you toward an optimal CPU-to-GPU ratio for your application and workflow. Where are the A100 nodes physically located, and will this impact my workflows? The A100 nodes are located in the DC BLOX Data Center, west of UAB Campus. Because Cheaha storage (GPFS) is located on campus, there may be slightly higher latency when transferring data between the A100 nodes and GPFS. Impacts will only occur if very small amounts of data are transferred very frequently, which is unusual for most GPU workflows. We strongly recommend copying your input data onto /local/$SLURM_JOB_ID prior to processing, see Ensuring IO Performance With A100 GPUs . What will happen to the P100 GPUs? We intend to retain all of the 18 existing P100 GPU nodes, of which 9 nodes are available now. The remaining 9 nodes have been temporarily taken offline as we reconfigure hardware, and will be reallocated based on demand and other factors. What else should I be aware of? Please be sure to clean your data off of /local/$SLURM_JOB_ID as soon as you no longer need it, before the job finishes. We have updated the CUDA and cuDNN modules to improve reliability and ease of use. Please see the section on CUDA Modules for more information.","title":"Frequently Asked Questions (FAQ) About A100 GPUs"},{"location":"cheaha/slurm/introduction/","text":"Introduction to Slurm \u00b6 All work on Cheaha must be submitted to the queueing system, Slurm. This doc gives a basic overview of Slurm and how to use it. Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more. The main Slurm documentation can be found at the Slurm site . The Slurm Quickstart can also be helpful for orienting researchers new to queueing systems on the cluster. Batch Job Workflow \u00b6 Stage data to $USER_DATA , $USER_SCRATCH , or a /data/project/... directory. Research how to run your directives in 'batch' mode. In other words, how to run your analysis pipeline from the command line, with no GUIs or researcher input. Identify the appropriate resources necessary to run the jobs (CPUs, time, memory, etc) Write a job script specifying these parameters using Slurm directives. Submit the job ( sbatch ) Monitor the job ( squeue ) Review the results, and modify/rerun if necessary ( sacct and seff ) Remove data from $USER_SCRATCH For more details, please see Submitting Jobs . For details on managing and reviewing jobs, please see Job Management . The Slurm Queue \u00b6 When working on Cheaha and with Research Computing, you will often hear references to the Slurm Queue. By its name, you might think that the Slurm Queue is a first-in-first-out (FIFO) queue like when waiting in line at an event or place of business. And, some institutions use a FIFO queue, as it is the default configuration for Slurm. At UAB Research Computing, we use a multifactor priority queue, meaning that those users with top priority are first to receive service, regardless of when they entered the queue. Slurm measures priority as a single number, and the highest value generally is first to receive service. Multiple factors play into the queue. The most important factors are given in the table below, in no particular order. Factor Description What Gives Higher Priority Example Age Lenght of time job has spent in queue. Longer time in queue Job in queue for 2 days will start faster than for 4 hours. Resources Quantity of resources requested for job. Request fewer resources 1 CPU will start faster than 4 CPUs. 2 hour time limit will start faster than 10 hours. Partition Which partition was requested for job. Lower resource partitions Express partition will start faster than Long. NOT related to Priority Tier. Fair Share What fraction of cluster resources are already being used by you. Fewer jobs currently running Having 1 job already running will start faster than having 10 of the same job. The fastest way to queue a job is to request minimal resources and time, have a smaller share of total resources already used, and use the shortest partition possible. Given two or more jobs with equal priority, the job on the partition with the largest \"Priority Tier\" value goes first. The scheduler cannot predict the future. If a job enters the queue with a higher priority than yours, it will start before yours. This may lead to a situation where your job no longer fits on any of the nodes. If this happens your job will have to wait until sufficient space opens regardless of its priority value. A possible strategy to minimize the risk of preemption is to request fewer resources per node, to more readily fill available space. If you are unsure of the best queueing strategy for your workflow, please Contact Us for a consultation, we are happy to help.","title":"Introduction"},{"location":"cheaha/slurm/introduction/#introduction-to-slurm","text":"All work on Cheaha must be submitted to the queueing system, Slurm. This doc gives a basic overview of Slurm and how to use it. Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based using resource requests such as number of CPUs, maximum memory (RAM) required per CPU, maximum run time, and more. The main Slurm documentation can be found at the Slurm site . The Slurm Quickstart can also be helpful for orienting researchers new to queueing systems on the cluster.","title":"Introduction to Slurm"},{"location":"cheaha/slurm/introduction/#batch-job-workflow","text":"Stage data to $USER_DATA , $USER_SCRATCH , or a /data/project/... directory. Research how to run your directives in 'batch' mode. In other words, how to run your analysis pipeline from the command line, with no GUIs or researcher input. Identify the appropriate resources necessary to run the jobs (CPUs, time, memory, etc) Write a job script specifying these parameters using Slurm directives. Submit the job ( sbatch ) Monitor the job ( squeue ) Review the results, and modify/rerun if necessary ( sacct and seff ) Remove data from $USER_SCRATCH For more details, please see Submitting Jobs . For details on managing and reviewing jobs, please see Job Management .","title":"Batch Job Workflow"},{"location":"cheaha/slurm/introduction/#the-slurm-queue","text":"When working on Cheaha and with Research Computing, you will often hear references to the Slurm Queue. By its name, you might think that the Slurm Queue is a first-in-first-out (FIFO) queue like when waiting in line at an event or place of business. And, some institutions use a FIFO queue, as it is the default configuration for Slurm. At UAB Research Computing, we use a multifactor priority queue, meaning that those users with top priority are first to receive service, regardless of when they entered the queue. Slurm measures priority as a single number, and the highest value generally is first to receive service. Multiple factors play into the queue. The most important factors are given in the table below, in no particular order. Factor Description What Gives Higher Priority Example Age Lenght of time job has spent in queue. Longer time in queue Job in queue for 2 days will start faster than for 4 hours. Resources Quantity of resources requested for job. Request fewer resources 1 CPU will start faster than 4 CPUs. 2 hour time limit will start faster than 10 hours. Partition Which partition was requested for job. Lower resource partitions Express partition will start faster than Long. NOT related to Priority Tier. Fair Share What fraction of cluster resources are already being used by you. Fewer jobs currently running Having 1 job already running will start faster than having 10 of the same job. The fastest way to queue a job is to request minimal resources and time, have a smaller share of total resources already used, and use the shortest partition possible. Given two or more jobs with equal priority, the job on the partition with the largest \"Priority Tier\" value goes first. The scheduler cannot predict the future. If a job enters the queue with a higher priority than yours, it will start before yours. This may lead to a situation where your job no longer fits on any of the nodes. If this happens your job will have to wait until sufficient space opens regardless of its priority value. A possible strategy to minimize the risk of preemption is to request fewer resources per node, to more readily fill available space. If you are unsure of the best queueing strategy for your workflow, please Contact Us for a consultation, we are happy to help.","title":"The Slurm Queue"},{"location":"cheaha/slurm/job_management/","text":"Managing Jobs \u00b6 When jobs are submitted, researchers can monitor their status using Slurm commands. Additionally, researchers can get information about completed jobs regarding their CPU and memory usage during execution for planning future jobs. Both of these cases should be a regular part of using Cheaha for researchers. In case jobs were submitted by accident or the code was written incorrectly, they can also be cancelled. Monitoring Queued Jobs with squeue \u00b6 Currently running jobs can be monitored using the squeue command. The basic command to list all jobs for a specific researcher is: squeue -u $USER The output of squeue will look like: By default the fields displayed are jobid , partition , jobname as name , blazerid as user , job state as st , total run time as time , number of nodes as node , and the list of nodes as nodelist , used for each job a researcher has submitted. For array jobs, the JobID will be formatted as jobid_arrayid . More information is available at the Official Documentation . Cancelling Jobs with scancel \u00b6 Cancelling queued and currently running jobs can be done using the scancel command. Importantly, this will only cancel jobs that were initiated by the researcher running the command. scancel is very flexible in how it behaves: # cancel a single job or an entire job array scancel <jobid> # cancel specific job array IDs, specified as single number or a range scancel <jobid_arrayid> # cancel all jobs on a partition for the user scancel -p <partition> # cancel all jobs for a researcher scancel -u $USER Warning Cancelling all jobs will also cancel the interactive jobs created on the Open OnDemand portal. More information is available at the Official Documentation . Reviewing Past Jobs with sacct \u00b6 If you are planning a new set of jobs and are estimating resource requests, it is useful to review similar jobs that have already completed. To list past jobs for a researcher, use the sacct command. Common use cases and information are detailed below. Full details are available at the Official Documentation . Tip To minimize queue wait times and make best use of resources, please review job efficiency using seff . See our Job Efficiency page for more information. Review Jobs by JobID \u00b6 The basic form is to use -j along with a JobID to list information about that job. sacct -j <jobid> You can also review multiple jobs using a comma-separated list of JobIDs. This command will output basic information such as the ID, Name, Partition, Allocated CPUs, and State for the given JobID. Jobs can have matching extern and/or batch job entries as well. These are not especially helpful for most researchers. You can remove these entries using the -X flag. sacct -j <jobid> -X Review Jobs Submitted Between Specific Timepoints \u00b6 If you do not remember the JobID, you can use the -S and -E flags to retrieve jobs submitted between the given start datetime and end datetime. For example, to retrieve jobs submitted during the month of July 2021, the command could be: sacct -S 070121 -E 073121 sacct -S 07 /01/21 -E 07 /31/21 sacct -S 2021 -07-01 -E 2021 -07-31 Customizing the Output \u00b6 You can add -o with a list of output fields to customize the information you see. sacct -o jobid,start,end,state,alloccpu,reqmem You may also use the format <field>%<width> to make columns be <width> characters wide. This is sometimes necessary for TRES fields and nodelist , among others. An example might be alloctres%40 to make the field 40 characters wide. This command will output the JobID, the start time, end time, the state, the number of allocated CPUs, and the requested memory for the specified job. All potential output fields can be seen using sacct --helpformat . Their descriptions can be found on the sacct documentation under Job Accounting Fields. Formatting the Output \u00b6 You can format the output of sacct using a delimiter with the flags --parsable2 and --delimiter=<delim> . Any number of characters may be used as a delimiter. The default is | . It is not recommended to use , as that is used in comma-separated lists throughout sacct fields. sacct Flags \u00b6 Flag Short Description Docs FILTERING --user -u Jobs from a specific user. Please only use your own blazerid. sacct --allocations -X Show jobs only, not steps. sacct --starttime -S Jobs starting at a given time. See Time formatting . sacct --endtime -E Jobs ending at a given time. See Time Formatting . sacct --state -s Jobs with a given state. See States . sacct --jobs -j Show only the jobids supplied in a comma-separated list. sacct FORMATTING --format -o Show only the Fields supplied in a comma-separated list. sacct --helpformat -e Show a list of available Fields . sacct --parsable2 -P Output as delimited data with --delimiter if supplied, default is | . sacct --delimiter n/a Characters to delimit field values. sacct --json n/a Output as JSON. (Not yet available on Cheaha). sacct --yaml n/a Output as YAML. (Not yet available on Cheaha). sacct --noconvert n/a Keep uniform units, e.g. all M instead of M and G. See Units . sacct A complete list of flags is available at Official Documentation . sacct Fields \u00b6 Field Description Same As... Job Step Docs METADATA jobid Slurm assigned job ID number. jobid format yes yes sacct jobname User assigned job name. --job-name yes yes sacct state Current state of the job. states yes yes sacct partition Partition job was submitted to. --partition yes yes sacct ntasks Number of requested tasks. --ntasks yes yes sacct nodelist List of nodes used. --nodelist if supplied yes yes sacct TIME submit Submit time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct start Start time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct end End time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct elapsed Elapsed time as DD-HH:MM:SS n/a yes yes sacct RESOURCE REQUESTED reqcpus CPUs requested. cpu calculation yes yes sacct reqmem Memory requested. Uses 10Gc for per core, 10Gn for per node. --mem-per-cpu or --mem yes no sacct reqnodes Nodes requested. --nodes yes yes sacct reqtres All requested resources. May be used to review GPUs. tres explanation yes yes sacct RESOUCES ALLOCATED alloccpus CPUs allocated. cpu calculation yes yes sacct allocnodes Nodes allocated --nodes yes yes sacct alloctres All allocated resources. May be used to review GPUs. tres explanation yes yes sacct averss Average resident set size (memory) in bytes across tasks. resident set size no yes sacct maxrss Maximum resident set size (memory) in bytes across tasks. resident set size no yes sacct A complete list of fields is available at the Official Documentation . Slurm Common Reference \u00b6 Slurm JobID Formatting \u00b6 JobID numbers are assigned automatically by the scheduler in the order submissions are received. All jobs have a single, unique JobID number associated with them. Some features will cause JobID numbers to be reported differently than their actual value. For non-array jobs submitted with sbatch , salloc , or with srun outside of a job context, the unique JobID number is reported directly. For array jobs submitted with sbatch , the array is assigned a master ID like 12345678 , and each task is reported as <master-job-id>_<task-id> . An example might be 12345678_987 . Each task still has a unique JobID number. For job steps submitted with srun inside of a job context, the JobID is reported as <job-id>.<task-name> . All jobs submitted generate a .batch step and a .extern step. An example might be 12345678.batch . Slurm Time Formatting \u00b6 Slurm formats time in two different ways: (1) time points and (2) durations. Time points are used whenever a single point in time is needed, such as the start or end of a job. Durations are needed for job requests and reported for elapsed times. Units are given a shorthand designations: YYYY four-digit year. MM two-digit month or two-digit minutes, depending on placement. DD two-digit day. HH two-digit hour. SS two-digit seconds. AM|PM literally AM or PM. Square brackets [] indicate the contents are optional. Time points may be formatted like any of the following. HH:MM[:SS][AM|PM] MMDD[YY][-HH:MM[:SS]] MM.DD[.YY][-HH:MM[:SS]] MM/DD[/YY][-HH:MM[:SS]] YYYY-MM-DD[THH:MM[:SS]] Duration requests are made like any of the following. MM[:SS] [HH:]MM:SS DD-HH[:MM[:SS]] Durations are reported like the following. [DD-[HH:]]MM:SS Slurm States \u00b6 Job states report on where the job is in the overall Slurm process. If all goes well, you will see jobs move through the following states: PENDING RUNNING A terminal state depending on what happens COMPLETED if the job finished normally and returns exit code zero CANCELLED if the researcher cancels the job FAILED if there is a software error or non-zero exit code TIMEOUT if the job had insufficient time Other states are possible. A complete list of job states is available at the Official Documentation . Slurm Units \u00b6 Slurm uses flexible units for memory to keep reports compact. It always prefers the shortest possible representation, and will choose the largest units by default. Other units may be used, and there are flags to allow reporting in uniform units. The memory units are KMGT for kilo , mega , giga , tera respectively. All are in bytes. Slurm uses the convention that e.g. \\[ \\begin{aligned} 1\\textrm{T} &=1024\\textrm{G}\\\\ &=1024^{2}\\textrm{M}\\\\ &=1024^{3}\\textrm{K} \\end{aligned} \\] TRES Explained \u00b6 The abbreviation TRES stands for \"trackable resources\". Any resource made available by Slurm that is trackable is recorded in the Slurm database and can be recovered using sacct . The fields reqtres and alloctres can be used to review CPUs, memory, nodes and GPUs. The data is stored as a comma-separated list of <resource>=<quantity> pairs, and all values are totals across the entire job, not per node or per task. An example might look like: billing=8,cpu=8,gres/gpu=2,mem=64G,node=1 RSS Explained \u00b6 The abbreviation RSS stands for \"resident set size\", and is related to memory usage by jobs in Slurm. Memory usage is challenging to record accurately. Recording memory means a request must be made to the operating system to obtain memory usage at a single point in time, which uses computational resources. There is a balance made between resolution in time, and computational overhead. The difficulty with recording memory usage contributes to difficulty diagnosing root causes of out of memory errors, bus errors, and segmentation faults. RSS is recorded by Slurm in the sacct fields averss and maxrss . These values are both reported in bytes, rather than the usual compact memory units . Slurm Resource Calculations \u00b6 Calculating CPUs \u00b6 \\[ \\begin{aligned} \\textrm{Total CPUs} &=\\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] Example: For a job with --cpus-per-task=16 --ntasks=2 --nodes=3 : \\[ \\begin{aligned} \\textrm{Total CPUs} &=16\\times 2\\times 3\\\\ &=96 \\end{aligned} \\] Calculating Memory \u00b6 \\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{Memory}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right)\\\\ \\\\ \\textrm{Total Memory} &=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{Memory}}{\\textrm{CPU}}\\right) \\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] Examples: For a job with --mem=40G --nodes=2 : \\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=40\\textrm{G}\\times 2\\\\ &=80\\textrm{G} \\end{aligned} \\] For a job with --mem-per-cpu=10G --cpus-per-task=8 --ntasks=2 --nodes=2 : \\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=10\\textrm{G}\\times 8\\times 2\\times 2\\\\ &=320\\textrm{G} \\end{aligned} \\]","title":"Managing Jobs"},{"location":"cheaha/slurm/job_management/#managing-jobs","text":"When jobs are submitted, researchers can monitor their status using Slurm commands. Additionally, researchers can get information about completed jobs regarding their CPU and memory usage during execution for planning future jobs. Both of these cases should be a regular part of using Cheaha for researchers. In case jobs were submitted by accident or the code was written incorrectly, they can also be cancelled.","title":"Managing Jobs"},{"location":"cheaha/slurm/job_management/#monitoring-queued-jobs-with-squeue","text":"Currently running jobs can be monitored using the squeue command. The basic command to list all jobs for a specific researcher is: squeue -u $USER The output of squeue will look like: By default the fields displayed are jobid , partition , jobname as name , blazerid as user , job state as st , total run time as time , number of nodes as node , and the list of nodes as nodelist , used for each job a researcher has submitted. For array jobs, the JobID will be formatted as jobid_arrayid . More information is available at the Official Documentation .","title":"Monitoring Queued Jobs with squeue"},{"location":"cheaha/slurm/job_management/#cancelling-jobs-with-scancel","text":"Cancelling queued and currently running jobs can be done using the scancel command. Importantly, this will only cancel jobs that were initiated by the researcher running the command. scancel is very flexible in how it behaves: # cancel a single job or an entire job array scancel <jobid> # cancel specific job array IDs, specified as single number or a range scancel <jobid_arrayid> # cancel all jobs on a partition for the user scancel -p <partition> # cancel all jobs for a researcher scancel -u $USER Warning Cancelling all jobs will also cancel the interactive jobs created on the Open OnDemand portal. More information is available at the Official Documentation .","title":"Cancelling Jobs with scancel"},{"location":"cheaha/slurm/job_management/#reviewing-past-jobs-with-sacct","text":"If you are planning a new set of jobs and are estimating resource requests, it is useful to review similar jobs that have already completed. To list past jobs for a researcher, use the sacct command. Common use cases and information are detailed below. Full details are available at the Official Documentation . Tip To minimize queue wait times and make best use of resources, please review job efficiency using seff . See our Job Efficiency page for more information.","title":"Reviewing Past Jobs with sacct"},{"location":"cheaha/slurm/job_management/#review-jobs-by-jobid","text":"The basic form is to use -j along with a JobID to list information about that job. sacct -j <jobid> You can also review multiple jobs using a comma-separated list of JobIDs. This command will output basic information such as the ID, Name, Partition, Allocated CPUs, and State for the given JobID. Jobs can have matching extern and/or batch job entries as well. These are not especially helpful for most researchers. You can remove these entries using the -X flag. sacct -j <jobid> -X","title":"Review Jobs by JobID"},{"location":"cheaha/slurm/job_management/#review-jobs-submitted-between-specific-timepoints","text":"If you do not remember the JobID, you can use the -S and -E flags to retrieve jobs submitted between the given start datetime and end datetime. For example, to retrieve jobs submitted during the month of July 2021, the command could be: sacct -S 070121 -E 073121 sacct -S 07 /01/21 -E 07 /31/21 sacct -S 2021 -07-01 -E 2021 -07-31","title":"Review Jobs Submitted Between Specific Timepoints"},{"location":"cheaha/slurm/job_management/#customizing-the-output","text":"You can add -o with a list of output fields to customize the information you see. sacct -o jobid,start,end,state,alloccpu,reqmem You may also use the format <field>%<width> to make columns be <width> characters wide. This is sometimes necessary for TRES fields and nodelist , among others. An example might be alloctres%40 to make the field 40 characters wide. This command will output the JobID, the start time, end time, the state, the number of allocated CPUs, and the requested memory for the specified job. All potential output fields can be seen using sacct --helpformat . Their descriptions can be found on the sacct documentation under Job Accounting Fields.","title":"Customizing the Output"},{"location":"cheaha/slurm/job_management/#formatting-the-output","text":"You can format the output of sacct using a delimiter with the flags --parsable2 and --delimiter=<delim> . Any number of characters may be used as a delimiter. The default is | . It is not recommended to use , as that is used in comma-separated lists throughout sacct fields.","title":"Formatting the Output"},{"location":"cheaha/slurm/job_management/#sacct-flags","text":"Flag Short Description Docs FILTERING --user -u Jobs from a specific user. Please only use your own blazerid. sacct --allocations -X Show jobs only, not steps. sacct --starttime -S Jobs starting at a given time. See Time formatting . sacct --endtime -E Jobs ending at a given time. See Time Formatting . sacct --state -s Jobs with a given state. See States . sacct --jobs -j Show only the jobids supplied in a comma-separated list. sacct FORMATTING --format -o Show only the Fields supplied in a comma-separated list. sacct --helpformat -e Show a list of available Fields . sacct --parsable2 -P Output as delimited data with --delimiter if supplied, default is | . sacct --delimiter n/a Characters to delimit field values. sacct --json n/a Output as JSON. (Not yet available on Cheaha). sacct --yaml n/a Output as YAML. (Not yet available on Cheaha). sacct --noconvert n/a Keep uniform units, e.g. all M instead of M and G. See Units . sacct A complete list of flags is available at Official Documentation .","title":"sacct Flags"},{"location":"cheaha/slurm/job_management/#sacct-fields","text":"Field Description Same As... Job Step Docs METADATA jobid Slurm assigned job ID number. jobid format yes yes sacct jobname User assigned job name. --job-name yes yes sacct state Current state of the job. states yes yes sacct partition Partition job was submitted to. --partition yes yes sacct ntasks Number of requested tasks. --ntasks yes yes sacct nodelist List of nodes used. --nodelist if supplied yes yes sacct TIME submit Submit time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct start Start time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct end End time as YYYY-MM-DDTHH:MM:SS n/a yes yes sacct elapsed Elapsed time as DD-HH:MM:SS n/a yes yes sacct RESOURCE REQUESTED reqcpus CPUs requested. cpu calculation yes yes sacct reqmem Memory requested. Uses 10Gc for per core, 10Gn for per node. --mem-per-cpu or --mem yes no sacct reqnodes Nodes requested. --nodes yes yes sacct reqtres All requested resources. May be used to review GPUs. tres explanation yes yes sacct RESOUCES ALLOCATED alloccpus CPUs allocated. cpu calculation yes yes sacct allocnodes Nodes allocated --nodes yes yes sacct alloctres All allocated resources. May be used to review GPUs. tres explanation yes yes sacct averss Average resident set size (memory) in bytes across tasks. resident set size no yes sacct maxrss Maximum resident set size (memory) in bytes across tasks. resident set size no yes sacct A complete list of fields is available at the Official Documentation .","title":"sacct Fields"},{"location":"cheaha/slurm/job_management/#slurm-common-reference","text":"","title":"Slurm Common Reference"},{"location":"cheaha/slurm/job_management/#slurm-jobid-formatting","text":"JobID numbers are assigned automatically by the scheduler in the order submissions are received. All jobs have a single, unique JobID number associated with them. Some features will cause JobID numbers to be reported differently than their actual value. For non-array jobs submitted with sbatch , salloc , or with srun outside of a job context, the unique JobID number is reported directly. For array jobs submitted with sbatch , the array is assigned a master ID like 12345678 , and each task is reported as <master-job-id>_<task-id> . An example might be 12345678_987 . Each task still has a unique JobID number. For job steps submitted with srun inside of a job context, the JobID is reported as <job-id>.<task-name> . All jobs submitted generate a .batch step and a .extern step. An example might be 12345678.batch .","title":"Slurm JobID Formatting"},{"location":"cheaha/slurm/job_management/#slurm-time-formatting","text":"Slurm formats time in two different ways: (1) time points and (2) durations. Time points are used whenever a single point in time is needed, such as the start or end of a job. Durations are needed for job requests and reported for elapsed times. Units are given a shorthand designations: YYYY four-digit year. MM two-digit month or two-digit minutes, depending on placement. DD two-digit day. HH two-digit hour. SS two-digit seconds. AM|PM literally AM or PM. Square brackets [] indicate the contents are optional. Time points may be formatted like any of the following. HH:MM[:SS][AM|PM] MMDD[YY][-HH:MM[:SS]] MM.DD[.YY][-HH:MM[:SS]] MM/DD[/YY][-HH:MM[:SS]] YYYY-MM-DD[THH:MM[:SS]] Duration requests are made like any of the following. MM[:SS] [HH:]MM:SS DD-HH[:MM[:SS]] Durations are reported like the following. [DD-[HH:]]MM:SS","title":"Slurm Time Formatting"},{"location":"cheaha/slurm/job_management/#slurm-states","text":"Job states report on where the job is in the overall Slurm process. If all goes well, you will see jobs move through the following states: PENDING RUNNING A terminal state depending on what happens COMPLETED if the job finished normally and returns exit code zero CANCELLED if the researcher cancels the job FAILED if there is a software error or non-zero exit code TIMEOUT if the job had insufficient time Other states are possible. A complete list of job states is available at the Official Documentation .","title":"Slurm States"},{"location":"cheaha/slurm/job_management/#slurm-units","text":"Slurm uses flexible units for memory to keep reports compact. It always prefers the shortest possible representation, and will choose the largest units by default. Other units may be used, and there are flags to allow reporting in uniform units. The memory units are KMGT for kilo , mega , giga , tera respectively. All are in bytes. Slurm uses the convention that e.g. \\[ \\begin{aligned} 1\\textrm{T} &=1024\\textrm{G}\\\\ &=1024^{2}\\textrm{M}\\\\ &=1024^{3}\\textrm{K} \\end{aligned} \\]","title":"Slurm Units"},{"location":"cheaha/slurm/job_management/#tres-explained","text":"The abbreviation TRES stands for \"trackable resources\". Any resource made available by Slurm that is trackable is recorded in the Slurm database and can be recovered using sacct . The fields reqtres and alloctres can be used to review CPUs, memory, nodes and GPUs. The data is stored as a comma-separated list of <resource>=<quantity> pairs, and all values are totals across the entire job, not per node or per task. An example might look like: billing=8,cpu=8,gres/gpu=2,mem=64G,node=1","title":"TRES Explained"},{"location":"cheaha/slurm/job_management/#rss-explained","text":"The abbreviation RSS stands for \"resident set size\", and is related to memory usage by jobs in Slurm. Memory usage is challenging to record accurately. Recording memory means a request must be made to the operating system to obtain memory usage at a single point in time, which uses computational resources. There is a balance made between resolution in time, and computational overhead. The difficulty with recording memory usage contributes to difficulty diagnosing root causes of out of memory errors, bus errors, and segmentation faults. RSS is recorded by Slurm in the sacct fields averss and maxrss . These values are both reported in bytes, rather than the usual compact memory units .","title":"RSS Explained"},{"location":"cheaha/slurm/job_management/#slurm-resource-calculations","text":"","title":"Slurm Resource Calculations"},{"location":"cheaha/slurm/job_management/#calculating-cpus","text":"\\[ \\begin{aligned} \\textrm{Total CPUs} &=\\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] Example: For a job with --cpus-per-task=16 --ntasks=2 --nodes=3 : \\[ \\begin{aligned} \\textrm{Total CPUs} &=16\\times 2\\times 3\\\\ &=96 \\end{aligned} \\]","title":"Calculating CPUs"},{"location":"cheaha/slurm/job_management/#calculating-memory","text":"\\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{Memory}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right)\\\\ \\\\ \\textrm{Total Memory} &=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=\\left(\\frac{\\textrm{Memory}}{\\textrm{CPU}}\\right) \\left(\\frac{\\textrm{CPU}}{\\textrm{Task}}\\right) \\left(\\frac{\\textrm{Task}}{\\textrm{Node}}\\right) \\left(\\textrm{Node}\\right) \\end{aligned} \\] Examples: For a job with --mem=40G --nodes=2 : \\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=40\\textrm{G}\\times 2\\\\ &=80\\textrm{G} \\end{aligned} \\] For a job with --mem-per-cpu=10G --cpus-per-task=8 --ntasks=2 --nodes=2 : \\[ \\begin{aligned} \\textrm{Total Memory} &=\\left(\\textrm{--mem-per-cpu}\\right) \\left(\\textrm{--cpus-per-task}\\right) \\left(\\textrm{--ntasks}\\right) \\left(\\textrm{--nodes}\\right)\\\\ &=10\\textrm{G}\\times 8\\times 2\\times 2\\\\ &=320\\textrm{G} \\end{aligned} \\]","title":"Calculating Memory"},{"location":"cheaha/slurm/practical_sbatch/","text":"Practical Examples of sbatch Usage With the --array Flag and Dynamic Indices \u00b6 Do you find yourself tediously creating many sbatch job scripts for the same type of data set? Or do you modify the same job script? Have you ever experienced frustrating errors or typos while doing this? Would you like to save time by using one script for many similar tasks? If so then please read on for how to use sbatch jobs with the --array flag. The --array flag transforms an sbatch job script for a single task into a collection of tasks that are all scheduled simultaneously. For programmers, the --array flag turns a job script into a parallel loop, or a loop where each iteration is run independently and in no particular order. Naturally, this means that your tasks must be independent and similar. The most common use case is running the same software with different inputs or on different data sets. To get the most out of this page, you'll want to be familiar with Submitting Jobs . We will show how to create and use sbatch jobs with the --array flag, or sbatch --array jobs. We will use a simplified, practical example that parallels the process of a computational scientific experiment. The practical task we will solve is simplified to enhance focus on the structure of the problem, rather than the content of the problem. The structure of the problem is what makes sbatch --array jobs more or less suitable for a particular need. Specifically, whether there are many independent subtasks that all have the same structure, with similar or the same parameters. For other examples of using Slurm and its other tools, please see Submitting Jobs and Managing Jobs . The Task \u00b6 Your task is to determine the statistical properties of dice rolls. To measure these properties, you'll need to simulate rolling the dice many times to obtain a lot of data. Because dice rolls are independent, the task of simulating many dice rolls can be subdivided into many independent subtasks, all with the same parameters. While we could simulate dice rolls one at a time, in sequence, we could instead use an sbatch --array job to simulate dice rolls in parallel. Suppose you've already written some code called simulate that performs these simulations. The code transforms three integer inputs into an output sequence of positive integers. Based on the usual rules for tabletop role-playing game (TTRPG) dice rolling. The block below shows a sample input requesting ten rolls that are the sum of two six-sided die rolls plus one. In TTRPG notation we would write this as 2d6+1 rolled ten times. The output, in the case below, is a sequence of ten integers. Each element of the sequence falls in the possible range of the 2d6+1 die roll, which is [3,13] . Inputs, Outputs, How to Call # a tuplet of integers like the following... # name | number of rolls | quantity | sides | modifier # properties | positive | positive | positive | any integer 10 , 2 , 6 , 1 # is transformed to sequence of positive integers 8 ,9,4,4,3,8,6,6,13,8 # based on a call like simulate $SEED $INPUT_FILE $OUTPUT_FILE The input data takes the form of a simple text file (specifically a comma-separated value or CSV file) with four integers as described above. The upstream source of this data puts each simulation in a separate file in a separate folder. This may feel contrived for a simple example, but real experimental data is often structured with one treatment per folder, so we are using it in this example. We do not necessarily know in advance how many data folders will be present when we run the code. Sure, we could count manually and then hardcode that value, but we are trying to automate our process to avoid introducing errors and to save time in the future. All of the above constraints must fit within the framework provided by Slurm and the sbatch --array job style. Now that we have a complete list of requirements, we are ready to start forming a solution. Building a Solution \u00b6 We are going to need three components to effectively use sbatch array jobs given the requirements and constraints of the task. The simulate code that transforms inputs to outputs. We are assuming this exists and will not discuss the implementation in detail here. A job shell script file that instructs Slurm how to allocate each array job task. A main shell script to automate the --array bounds and call the job script. Job Script \u00b6 The job shell script file will be very much like a typical sbatch job script. The preamble will contain the details of Slurm scheduler instructions in the form of flags. After the preamble comes the payload, where we instruct the shell what commands need to be run within each task. Preamble \u00b6 The preamble of an sbatch job script instructs Slurm how to queue the job and what resources to allocate. Our preamble is relatively straightforward and should look familiar if you've written job scripts before. For more detailed information on what the flags mean please see Slurm Flags . job script preamble #! /bin/bash #SBATCH --job-name=simulate_dice_rolls #SBATCH --output=%x-%A-%a.log #SBATCH --error=%x-%A-%a.log #SBATCH --partition=express #SBATCH --time=00:02:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G If we had a fixed number of datasets, say ten, we could add the line #SBATCH --array=0-9 to the preamble and not require a main.sh file. In that case we would just use sbatch job.sh and be done. However, for this example, we require a dynamic upper bound on the --array flag, which isn't possible in the preamble of a script file. Payload \u00b6 For our example, the payload consists of multiple parts. We will need to extract the file from just one of the folders, ensuring that each folder is used exactly once. Dependencies must be loaded, and then the software must be run. The most interesting part will be extracting the files. File Extraction \u00b6 The general idea for file extraction, and generally use of the $SLURM_ARRAY_TASK_ID variable, is for each task to pull one unique element out of a list of possibilities. The list of possibilities can take many forms. In our case, we can use a shell array of strings constructed from a glob pattern . Other workflows may extract lines from a manifest file, or even have a computation that transforms the job array index provided by $SLURM_ARRAY_TASK_ID into some other number or collection of numbers. Below is the file extraction portion of job.sh , along with descriptions of each line. Job Script Payload: File Extraction shopt -s nullglob input_files =( ../inputs/**/dice.csv ) INPUT_FILE = ${ input_files [ $SLURM_ARRAY_TASK_ID ] } OUTPUT_FILE = ${ INPUT_FILE /inputs/outputs } OUTPUT_FILE = ${ OUTPUT_FILE /dice/rolls } The line shopt -s nullglob instructs bash to allow empty arrays created with glob patterns. The line input_files=(../inputs/**/dice.csv) uses the glob pattern ../inputs/**/dice.csv to create a shell array of paths to files in our dataset. Note that this shell array is distinct from the job --array , though they should have the same total number of elements, one for each task. The glob pattern can be read as: ../ : move up one folder then... inputs/ : look in the folder inputs for... **/ : any number of nested subfolders containing... dice.csv : the file dice.csv . The parentheses around the glob pattern instructs bash to create a shell array of strings from the results of the glob pattern. If there are multiple folders, each with one dice.csv file, then the shell array will have one entry for each of them. The shell array is then stored in the variable input_files . The line INPUT_FILE=${input_files[$SLURM_ARRAY_TASK_ID]} extracts exactly one entry from the input_files shell array and puts it in the variable INPUT_FILE . The value of $SLURM_ARRAY_TASK_ID is set by the Slurm schedule when each task starts. If there are ten tasks, as with --array=0-9 , then each task has $SLURM_ARRAY_TASK_ID set to a unique value from [0,9] . We index the shell array input_files using $SLURM_ARRAY_TASK_ID to get a single entry from the shell array. Putting it all together, each task will pull out exactly one file from the set of data folders. Tip Curly braces with a leading dollar sign like ${...} are used for evaluating some modification to a variable. The line OUTPUT_FILE=${INPUT_FILE/inputs/outputs} transforms the variable INPUT_FILE so that any instance of the word inputs is replaced with the word outputs . The result is assigned to the variable OUTPUT_FILE . The net result is the output file currently will be ../outputs/folder/dice.csv . We're not quite done setting up the output path. The line OUTPUT_FILE=${OUTPUT_FILE/dics/rolls} further replaces dice with rolls . The net result is ../outputs/folder/rolls.csv . Lines 4 and 5 result in a parallel folder structure between inputs and outputs. It is possible to use other structures such as all outputs in the same folder, or outputs in the same folders as inputs, but we won't go into detail here on how to achieve those. Running the Software \u00b6 Running the software requires setting up the random number generator seed (for repeatability), loading module dependencies, creating the output directory, and finally running the simulation. Job Script Payload: Running the Software SEED = 314159 module load ... OUTPUT_DIRECTORY = $( dirname \" $OUTPUT_FILE \" ) mkdir -p $OUTPUT_DIRECTORY simulate $SEED $INPUT_FILE $OUTPUT_FILE The line SEED=314159 sets the variable SEED with a static number for repeatability. It is set in a variable, rather than directly on the final line, so future readers of the code will understand the purpose of the number. The line module load ... is where any necessary module load or conda activate executions would go to prepare dependencies. The line OUTPUT_DIRECTORY=$(dirname \"$OUTPUT_FILE\") extracts the directory part of the output file path and assigns it to the variable OUTPUT_DIRECTORY . Tip Single parentheses with a leading dollar sign like $(...) are used for capturing the string output of a command in a variable. The line mkdir -p $OUTPUT_DIRECTORY creates the directory at the path stored in the variable $OUTPUT_DIRECTORY . The flag -p means the mkdir command will not produce an error if the directory already exists. Tip If you know your software will create any necessary output directories, then this and the previous line are not necessary. The line simulate $SEED $INPUT_FILE $OUTPUT_FILE runs the simulation. Main Script \u00b6 The main shell script will determine the upper bound $N of the --array flag, and then call sbatch --array=1-$N job.sh . It will be up to job.sh to determine how to use $SLURM_ARRAY_TASK_ID . Before we go too much further, it may be helpful to think of sbatch --array=1-$N job.sh as creating an indexed loop, from 1 to $N , and running job.sh on each of those index values. The important point is that the loop indices are run in parallel, so whatever happens in each call to job.sh must be independent. The main.sh file is the same for all languages and is shown in the code block below. The comments describe what each segment of code is doing. main.sh #! /bin/bash shopt -s nullglob input_files =( ../inputs/**/dice.csv ) FILE_COUNT = ${# input_files [@] } FILE_COUNT = $(( $FILE_COUNT - 1 )) sbatch --array = 0 - $FILE_COUNT job.sh The line #! /bin/bash instructs the operating system what interpreter to use if called without an explicit interpreter, like ./main.sh . It is best practice to have this line for scripts running in bash . Other lines are possible for other interpreters. The line shopt -s nullglob instructs bash to allow empty arrays created with glob patterns. The line input_files=(../inputs/**/dice.csv) uses the glob pattern ../inputs/**/dice.csv to create a shell array of paths to files in our dataset. The details of this are discussed above in File Extraction . The line FILE_COUNT=${#input_files[@]} gets all entries from the input_files array using the special index @ (for all elements), then counting them with the prefix symbol # . Tip Curly braces with a leading dollar sign like ${...} are used for evaluating string modifications to a variable. The line FILE_COUNT=$(( FILE_COUNT - 1 )) subtracts one from the file count. We must do this because array variables, as used in job.sh , start counting at zero (they are zero-indexed). So instead of counting from 1 to 10 , we would count from 0 to 9 . Tip Double parentheses with a leading dollar sign like $((...)) are used for evaluating integer arithmetic to a variable. The line sbatch --array=0-$FILE_COUNT job.sh puts the array tasks in the Slurm queue using the job.sh script. The number of tasks runs from 0 to $FILE_COUNT as compute above. To use the script, enter the command bash main.sh at the terminal. Putting it All Together \u00b6 We needed three parts to make the sbatch --array job work for our task. Each of these parts has been described above in some detail. simulate program to run a simulation. job.sh to instruct the Slurm scheduler what to do in each parallel task. main.sh to run everything. Executing bash main.sh at the terminal will first compute the number of array tasks, then call sbatch --array with that number of tasks on job.sh . The scheduler will then schedule that many jobs to be run. Each job will have a unique task ID, which will be used to access unique input files and write to unique output files. All of them will be run in parallel. The reason sbatch --array could be used on our dice rolling statistics task is because dice rolls are independent. It doesn't matter when I roll the dice or whether I roll them together or sequentially, the results will be statistically the same. Any task that can be broken into independent subtasks with similar input parameters can be used with sbatch --array in this way. Please feel free to use the scripts provided as a template for your own sbatch --array jobs, modifying them as appropriate. The Example Scripts \u00b6 For reference, here are the full scripts. job.sh #! /bin/bash #SBATCH --job-name=simulate_dice_rolls #SBATCH --output=%x-%A-%a.log #SBATCH --error=%x-%A-%a.log #SBATCH --partition=express #SBATCH --time=00:02:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G shopt -s nullglob input_files =( ../inputs/**/dice.csv ) INPUT_FILE = ${ input_files [ $SLURM_ARRAY_TASK_ID ] } OUTPUT_FILE = ${ INPUT_FILE /inputs/outputs } OUTPUT_FILE = ${ OUTPUT_FILE /dice/rolls } SEED = 314159 module load ... OUTPUT_DIRECTORY = $( dirname \" $OUTPUT_FILE \" ) mkdir -p $OUTPUT_DIRECTORY simulate $SEED $INPUT_FILE $OUTPUT_FILE main.sh #! /bin/bash shopt -s nullglob input_files =( ../inputs/**/dice.csv ) FILE_COUNT = ${# input_files [@] } FILE_COUNT = $(( $FILE_COUNT - 1 )) sbatch --array = 0 - $FILE_COUNT job.sh","title":"Practical Batch Array Jobs"},{"location":"cheaha/slurm/practical_sbatch/#practical-examples-of-sbatch-usage-with-the-array-flag-and-dynamic-indices","text":"Do you find yourself tediously creating many sbatch job scripts for the same type of data set? Or do you modify the same job script? Have you ever experienced frustrating errors or typos while doing this? Would you like to save time by using one script for many similar tasks? If so then please read on for how to use sbatch jobs with the --array flag. The --array flag transforms an sbatch job script for a single task into a collection of tasks that are all scheduled simultaneously. For programmers, the --array flag turns a job script into a parallel loop, or a loop where each iteration is run independently and in no particular order. Naturally, this means that your tasks must be independent and similar. The most common use case is running the same software with different inputs or on different data sets. To get the most out of this page, you'll want to be familiar with Submitting Jobs . We will show how to create and use sbatch jobs with the --array flag, or sbatch --array jobs. We will use a simplified, practical example that parallels the process of a computational scientific experiment. The practical task we will solve is simplified to enhance focus on the structure of the problem, rather than the content of the problem. The structure of the problem is what makes sbatch --array jobs more or less suitable for a particular need. Specifically, whether there are many independent subtasks that all have the same structure, with similar or the same parameters. For other examples of using Slurm and its other tools, please see Submitting Jobs and Managing Jobs .","title":"Practical Examples of sbatch Usage With the --array Flag and Dynamic Indices"},{"location":"cheaha/slurm/practical_sbatch/#the-task","text":"Your task is to determine the statistical properties of dice rolls. To measure these properties, you'll need to simulate rolling the dice many times to obtain a lot of data. Because dice rolls are independent, the task of simulating many dice rolls can be subdivided into many independent subtasks, all with the same parameters. While we could simulate dice rolls one at a time, in sequence, we could instead use an sbatch --array job to simulate dice rolls in parallel. Suppose you've already written some code called simulate that performs these simulations. The code transforms three integer inputs into an output sequence of positive integers. Based on the usual rules for tabletop role-playing game (TTRPG) dice rolling. The block below shows a sample input requesting ten rolls that are the sum of two six-sided die rolls plus one. In TTRPG notation we would write this as 2d6+1 rolled ten times. The output, in the case below, is a sequence of ten integers. Each element of the sequence falls in the possible range of the 2d6+1 die roll, which is [3,13] . Inputs, Outputs, How to Call # a tuplet of integers like the following... # name | number of rolls | quantity | sides | modifier # properties | positive | positive | positive | any integer 10 , 2 , 6 , 1 # is transformed to sequence of positive integers 8 ,9,4,4,3,8,6,6,13,8 # based on a call like simulate $SEED $INPUT_FILE $OUTPUT_FILE The input data takes the form of a simple text file (specifically a comma-separated value or CSV file) with four integers as described above. The upstream source of this data puts each simulation in a separate file in a separate folder. This may feel contrived for a simple example, but real experimental data is often structured with one treatment per folder, so we are using it in this example. We do not necessarily know in advance how many data folders will be present when we run the code. Sure, we could count manually and then hardcode that value, but we are trying to automate our process to avoid introducing errors and to save time in the future. All of the above constraints must fit within the framework provided by Slurm and the sbatch --array job style. Now that we have a complete list of requirements, we are ready to start forming a solution.","title":"The Task"},{"location":"cheaha/slurm/practical_sbatch/#building-a-solution","text":"We are going to need three components to effectively use sbatch array jobs given the requirements and constraints of the task. The simulate code that transforms inputs to outputs. We are assuming this exists and will not discuss the implementation in detail here. A job shell script file that instructs Slurm how to allocate each array job task. A main shell script to automate the --array bounds and call the job script.","title":"Building a Solution"},{"location":"cheaha/slurm/practical_sbatch/#job-script","text":"The job shell script file will be very much like a typical sbatch job script. The preamble will contain the details of Slurm scheduler instructions in the form of flags. After the preamble comes the payload, where we instruct the shell what commands need to be run within each task.","title":"Job Script"},{"location":"cheaha/slurm/practical_sbatch/#preamble","text":"The preamble of an sbatch job script instructs Slurm how to queue the job and what resources to allocate. Our preamble is relatively straightforward and should look familiar if you've written job scripts before. For more detailed information on what the flags mean please see Slurm Flags . job script preamble #! /bin/bash #SBATCH --job-name=simulate_dice_rolls #SBATCH --output=%x-%A-%a.log #SBATCH --error=%x-%A-%a.log #SBATCH --partition=express #SBATCH --time=00:02:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G If we had a fixed number of datasets, say ten, we could add the line #SBATCH --array=0-9 to the preamble and not require a main.sh file. In that case we would just use sbatch job.sh and be done. However, for this example, we require a dynamic upper bound on the --array flag, which isn't possible in the preamble of a script file.","title":"Preamble"},{"location":"cheaha/slurm/practical_sbatch/#payload","text":"For our example, the payload consists of multiple parts. We will need to extract the file from just one of the folders, ensuring that each folder is used exactly once. Dependencies must be loaded, and then the software must be run. The most interesting part will be extracting the files.","title":"Payload"},{"location":"cheaha/slurm/practical_sbatch/#file-extraction","text":"The general idea for file extraction, and generally use of the $SLURM_ARRAY_TASK_ID variable, is for each task to pull one unique element out of a list of possibilities. The list of possibilities can take many forms. In our case, we can use a shell array of strings constructed from a glob pattern . Other workflows may extract lines from a manifest file, or even have a computation that transforms the job array index provided by $SLURM_ARRAY_TASK_ID into some other number or collection of numbers. Below is the file extraction portion of job.sh , along with descriptions of each line. Job Script Payload: File Extraction shopt -s nullglob input_files =( ../inputs/**/dice.csv ) INPUT_FILE = ${ input_files [ $SLURM_ARRAY_TASK_ID ] } OUTPUT_FILE = ${ INPUT_FILE /inputs/outputs } OUTPUT_FILE = ${ OUTPUT_FILE /dice/rolls } The line shopt -s nullglob instructs bash to allow empty arrays created with glob patterns. The line input_files=(../inputs/**/dice.csv) uses the glob pattern ../inputs/**/dice.csv to create a shell array of paths to files in our dataset. Note that this shell array is distinct from the job --array , though they should have the same total number of elements, one for each task. The glob pattern can be read as: ../ : move up one folder then... inputs/ : look in the folder inputs for... **/ : any number of nested subfolders containing... dice.csv : the file dice.csv . The parentheses around the glob pattern instructs bash to create a shell array of strings from the results of the glob pattern. If there are multiple folders, each with one dice.csv file, then the shell array will have one entry for each of them. The shell array is then stored in the variable input_files . The line INPUT_FILE=${input_files[$SLURM_ARRAY_TASK_ID]} extracts exactly one entry from the input_files shell array and puts it in the variable INPUT_FILE . The value of $SLURM_ARRAY_TASK_ID is set by the Slurm schedule when each task starts. If there are ten tasks, as with --array=0-9 , then each task has $SLURM_ARRAY_TASK_ID set to a unique value from [0,9] . We index the shell array input_files using $SLURM_ARRAY_TASK_ID to get a single entry from the shell array. Putting it all together, each task will pull out exactly one file from the set of data folders. Tip Curly braces with a leading dollar sign like ${...} are used for evaluating some modification to a variable. The line OUTPUT_FILE=${INPUT_FILE/inputs/outputs} transforms the variable INPUT_FILE so that any instance of the word inputs is replaced with the word outputs . The result is assigned to the variable OUTPUT_FILE . The net result is the output file currently will be ../outputs/folder/dice.csv . We're not quite done setting up the output path. The line OUTPUT_FILE=${OUTPUT_FILE/dics/rolls} further replaces dice with rolls . The net result is ../outputs/folder/rolls.csv . Lines 4 and 5 result in a parallel folder structure between inputs and outputs. It is possible to use other structures such as all outputs in the same folder, or outputs in the same folders as inputs, but we won't go into detail here on how to achieve those.","title":"File Extraction"},{"location":"cheaha/slurm/practical_sbatch/#running-the-software","text":"Running the software requires setting up the random number generator seed (for repeatability), loading module dependencies, creating the output directory, and finally running the simulation. Job Script Payload: Running the Software SEED = 314159 module load ... OUTPUT_DIRECTORY = $( dirname \" $OUTPUT_FILE \" ) mkdir -p $OUTPUT_DIRECTORY simulate $SEED $INPUT_FILE $OUTPUT_FILE The line SEED=314159 sets the variable SEED with a static number for repeatability. It is set in a variable, rather than directly on the final line, so future readers of the code will understand the purpose of the number. The line module load ... is where any necessary module load or conda activate executions would go to prepare dependencies. The line OUTPUT_DIRECTORY=$(dirname \"$OUTPUT_FILE\") extracts the directory part of the output file path and assigns it to the variable OUTPUT_DIRECTORY . Tip Single parentheses with a leading dollar sign like $(...) are used for capturing the string output of a command in a variable. The line mkdir -p $OUTPUT_DIRECTORY creates the directory at the path stored in the variable $OUTPUT_DIRECTORY . The flag -p means the mkdir command will not produce an error if the directory already exists. Tip If you know your software will create any necessary output directories, then this and the previous line are not necessary. The line simulate $SEED $INPUT_FILE $OUTPUT_FILE runs the simulation.","title":"Running the Software"},{"location":"cheaha/slurm/practical_sbatch/#main-script","text":"The main shell script will determine the upper bound $N of the --array flag, and then call sbatch --array=1-$N job.sh . It will be up to job.sh to determine how to use $SLURM_ARRAY_TASK_ID . Before we go too much further, it may be helpful to think of sbatch --array=1-$N job.sh as creating an indexed loop, from 1 to $N , and running job.sh on each of those index values. The important point is that the loop indices are run in parallel, so whatever happens in each call to job.sh must be independent. The main.sh file is the same for all languages and is shown in the code block below. The comments describe what each segment of code is doing. main.sh #! /bin/bash shopt -s nullglob input_files =( ../inputs/**/dice.csv ) FILE_COUNT = ${# input_files [@] } FILE_COUNT = $(( $FILE_COUNT - 1 )) sbatch --array = 0 - $FILE_COUNT job.sh The line #! /bin/bash instructs the operating system what interpreter to use if called without an explicit interpreter, like ./main.sh . It is best practice to have this line for scripts running in bash . Other lines are possible for other interpreters. The line shopt -s nullglob instructs bash to allow empty arrays created with glob patterns. The line input_files=(../inputs/**/dice.csv) uses the glob pattern ../inputs/**/dice.csv to create a shell array of paths to files in our dataset. The details of this are discussed above in File Extraction . The line FILE_COUNT=${#input_files[@]} gets all entries from the input_files array using the special index @ (for all elements), then counting them with the prefix symbol # . Tip Curly braces with a leading dollar sign like ${...} are used for evaluating string modifications to a variable. The line FILE_COUNT=$(( FILE_COUNT - 1 )) subtracts one from the file count. We must do this because array variables, as used in job.sh , start counting at zero (they are zero-indexed). So instead of counting from 1 to 10 , we would count from 0 to 9 . Tip Double parentheses with a leading dollar sign like $((...)) are used for evaluating integer arithmetic to a variable. The line sbatch --array=0-$FILE_COUNT job.sh puts the array tasks in the Slurm queue using the job.sh script. The number of tasks runs from 0 to $FILE_COUNT as compute above. To use the script, enter the command bash main.sh at the terminal.","title":"Main Script"},{"location":"cheaha/slurm/practical_sbatch/#putting-it-all-together","text":"We needed three parts to make the sbatch --array job work for our task. Each of these parts has been described above in some detail. simulate program to run a simulation. job.sh to instruct the Slurm scheduler what to do in each parallel task. main.sh to run everything. Executing bash main.sh at the terminal will first compute the number of array tasks, then call sbatch --array with that number of tasks on job.sh . The scheduler will then schedule that many jobs to be run. Each job will have a unique task ID, which will be used to access unique input files and write to unique output files. All of them will be run in parallel. The reason sbatch --array could be used on our dice rolling statistics task is because dice rolls are independent. It doesn't matter when I roll the dice or whether I roll them together or sequentially, the results will be statistically the same. Any task that can be broken into independent subtasks with similar input parameters can be used with sbatch --array in this way. Please feel free to use the scripts provided as a template for your own sbatch --array jobs, modifying them as appropriate.","title":"Putting it All Together"},{"location":"cheaha/slurm/practical_sbatch/#the-example-scripts","text":"For reference, here are the full scripts. job.sh #! /bin/bash #SBATCH --job-name=simulate_dice_rolls #SBATCH --output=%x-%A-%a.log #SBATCH --error=%x-%A-%a.log #SBATCH --partition=express #SBATCH --time=00:02:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G shopt -s nullglob input_files =( ../inputs/**/dice.csv ) INPUT_FILE = ${ input_files [ $SLURM_ARRAY_TASK_ID ] } OUTPUT_FILE = ${ INPUT_FILE /inputs/outputs } OUTPUT_FILE = ${ OUTPUT_FILE /dice/rolls } SEED = 314159 module load ... OUTPUT_DIRECTORY = $( dirname \" $OUTPUT_FILE \" ) mkdir -p $OUTPUT_DIRECTORY simulate $SEED $INPUT_FILE $OUTPUT_FILE main.sh #! /bin/bash shopt -s nullglob input_files =( ../inputs/**/dice.csv ) FILE_COUNT = ${# input_files [@] } FILE_COUNT = $(( $FILE_COUNT - 1 )) sbatch --array = 0 - $FILE_COUNT job.sh","title":"The Example Scripts"},{"location":"cheaha/slurm/submitting_jobs/","text":"Submitting Jobs with Slurm \u00b6 Processing computational tasks with Cheaha at the terminal requires submitting jobs to the Slurm scheduler. Slurm offers two commands to submit jobs: sbatch and srun . Always use sbatch to submit jobs to the scheduler, unless you need an interactive terminal . Otherwise only use srun within sbatch for submitting job steps within an sbatch script context. The command sbatch accepts script files as input. Scripts should be written in an available shell language on Cheaha, typically bash, and should include the appropriate Slurm directives at the top of the script telling the scheduler the requested resources. Read on to learn more about how to use Slurm effectively. Important Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources. Common Slurm Terminology \u00b6 Node: A self-contained computing devices, forming the basic unit of the cluster. A node has multiple CPUs, memory, and some have GPUs. Jobs requiring multiple nodes must use a protocol such as MPI to communicate between them. Login nodes: Gateway for reseacher access to computing resources, shared among all users. DO NOT run research computation tasks on the login node. Compute nodes: Dedicated nodes for running research computation tasks. Core: A single unit of computational processing, not to be confused with a CPU, which may have many cores. Partition: A logical subset of nodes sharing computational features. Different partitions have different resource limits, priorities, and hardware. Job: A collection of commands that require computational resources to perform. Can be interactive with srun or submitted to the scheduler with srun or sbatch . Batch Job: An array of jobs which all have the same plan for execution, but may vary in terms of input and output. Only available in non-interactive batch mode via sbatch Job ID: The unique number representing the job, returned by srun and sbatch . Stored in $SLURM_JOB_ID within a job. Job Index Number: For array jobs, the index of the currently running job within the array. Stored in $SLURM_ARRAY_TASK_ID within a job. Slurm Flags and Environment Variables \u00b6 Slurm has many flags a researcher can use when creating a job, but a short list of the most important ones for are described below. It is highly recommended to be as explicit as possible with flags and not rely on system defaults. Explicitly using the flags below makes your scripts more portable, shareable and reproducible. Flag Short Environment Variable Description sbatch srun --job-name -J SBATCH_JOB_NAME Name of job stored in records and visible in squeue . sbatch srun SLURM_JOB_ID Job ID number of running job or array task. May differ from SLURM_ARRAY_JOB_ID depending on array task index sbatch srun --output -o SBATCH_OUTPUT Path to file storing text output. sbatch srun --error -e SBATCH_ERROR Path to file storing error output. sbatch srun --partition -p SBATCH_PARTITION Partition to submit job to. More details below. sbatch srun --time -t SBATCH_TIMELIMIT Maximum allowed runtime of job. Allowed formats below. sbatch srun --nodes -N Number of nodes needed. Set to 1 if your software does not use MPI or if unsure. sbatch srun --ntasks -n SLURM_NTASKS Number of tasks planned per node. Mostly used for bookkeeping and calculating total cpus per node. If unsure set to 1 . sbatch srun --cpus-per-task -c SLURM_CPUS_PER_TASK Number of needed cores per task. Cores per node equals -n times -c . sbatch srun SLURM_CPUS_ON_NODE Number of cpus available on this node. sbatch srun --mem SLURM_MEM_PER_NODE Amount of RAM needed per node in MB. Can specify 16 GB using 16384 or 16G. sbatch srun --gres SBATCH_GRES Used to request GPUs per node. For 2 GPUs per node use --gres=gpu:2 . sbatch srun --array SBATCH_ARRAY_INX Comma-separated list of similar tasks to run. More details below. sbatch n/a SBATCH_ARRAY_JOB_ID Parent Job ID number of array task. Same for all array tasks submitted with same script. May differ from SLURM_JOB_ID depending on array task index. sbatch n/a SLURM_ARRAY_TASK_COUNT Total number of array tasks. sbatch n/a SLURM_ARRAY_TASK_ID Current array task index. sbatch n/a Available Partitions for --partition \u00b6 Please see Cheaha Hardware for more information. Remember, the smaller your resource request, the sooner your job will get through the queue. Requesting GPUs \u00b6 Please see the GPUs page for more information. Dynamic --output and --error File Names \u00b6 The --output and --error flags can use dynamic job information as part of the name: %j is the Job ID, equal to $SLURM_JOB_ID . %A is the main Array Job ID, equal to $SLURM_ARRAY_JOB_ID . %a is the Array job index number, equal to $SLURM_ARRAY_TASK_ID . %x is the --job-name , equal to $SLURM_JOB_NAME . For example if using --job-name=my-job , then to create an output file like my-job-12345678 use --output=%x-%j . If also using --array=0-4 , then to create an output file like my-job-12345678-0 use --output=%x-%A-%a . Batch Jobs with sbatch \u00b6 Important The following examples assume familiarity with the Linux terminal. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources. Batch jobs are typically submitted using scripts with sbatch . Using sbatch this way is the preferred method for submitting jobs to Slurm on Cheaha. It is more portable, shareable, reproducible and scripts can be version controlled using Git . For batch jobs, flags are typically included as directive comments at the top of the script like #SBATCH --job-name=my-job . Read on to see examples of batch jobs using sbatch . A Simple Batch Job \u00b6 Below is an example batch job script. To test it, copy and paste it into a plain text file testjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testjob.sh . Momentarily, two text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%j.out #SBATCH --error=%x_%j.err echo \"Hello World\" echo \"Hello Error\" 1 > & 2 There is a lot going on in the above script, so let's break it down. There are three main chunks of this script: Line 1 is the interpreter directive: #!/bin/bash . This tells the shell what application to use to execute this script. All sbatch scripts on Cheaha should start with this line. Lines 3-11 are the sbatch flags which tell the scheduler what resources you need and how to manage your job. Line 3: The job name is test . Lines 4-7: The job will have 1 node, with 1 core and 1 GB of memory. Line 8: The job will be on the express partition. Line 9: The job will be no longer than 10 minutes, and will be terminated if it runs over. Line 10: Any standard output ( stdout ) will be written to the file test_$SLURM_JOB_ID.out in the same directory as the script, whatever the $SLURM_JOB_ID happens to be when the job is submitted. The name comes from %x equal to test , the --job-name , and %j equal to the Job ID. Line 11: Any error output ( stderr ) will be written to a different file test_$SLURM_JOB_ID.err in the same directory. Lines 13 and 14 are the payload, or tasks to be run. They will be executed in order from top to bottom just like any shell script. In this case, it is simply writing \"Hello World\" to the --output file and \"Hello Error\" to the --error file. The 1>&2 Means redirect a copy ( >& ) of stdout to stderr . Batch Array Jobs With Known Indices \u00b6 Building on the job script above, below is an array job. Array jobs are useful when you need to perform the same analysis on slightly different inputs with no interaction between those analyses. We call this situation \"pleasingly parallel\". We can take advantage of an array job using the variable $SLURM_ARRAY_TASK_ID , which will have an integer in the set of values we give to the --array flag. To test the script below, copy and paste it into a plain text file testarrayjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testarrayjob.sh . Momentarily, 16 text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%A_%a.out #SBATCH --error=%x_%A_%a.err #SBATCH --array=0-9 echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID This script is very similar to the one above, but will submit 10 jobs to the scheduler that all do slightly different things. Each of the 10 jobs will have the same amount and type of resources allocated, and can run in parallel. The 10 jobs come from --array=0-9 . The output of each job will be one of the numbers in the set {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} , depending on which job is running. The output files will look like test_$(SLURM_ARRAY_JOB_ID)_$(SLURM_ARRAY_TASK_ID).out or .err . The value of $(SLURM_ARRAY_JOB_ID) is the main Job ID given to the entire array submission. Scripts can be written to take advantage of the $SLURM_ARRAY_TASK_ID variable indexing variable. For example, a project could have a list of participants that should be processed in the same way, and the analysis script uses the array task ID as an index to pull out one entry from that list for each job. Many common programming languages can interact with shell variables like $SLURM_ARRAY_TASK_ID , or the values can be passed to a program as an argument. You can override the --array flag stored in the script when you call sbatch . To do so, pass another --array flag along with the script name like below. This allows you to rerun only subsets of your array script. # submit jobs with index 0, 3, and 7 sbatch --array = 0 ,3,7 array.sh # submit jobs with index 0, 2, 4, and 6 sbatch --array = 0 -6:2 array.sh For more details on using sbatch please see the official documentation . Note If you are using bash or shell arrays, it is crucial to note they use 0-based indexing. Plan your --array flag indices accordingly. Batch Array Jobs With Dynamic or Computed Indices \u00b6 For a practical example with dynamic indices, please visit our Practical sbatch Examples Interactive Jobs with srun \u00b6 Jobs should be submitted to the Slurm job scheduler either using a batch job or an Open OnDemand (OOD) interactive job . You can use srun for working on short interactive tasks such as creating an Anaconda environment and running parallel tasks within an sbatch script. Warning The limitations of srun is that the jobs/execution die if the internet connection is down, and you may have to rerun the job again. Let us see how to acquire a compute node quickly using srun . You can run interactive job using srun command with the --pty /bin/bash flag. Here is an example, $srun --ntasks = 2 --time = 01 :00:00 --mem-per-cpu = 8G --partition = medium --job-name = test_srun --pty /bin/bash srun: job 21648044 queued and waiting for resources srun: job 21648044 has been allocated resources The above example allocates a compute node with a 8GB of RAM on a medium partition with --ntasks=2 to run short tasks. srun for running parallel jobs \u00b6 srun is used to run executables in parallel, and is used within sbatch script. Let us see an example where srun is used to launch multiple (parallel) instances of a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --job-name=srun_test #SBATCH --partition=long #SBATCH --time=05:00 #SBATCH --mem=4G srun hostname In the script above, we have asked for two nodes --nodes=2, and each node will run a single instance of a hostname as we requested --ntasks-per-node=1. The output for the above script is, c0187 c0188 Here is another example of running different independent programs simultaneously on different resources within a batch job. Multiple srun can execute simultaneously as long as they do not exceed the resources reserved for that job i.e., step 1 executes in node 1 with --ntasks=4, and step 2 executes in node 2 with --ntasks=4 simultaneously. Note that --nodes=1 -r1 in step 2 defines the number of nodes and their relative node position within the resources assigned to the job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks=8 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=1 #SBATCH --partition=amd-hdr100 #SBATCH --time=05:00 #SBATCH --mem-per-cpu=1G #Partioning of resources for two different tasks #STEP 1 srun --nodes = 1 --ntasks = 4 hostname #STEP 2 srun --nodes = 1 -r1 --ntasks = 4 uname -a Here is the output for running multiple srun in a single job, i.e., executing the hostname and uname -a tasks simultaneously but on different nodes. c0203 c0203 c0203 c0203 Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Alternatively, srun can also run MPI, OpenMP, hybrid MPI/OpenMP, and many more parallel jobs. For more details on using srun , please see the official documentation . Important srun has been disabled for use with MPI. We have removed this functionality due to an open vulnerability: https://nvd.nist.gov/vuln/detail/CVE-2023-41915 . The vulnerability could allow an attacker to escalate privileges to root and/or access data they do not have permissions for. Instead of srun , please load one of the OpenMPI modules with an appropriate version. Please contact Support with any questions or concerns. Graphical Interactive Jobs \u00b6 It is highly recommended to use the Open OnDemand web portal for interactive apps . Interactive sessions for certain software such as MATLAB and RStudio can be created directly from the browser while an HPC Desktop is available to access all of the other software on Cheaha. A terminal is also available through Open OnDemand. It is possible to use other remote desktop software, such as VNC, to start and interact with jobs. These methods are not officially supported and we do not have the capacity to help with remote desktop connections. Instead, please consider switching your workflow to use the Open OnDemand HPC Desktop . If you are unable to use this method, please contact Support . Estimating Compute Resources \u00b6 Being able to estimate how many resources a job will need is critical. Requesting many more resources than necessary bottlenecks the cluster by reserving unused resources for an inefficient job preventing other jobs from using them. However, requesting too few resources will slow down the job or cause it to error. Questions to ask yourself when requesting job resources: Can my scripts take advantage of multiple CPUs? For instance, RStudio generally works on a single thread. Requesting more than 1 CPU here would not improve performance. How large is the data I'm working with? Do my pipelines keep large amounts of data in memory? How long should my job take? For example, do not request 50 hours time for a 15 hour process. Have a reasonable buffer included to account for unexpected processing delays, but do not request the maximum time on a partition if that's unnecessary. Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. To get the most out of your Cheaha experience and ensure your jobs get through the queue as fast as possible, please read about Job Efficiency . Faster Queuing with Job Efficiency \u00b6 Please see our page on Job Efficiency for more information on making the best use of cluster resources to minimize your queue wait times.","title":"Submitting Jobs"},{"location":"cheaha/slurm/submitting_jobs/#submitting-jobs-with-slurm","text":"Processing computational tasks with Cheaha at the terminal requires submitting jobs to the Slurm scheduler. Slurm offers two commands to submit jobs: sbatch and srun . Always use sbatch to submit jobs to the scheduler, unless you need an interactive terminal . Otherwise only use srun within sbatch for submitting job steps within an sbatch script context. The command sbatch accepts script files as input. Scripts should be written in an available shell language on Cheaha, typically bash, and should include the appropriate Slurm directives at the top of the script telling the scheduler the requested resources. Read on to learn more about how to use Slurm effectively. Important Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources.","title":"Submitting Jobs with Slurm"},{"location":"cheaha/slurm/submitting_jobs/#common-slurm-terminology","text":"Node: A self-contained computing devices, forming the basic unit of the cluster. A node has multiple CPUs, memory, and some have GPUs. Jobs requiring multiple nodes must use a protocol such as MPI to communicate between them. Login nodes: Gateway for reseacher access to computing resources, shared among all users. DO NOT run research computation tasks on the login node. Compute nodes: Dedicated nodes for running research computation tasks. Core: A single unit of computational processing, not to be confused with a CPU, which may have many cores. Partition: A logical subset of nodes sharing computational features. Different partitions have different resource limits, priorities, and hardware. Job: A collection of commands that require computational resources to perform. Can be interactive with srun or submitted to the scheduler with srun or sbatch . Batch Job: An array of jobs which all have the same plan for execution, but may vary in terms of input and output. Only available in non-interactive batch mode via sbatch Job ID: The unique number representing the job, returned by srun and sbatch . Stored in $SLURM_JOB_ID within a job. Job Index Number: For array jobs, the index of the currently running job within the array. Stored in $SLURM_ARRAY_TASK_ID within a job.","title":"Common Slurm Terminology"},{"location":"cheaha/slurm/submitting_jobs/#slurm-flags-and-environment-variables","text":"Slurm has many flags a researcher can use when creating a job, but a short list of the most important ones for are described below. It is highly recommended to be as explicit as possible with flags and not rely on system defaults. Explicitly using the flags below makes your scripts more portable, shareable and reproducible. Flag Short Environment Variable Description sbatch srun --job-name -J SBATCH_JOB_NAME Name of job stored in records and visible in squeue . sbatch srun SLURM_JOB_ID Job ID number of running job or array task. May differ from SLURM_ARRAY_JOB_ID depending on array task index sbatch srun --output -o SBATCH_OUTPUT Path to file storing text output. sbatch srun --error -e SBATCH_ERROR Path to file storing error output. sbatch srun --partition -p SBATCH_PARTITION Partition to submit job to. More details below. sbatch srun --time -t SBATCH_TIMELIMIT Maximum allowed runtime of job. Allowed formats below. sbatch srun --nodes -N Number of nodes needed. Set to 1 if your software does not use MPI or if unsure. sbatch srun --ntasks -n SLURM_NTASKS Number of tasks planned per node. Mostly used for bookkeeping and calculating total cpus per node. If unsure set to 1 . sbatch srun --cpus-per-task -c SLURM_CPUS_PER_TASK Number of needed cores per task. Cores per node equals -n times -c . sbatch srun SLURM_CPUS_ON_NODE Number of cpus available on this node. sbatch srun --mem SLURM_MEM_PER_NODE Amount of RAM needed per node in MB. Can specify 16 GB using 16384 or 16G. sbatch srun --gres SBATCH_GRES Used to request GPUs per node. For 2 GPUs per node use --gres=gpu:2 . sbatch srun --array SBATCH_ARRAY_INX Comma-separated list of similar tasks to run. More details below. sbatch n/a SBATCH_ARRAY_JOB_ID Parent Job ID number of array task. Same for all array tasks submitted with same script. May differ from SLURM_JOB_ID depending on array task index. sbatch n/a SLURM_ARRAY_TASK_COUNT Total number of array tasks. sbatch n/a SLURM_ARRAY_TASK_ID Current array task index. sbatch n/a","title":"Slurm Flags and Environment Variables"},{"location":"cheaha/slurm/submitting_jobs/#available-partitions-for-partition","text":"Please see Cheaha Hardware for more information. Remember, the smaller your resource request, the sooner your job will get through the queue.","title":"Available Partitions for --partition"},{"location":"cheaha/slurm/submitting_jobs/#requesting-gpus","text":"Please see the GPUs page for more information.","title":"Requesting GPUs"},{"location":"cheaha/slurm/submitting_jobs/#dynamic-output-and-error-file-names","text":"The --output and --error flags can use dynamic job information as part of the name: %j is the Job ID, equal to $SLURM_JOB_ID . %A is the main Array Job ID, equal to $SLURM_ARRAY_JOB_ID . %a is the Array job index number, equal to $SLURM_ARRAY_TASK_ID . %x is the --job-name , equal to $SLURM_JOB_NAME . For example if using --job-name=my-job , then to create an output file like my-job-12345678 use --output=%x-%j . If also using --array=0-4 , then to create an output file like my-job-12345678-0 use --output=%x-%A-%a .","title":"Dynamic --output and --error File Names"},{"location":"cheaha/slurm/submitting_jobs/#batch-jobs-with-sbatch","text":"Important The following examples assume familiarity with the Linux terminal. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources. Batch jobs are typically submitted using scripts with sbatch . Using sbatch this way is the preferred method for submitting jobs to Slurm on Cheaha. It is more portable, shareable, reproducible and scripts can be version controlled using Git . For batch jobs, flags are typically included as directive comments at the top of the script like #SBATCH --job-name=my-job . Read on to see examples of batch jobs using sbatch .","title":"Batch Jobs with sbatch"},{"location":"cheaha/slurm/submitting_jobs/#a-simple-batch-job","text":"Below is an example batch job script. To test it, copy and paste it into a plain text file testjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testjob.sh . Momentarily, two text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%j.out #SBATCH --error=%x_%j.err echo \"Hello World\" echo \"Hello Error\" 1 > & 2 There is a lot going on in the above script, so let's break it down. There are three main chunks of this script: Line 1 is the interpreter directive: #!/bin/bash . This tells the shell what application to use to execute this script. All sbatch scripts on Cheaha should start with this line. Lines 3-11 are the sbatch flags which tell the scheduler what resources you need and how to manage your job. Line 3: The job name is test . Lines 4-7: The job will have 1 node, with 1 core and 1 GB of memory. Line 8: The job will be on the express partition. Line 9: The job will be no longer than 10 minutes, and will be terminated if it runs over. Line 10: Any standard output ( stdout ) will be written to the file test_$SLURM_JOB_ID.out in the same directory as the script, whatever the $SLURM_JOB_ID happens to be when the job is submitted. The name comes from %x equal to test , the --job-name , and %j equal to the Job ID. Line 11: Any error output ( stderr ) will be written to a different file test_$SLURM_JOB_ID.err in the same directory. Lines 13 and 14 are the payload, or tasks to be run. They will be executed in order from top to bottom just like any shell script. In this case, it is simply writing \"Hello World\" to the --output file and \"Hello Error\" to the --error file. The 1>&2 Means redirect a copy ( >& ) of stdout to stderr .","title":"A Simple Batch Job"},{"location":"cheaha/slurm/submitting_jobs/#batch-array-jobs-with-known-indices","text":"Building on the job script above, below is an array job. Array jobs are useful when you need to perform the same analysis on slightly different inputs with no interaction between those analyses. We call this situation \"pleasingly parallel\". We can take advantage of an array job using the variable $SLURM_ARRAY_TASK_ID , which will have an integer in the set of values we give to the --array flag. To test the script below, copy and paste it into a plain text file testarrayjob.sh in your Home Directory on Cheaha. Run it at the terminal by navigating to your home directory by entering cd ~ and then entering sbatch testarrayjob.sh . Momentarily, 16 text files with .out and .err suffixes will be produced in your home directory. #!/bin/bash # #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --partition=express #SBATCH --time=00:10:00 #SBATCH --output=%x_%A_%a.out #SBATCH --error=%x_%A_%a.err #SBATCH --array=0-9 echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID This script is very similar to the one above, but will submit 10 jobs to the scheduler that all do slightly different things. Each of the 10 jobs will have the same amount and type of resources allocated, and can run in parallel. The 10 jobs come from --array=0-9 . The output of each job will be one of the numbers in the set {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} , depending on which job is running. The output files will look like test_$(SLURM_ARRAY_JOB_ID)_$(SLURM_ARRAY_TASK_ID).out or .err . The value of $(SLURM_ARRAY_JOB_ID) is the main Job ID given to the entire array submission. Scripts can be written to take advantage of the $SLURM_ARRAY_TASK_ID variable indexing variable. For example, a project could have a list of participants that should be processed in the same way, and the analysis script uses the array task ID as an index to pull out one entry from that list for each job. Many common programming languages can interact with shell variables like $SLURM_ARRAY_TASK_ID , or the values can be passed to a program as an argument. You can override the --array flag stored in the script when you call sbatch . To do so, pass another --array flag along with the script name like below. This allows you to rerun only subsets of your array script. # submit jobs with index 0, 3, and 7 sbatch --array = 0 ,3,7 array.sh # submit jobs with index 0, 2, 4, and 6 sbatch --array = 0 -6:2 array.sh For more details on using sbatch please see the official documentation . Note If you are using bash or shell arrays, it is crucial to note they use 0-based indexing. Plan your --array flag indices accordingly.","title":"Batch Array Jobs With Known Indices"},{"location":"cheaha/slurm/submitting_jobs/#batch-array-jobs-with-dynamic-or-computed-indices","text":"For a practical example with dynamic indices, please visit our Practical sbatch Examples","title":"Batch Array Jobs With Dynamic or Computed Indices"},{"location":"cheaha/slurm/submitting_jobs/#interactive-jobs-with-srun","text":"Jobs should be submitted to the Slurm job scheduler either using a batch job or an Open OnDemand (OOD) interactive job . You can use srun for working on short interactive tasks such as creating an Anaconda environment and running parallel tasks within an sbatch script. Warning The limitations of srun is that the jobs/execution die if the internet connection is down, and you may have to rerun the job again. Let us see how to acquire a compute node quickly using srun . You can run interactive job using srun command with the --pty /bin/bash flag. Here is an example, $srun --ntasks = 2 --time = 01 :00:00 --mem-per-cpu = 8G --partition = medium --job-name = test_srun --pty /bin/bash srun: job 21648044 queued and waiting for resources srun: job 21648044 has been allocated resources The above example allocates a compute node with a 8GB of RAM on a medium partition with --ntasks=2 to run short tasks.","title":"Interactive Jobs with srun"},{"location":"cheaha/slurm/submitting_jobs/#srun-for-running-parallel-jobs","text":"srun is used to run executables in parallel, and is used within sbatch script. Let us see an example where srun is used to launch multiple (parallel) instances of a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --job-name=srun_test #SBATCH --partition=long #SBATCH --time=05:00 #SBATCH --mem=4G srun hostname In the script above, we have asked for two nodes --nodes=2, and each node will run a single instance of a hostname as we requested --ntasks-per-node=1. The output for the above script is, c0187 c0188 Here is another example of running different independent programs simultaneously on different resources within a batch job. Multiple srun can execute simultaneously as long as they do not exceed the resources reserved for that job i.e., step 1 executes in node 1 with --ntasks=4, and step 2 executes in node 2 with --ntasks=4 simultaneously. Note that --nodes=1 -r1 in step 2 defines the number of nodes and their relative node position within the resources assigned to the job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks=8 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=1 #SBATCH --partition=amd-hdr100 #SBATCH --time=05:00 #SBATCH --mem-per-cpu=1G #Partioning of resources for two different tasks #STEP 1 srun --nodes = 1 --ntasks = 4 hostname #STEP 2 srun --nodes = 1 -r1 --ntasks = 4 uname -a Here is the output for running multiple srun in a single job, i.e., executing the hostname and uname -a tasks simultaneously but on different nodes. c0203 c0203 c0203 c0203 Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Linux c0204 3 .10.0-1160.24.1.el7.x86_64 #1 SMP Thu Mar 25 21:21:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Alternatively, srun can also run MPI, OpenMP, hybrid MPI/OpenMP, and many more parallel jobs. For more details on using srun , please see the official documentation . Important srun has been disabled for use with MPI. We have removed this functionality due to an open vulnerability: https://nvd.nist.gov/vuln/detail/CVE-2023-41915 . The vulnerability could allow an attacker to escalate privileges to root and/or access data they do not have permissions for. Instead of srun , please load one of the OpenMPI modules with an appropriate version. Please contact Support with any questions or concerns.","title":"srun for running parallel jobs"},{"location":"cheaha/slurm/submitting_jobs/#graphical-interactive-jobs","text":"It is highly recommended to use the Open OnDemand web portal for interactive apps . Interactive sessions for certain software such as MATLAB and RStudio can be created directly from the browser while an HPC Desktop is available to access all of the other software on Cheaha. A terminal is also available through Open OnDemand. It is possible to use other remote desktop software, such as VNC, to start and interact with jobs. These methods are not officially supported and we do not have the capacity to help with remote desktop connections. Instead, please consider switching your workflow to use the Open OnDemand HPC Desktop . If you are unable to use this method, please contact Support .","title":"Graphical Interactive Jobs"},{"location":"cheaha/slurm/submitting_jobs/#estimating-compute-resources","text":"Being able to estimate how many resources a job will need is critical. Requesting many more resources than necessary bottlenecks the cluster by reserving unused resources for an inefficient job preventing other jobs from using them. However, requesting too few resources will slow down the job or cause it to error. Questions to ask yourself when requesting job resources: Can my scripts take advantage of multiple CPUs? For instance, RStudio generally works on a single thread. Requesting more than 1 CPU here would not improve performance. How large is the data I'm working with? Do my pipelines keep large amounts of data in memory? How long should my job take? For example, do not request 50 hours time for a 15 hour process. Have a reasonable buffer included to account for unexpected processing delays, but do not request the maximum time on a partition if that's unnecessary. Note Reasonable overestimation of resources is better than underestimation. However, gross overestimation may cause admins to contact you about adjusting resources for future jobs. To get the most out of your Cheaha experience and ensure your jobs get through the queue as fast as possible, please read about Job Efficiency .","title":"Estimating Compute Resources"},{"location":"cheaha/slurm/submitting_jobs/#faster-queuing-with-job-efficiency","text":"Please see our page on Job Efficiency for more information on making the best use of cluster resources to minimize your queue wait times.","title":"Faster Queuing with Job Efficiency"},{"location":"cheaha/software/modules/","text":"Modules and Applications \u00b6 Most software available on Cheaha is installed as modules, managed by the Lmod system. This document will provide a basic rundown of using Lmod commands to customize a software environment. module is the main command used to interface with module files in Lmod. Listing and Searching Modules \u00b6 To begin, all module commands are run from the terminal. To know what software is installed on Cheaha, use the avail command. module avail If you need to know what software is already loaded in your environment, run: module list If there is specific software you want to search for, you can use the spider subcommand, and provide a string or regular expression to match against. All modules containing the string (case-insensitive) or matching the regular expression will be returned along with their installed versions. # list modules containing string module spider <string> # list modules matching a regular expression module -r spider <regex> Loading Modules \u00b6 To load modules, run: module load module1 module2 ... Note If you only specify a module name without an accompanying version tag, the most recently installed version will be loaded into the workspace. If your scripts depend on specific versions of software being used, explicitly load the module version you need. To unload packages, run: module unload package1 package2 ... If you want to revert to the default modules, you can use: module reset Best Practice for Loading Modules \u00b6 To reduce unexpected behavior and/or to get rid of Lmod errors, Avoid using module load in $HOME/.bashrc . Instead, create a bash script with the module load commands and source it each time to load the modules needed in a shell/ sbatch script . Here is an example of loading module in a bash script named module_test.sh and compilation, #!/bin/bash module reset module load Bowtie/1.1.2-foss-2016a module load SAMtools/1.3.1-foss-2016a module load TopHat/2.1.1-foss-2016a module -t list $ chmod +x module_test.sh $ source ./module_test.sh Resetting modules to system default # Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules GCCcore/4.9.3 binutils/2.25-GCCcore-4.9.3 GCC/4.9.3-2.25 numactl/2.0.11-GCC-4.9.3-2.25 hwloc/1.11.2-GCC-4.9.3-2.25 OpenMPI/1.10.2-GCC-4.9.3-2.25 OpenBLAS/0.2.15-GCC-4.9.3-2.25-LAPACK-3.6.0 gompi/2016a FFTW/3.3.4-gompi-2016a ScaLAPACK/2.0.2-gompi-2016a-OpenBLAS-0.2.15-LAPACK-3.6.0 foss/2016a Bowtie/1.1.2-foss-2016a ncurses/6.0-foss-2016a zlib/1.2.8-foss-2016a SAMtools/1.3.1-foss-2016a bzip2/1.0.6-foss-2016a Boost/1.61.0-foss-2016a TopHat/2.1.1-foss-2016a Before loading modules in a shell/bash/sbatch script, use a clean shell by using module reset at the beginning to restore to default system settings. Using module reset before loading modules separates what software is loaded in the working shell from the software loaded in the script shell. Be aware that forked processes (like scripts) and Slurm commands inherit the environment variables of the working shell, including loaded modules. Here is an example that shows module conflict between cuda11.8 and cuda11.4 versions that may lead to unexpected behavior or an erroneous output. # Working shell where you may try testing module load and your run script $ module load cuda11.4/toolkit $ module -t list #Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules cuda11.4/toolkit/11.4.2 # bash script you are passing in a sbatch script #!/bin/bash module load cuda11.8/toolkit module -t list # Not using `module reset` at the beginning of the bash script could cause CUDA conflict issues. $ source ./module_test2.sh #Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules cuda11.4/toolkit/11.4.2 cuda11.8/toolkit/11.8.0 Note The best practice would be to avoid using module reset in the Environment Setup of Open OnDemand jobs as the OOD session, by default, resets the module at the beginning of every session. It is observed to cause unexpected behavior if module reset is used in the Rstudio server OOD sessions. Licensed and Commercial Software Restrictions \u00b6 The following software have license restrictions that may preclude some researchers or collaborators depending on their departmental or group affiliations. In the table, \"affiliated\" means employed by, or a student of, unless otherwise noted. External collaborators are not considered affiliated with UAB for the purposes of software licensing and access, unless otherwise noted. These software packages may be commercial paid software. If you believe you should have access to software that you do not have access to, please contact Support . Software Restrictions License Holder Ansys School of Engineering affiliated School of Engineering Gurobi One individual user per license Individuals LS-Dyna School of Engineering affiliated School of Engineering MATLAB UAB affiliated UAB Campus Parabricks 2 GPUs for researchers using RC systems Research Computing SAS UAB affiliated UAB Campus Stata UAB affiliated UAB Campus Use of these software packages without authorization may be a violation of the UAB IT Acceptable Use Policy . Security Issues \u00b6 IGV \u00b6 Danger Versions of IGV prior to 2.11.9 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of IGV >= 2.11.9 . GSEA \u00b6 Danger Versions of GSEA prior to 4.2.3 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of GSEA >= 4.2.3 .","title":"Preinstalled Modules"},{"location":"cheaha/software/modules/#modules-and-applications","text":"Most software available on Cheaha is installed as modules, managed by the Lmod system. This document will provide a basic rundown of using Lmod commands to customize a software environment. module is the main command used to interface with module files in Lmod.","title":"Modules and Applications"},{"location":"cheaha/software/modules/#listing-and-searching-modules","text":"To begin, all module commands are run from the terminal. To know what software is installed on Cheaha, use the avail command. module avail If you need to know what software is already loaded in your environment, run: module list If there is specific software you want to search for, you can use the spider subcommand, and provide a string or regular expression to match against. All modules containing the string (case-insensitive) or matching the regular expression will be returned along with their installed versions. # list modules containing string module spider <string> # list modules matching a regular expression module -r spider <regex>","title":"Listing and Searching Modules"},{"location":"cheaha/software/modules/#loading-modules","text":"To load modules, run: module load module1 module2 ... Note If you only specify a module name without an accompanying version tag, the most recently installed version will be loaded into the workspace. If your scripts depend on specific versions of software being used, explicitly load the module version you need. To unload packages, run: module unload package1 package2 ... If you want to revert to the default modules, you can use: module reset","title":"Loading Modules"},{"location":"cheaha/software/modules/#best-practice-for-loading-modules","text":"To reduce unexpected behavior and/or to get rid of Lmod errors, Avoid using module load in $HOME/.bashrc . Instead, create a bash script with the module load commands and source it each time to load the modules needed in a shell/ sbatch script . Here is an example of loading module in a bash script named module_test.sh and compilation, #!/bin/bash module reset module load Bowtie/1.1.2-foss-2016a module load SAMtools/1.3.1-foss-2016a module load TopHat/2.1.1-foss-2016a module -t list $ chmod +x module_test.sh $ source ./module_test.sh Resetting modules to system default # Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules GCCcore/4.9.3 binutils/2.25-GCCcore-4.9.3 GCC/4.9.3-2.25 numactl/2.0.11-GCC-4.9.3-2.25 hwloc/1.11.2-GCC-4.9.3-2.25 OpenMPI/1.10.2-GCC-4.9.3-2.25 OpenBLAS/0.2.15-GCC-4.9.3-2.25-LAPACK-3.6.0 gompi/2016a FFTW/3.3.4-gompi-2016a ScaLAPACK/2.0.2-gompi-2016a-OpenBLAS-0.2.15-LAPACK-3.6.0 foss/2016a Bowtie/1.1.2-foss-2016a ncurses/6.0-foss-2016a zlib/1.2.8-foss-2016a SAMtools/1.3.1-foss-2016a bzip2/1.0.6-foss-2016a Boost/1.61.0-foss-2016a TopHat/2.1.1-foss-2016a Before loading modules in a shell/bash/sbatch script, use a clean shell by using module reset at the beginning to restore to default system settings. Using module reset before loading modules separates what software is loaded in the working shell from the software loaded in the script shell. Be aware that forked processes (like scripts) and Slurm commands inherit the environment variables of the working shell, including loaded modules. Here is an example that shows module conflict between cuda11.8 and cuda11.4 versions that may lead to unexpected behavior or an erroneous output. # Working shell where you may try testing module load and your run script $ module load cuda11.4/toolkit $ module -t list #Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules cuda11.4/toolkit/11.4.2 # bash script you are passing in a sbatch script #!/bin/bash module load cuda11.8/toolkit module -t list # Not using `module reset` at the beginning of the bash script could cause CUDA conflict issues. $ source ./module_test2.sh #Currently Loaded Modules shared slurm/18.08.9 rc-base DefaultModules cuda11.4/toolkit/11.4.2 cuda11.8/toolkit/11.8.0 Note The best practice would be to avoid using module reset in the Environment Setup of Open OnDemand jobs as the OOD session, by default, resets the module at the beginning of every session. It is observed to cause unexpected behavior if module reset is used in the Rstudio server OOD sessions.","title":"Best Practice for Loading Modules"},{"location":"cheaha/software/modules/#licensed-and-commercial-software-restrictions","text":"The following software have license restrictions that may preclude some researchers or collaborators depending on their departmental or group affiliations. In the table, \"affiliated\" means employed by, or a student of, unless otherwise noted. External collaborators are not considered affiliated with UAB for the purposes of software licensing and access, unless otherwise noted. These software packages may be commercial paid software. If you believe you should have access to software that you do not have access to, please contact Support . Software Restrictions License Holder Ansys School of Engineering affiliated School of Engineering Gurobi One individual user per license Individuals LS-Dyna School of Engineering affiliated School of Engineering MATLAB UAB affiliated UAB Campus Parabricks 2 GPUs for researchers using RC systems Research Computing SAS UAB affiliated UAB Campus Stata UAB affiliated UAB Campus Use of these software packages without authorization may be a violation of the UAB IT Acceptable Use Policy .","title":"Licensed and Commercial Software Restrictions"},{"location":"cheaha/software/modules/#security-issues","text":"","title":"Security Issues"},{"location":"cheaha/software/modules/#igv","text":"Danger Versions of IGV prior to 2.11.9 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of IGV >= 2.11.9 .","title":"IGV"},{"location":"cheaha/software/modules/#gsea","text":"Danger Versions of GSEA prior to 4.2.3 use a compromised version of log4j. Those versions are affected by a serious remote code execution issue . Please transition your software to use versions of GSEA >= 4.2.3 .","title":"GSEA"},{"location":"cheaha/software/software/","text":"Software Installation \u00b6 Anaconda on Cheaha \u00b6 For additional general information on using Anaconda please see Anaconda Environments . If you are using Jupyter Notebook, please see our section on Packages for Jupyter . Loading Anaconda \u00b6 Anaconda is installed on Cheaha as a family of modules, and does not need to be installed by Researchers. Instead, the most recent version of Anaconda installed on Cheaha may be loaded using the command module load Anaconda3 . Other versions may be discovered using the command module avail Anaconda . We recommend always using the latest version. Note If you are using Open OnDemand Jupyter Notebook you do not need to use the module load command as part of creating the job. Using Anaconda \u00b6 Anaconda on Cheaha works like it does on any other system, once the module has been loaded, with a couple of important differences in the callouts below. Note The base environment is installed in a shared location and cannot be modified by researchers. Other environments are installed in your home directory by default. Important Only create environments on compute nodes. Anaconda environment creation consumes substantial resources and should not be run on the login node. Warning The Cheaha operating system has a version of Python installed. This version is used by python calls when Anaconda has not been loaded. This can cause unexpected errors. Be sure you've loaded the Anaconda environment you need before using Python. Danger Do not use conda init on Cheaha! Anaconda is managed as a module , including script setup. Using conda init at any point can cause hard-to-diagnose issues with Open OnDemand Interactive Jobs . Please see this ask.ci FAQ for how to undo what conda init does. If the Anaconda software instructs you to use conda init while on Cheaha, please ignore it to avoid future issues with Open OnDemand . For more information on usage with examples, see Anaconda Environments . Speedups Using Mamba \u00b6 Use of Mamba has been deprecated on Cheaha. On Cheaha, use module load Anaconda3 and the usual conda commands instead. The backend of conda has been set to use libmamba and is now equally performant. Singularity Containers \u00b6 Containers are a very useful resource for installing software without needing administrator permission. Please read the full documentation about singularity and containers on our main Singularity page .","title":"Self-Installed Software"},{"location":"cheaha/software/software/#software-installation","text":"","title":"Software Installation"},{"location":"cheaha/software/software/#anaconda-on-cheaha","text":"For additional general information on using Anaconda please see Anaconda Environments . If you are using Jupyter Notebook, please see our section on Packages for Jupyter .","title":"Anaconda on Cheaha"},{"location":"cheaha/software/software/#loading-anaconda","text":"Anaconda is installed on Cheaha as a family of modules, and does not need to be installed by Researchers. Instead, the most recent version of Anaconda installed on Cheaha may be loaded using the command module load Anaconda3 . Other versions may be discovered using the command module avail Anaconda . We recommend always using the latest version. Note If you are using Open OnDemand Jupyter Notebook you do not need to use the module load command as part of creating the job.","title":"Loading Anaconda"},{"location":"cheaha/software/software/#using-anaconda","text":"Anaconda on Cheaha works like it does on any other system, once the module has been loaded, with a couple of important differences in the callouts below. Note The base environment is installed in a shared location and cannot be modified by researchers. Other environments are installed in your home directory by default. Important Only create environments on compute nodes. Anaconda environment creation consumes substantial resources and should not be run on the login node. Warning The Cheaha operating system has a version of Python installed. This version is used by python calls when Anaconda has not been loaded. This can cause unexpected errors. Be sure you've loaded the Anaconda environment you need before using Python. Danger Do not use conda init on Cheaha! Anaconda is managed as a module , including script setup. Using conda init at any point can cause hard-to-diagnose issues with Open OnDemand Interactive Jobs . Please see this ask.ci FAQ for how to undo what conda init does. If the Anaconda software instructs you to use conda init while on Cheaha, please ignore it to avoid future issues with Open OnDemand . For more information on usage with examples, see Anaconda Environments .","title":"Using Anaconda"},{"location":"cheaha/software/software/#speedups-using-mamba","text":"Use of Mamba has been deprecated on Cheaha. On Cheaha, use module load Anaconda3 and the usual conda commands instead. The backend of conda has been set to use libmamba and is now equally performant.","title":"Speedups Using Mamba"},{"location":"cheaha/software/software/#singularity-containers","text":"Containers are a very useful resource for installing software without needing administrator permission. Please read the full documentation about singularity and containers on our main Singularity page .","title":"Singularity Containers"},{"location":"contributing/contributor_guide/","text":"Contributor Guide \u00b6 We welcome contributions from our community. To ensure a high-quality documentation experience, we have some guidelines for contributors who wish to create. Prerequisites \u00b6 We are using Visual Studio Code (VSCode) for development with the following extensions installed. While VSCode is not required, it can help with automating formatting, linting and Anaconda environment management. VSCode may be obtained from Visual Studio Code and documentation is available at VSCode: Docs . It is assumed you have Anaconda installed on your local machine. See https://docs.anaconda.com/anaconda/install/ for more information on installing Anaconda. Required VSCode Extensions \u00b6 Python: ms-python.python (for Anaconda environment management) Markdown All in One: yzhang.markdown-all-in-one markdownlint: DavidAnson.vscode-markdownlint Prettier - Code formatter: esbenp.prettier-vscode Search for and install extensions using the extension menu (keyboard shortcut: Ctrl + Shift + X ). More information on managing extensions is available at VSCode: Extension Marketplace . Handy VSCode Hotkeys \u00b6 Open command palette: Ctrl + Shift + P Open extensions: Ctrl + Shift + X Open new terminal: Ctrl + Shift + ` Find in all files: Ctrl + Shift + F Commenting and uncommenting code using Ctrl + / For a single line, click anywhere on that line to place the caret, then press Ctrl + / For commenting a block of text, select the block, then press Ctrl + / VSCode settings.json Additions \u00b6 To make the best use of formatting extensions for this project, please add the following block to your settings.json file. These changes can be made: Within the VSCode project file in the .vscode folder, affecting only this project. To make these changes find or create the file .vscode/settings.json --OR-- To the global VSCode settings file, affecting all projects. To change the global file, press Ctrl + Shift + P to open the Command Palette, then search for Preferences: Open Settings (JSON) and append the following content. \"[html]\" : { \"editor.defaultFormatter\" : \"vscode.html-language-features\" }, \"[markdown]\" : { \"editor.defaultFormatter\" : \"yzhang.markdown-all-in-one\" , \"editor.detectIndentation\" : false , \"editor.insertSpaces\" : true , \"editor.tabSize\" : 4 , \"editor.wordWrap\" : \"on\" }, \"[yaml]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"markdown.extension.list.indentationSize\" : \"inherit\" , \"markdownlint.config\" : { \"MD046\" : { \"style\" : \"fenced\" } } Obtaining a Working Copy of the Code \u00b6 Before you can get started working on contributions, you'll need a copy of the repository. The first step, done only once, is to fork the repository in GitHub to your personal account. The repository is located at https://github.com/uabrc/uabrc.github.io . More in-depth documentation on forking can be found at GitHub: Fork a Repo . Once the fork has been created, you can clone your fork using the Command Palette ( Ctrl + Shift + P ) and Git: Clone... in VSCode, or at the command line. More information on cloning can be found at GitHub: Cloning a Repository . More information on using git can be found at our git page . Local Machine Setup (Laptop/Desktop) \u00b6 The Python extension will activate when you open any Python file. The file test.py has been added for convenience, simply open that file to activate the extension. The extension will show the currently activated environment near the bottom-left corner of the VSCode window. There is no other known way to activate the Python extension at this time. Please create an Anaconda environment using build_env.yml using the following in the VSCode terminal. conda env create -f build_env.yml Activate the environment in VSCode by clicking the currently activated environment in the bottom-left of the VSCode window. A menu will appear allowing you to select from discovered environments. You may need to reload VSCode to get the environment to appear if it was just created. To build the documentation locally, use mkdocs build in the VSCode terminal. Be sure to fix all warnings before submitting a pull request. Workflow for Contributing \u00b6 The workflow below assumes you are using VSCode and all of the prerequisites listed above. Some familiarity with git and GitHub are assumed. Create a working branch \u00b6 You'll need to create a new branch on your local machine (the working branch). The purpose of a branch is to encapsulate a coherent set of changes to add or deprecate a feature, or fix a bug. Typically each bug is associated with one issue, or a set of very closely-related issues. See our Issue Tracker for available issues. A branch should be created to resolve an issue, and lives only until the issue is resolved. Give the branch a short but meaningful name for your intended changes. For bug fixes use names like fix-broken-link or fix-page-formatting . For new features or pages, use names like feat-lts-page or feat-accessibility-improvement . Command to create new branch in terminal: git checkout -b <name> . Implement your changes \u00b6 You'll need to add, remove or otherwise modify files as appropriate to implement the changes you intend. Stage and commit changes in small units as you go. Be sure you are on the correct branch , i.e. your working branch! VSCode facilitates staging and committing files. Verify your changes \u00b6 Activate your conda environment. Open the file test.py in the repository to start the Python extension. Select the interpreter using https://code.visualstudio.com/docs/python/environments#_select-and-activate-an-environment Open a VSCode terminal using Ctrl + Shift + ` . Execute the command mkdocs serve If a new browser tab does not open automatically, use your browser to navigate to http://localhost:8000 . Ensure your changes look and function as expected. Make a pull request \u00b6 Push your local working branch to your GitHub remote repository. Navigate to the upstream repository at https://github.com/uabrc/uabrc.github.io . Click the \"Pull requests\" tab and click the \"New pull request\" button. Click the link \"compare across forks\". There are four drop-down menus. The left two drop-down menus are for the base repository and should say uabrc/uabrc.github.io and main by default. Be sure that they do. In the third drop-down menu, select your fork. In the fourth drop-down menu, select your working branch. Click the \"Create pull request\" button to open the pull request creation form. Give your pull request a concise and informative name. The name should describe what the pull request changes at a high level. In the description box, give details about what was changed at a conceptual level. The actual details of the changes can be viewed in the \"Commits\" and \"Files changed\" tabs. If you want reviewers to be able to make changes to your pull request (recommended) then leave the \"Allow edits\" checkbox checked. Wait for review \u00b6 From here your pull request will go through a review process. The following criteria are checked. No linting errors Correct formatting Image alternate text (alt text) Images must use the gallery functionality, formatted as ![!alttext](path/to/file) . Note the leading ! in the alttext. Valid internal and external links. Quality, organization and accuracy of contribution. We will do our best to check information for accuracy, as well as proofread the text. Bear in mind Research Computing staff time is limited and we are not infallible, so please double-check your pull requests! Your audience is your research colleagues at UAB and beyond, and possibly even you at a future date! Reviewing Pull Requests \u00b6 Note Currently only RC Data Science staff have permissions to review pull requests. Reviewing a pull request means obtaining a copy of the pull request branch and Verifying the Changes on your local machine or on your fork. GitHub provides a facility for obtaining pull request branches directly from the upstream repository. Add upstream remote \u00b6 Add the Upstream Remote using git remote add upstream https://github.com/uabrc/uabrc.github.io.git . Pull the pull request \u00b6 Fetch the pull request with git fetch upstream pull/<id>/head:<branch-name> . Replace <id> with the pull request id number. Replace <branch-name> with the branch name from the pull request source. Checkout the branch using git checkout <branch-name> . Follow the instructions for Verifying Changes (Optional) make modifications to the pull request. Before starting, make sure that the pull request author has allowed edits to their branch. Add the Author's Fork as a Remote . Push changes to the Author's Fork . Be sure to push to the correct remote! File Organization \u00b6 Main headings are based on UAB Research Computing services Favor placing new pages and information into an existing section over creating Approach documentation from a problem solving angle rather than a technology. Examples: Section title \"Installing Software Yourself with Anaconda\" vs \"Anaconda\" Section title \"Running Analysis Jobs\" vs \"Slurm\" Put redirects for any page moves in case someone has bookmarked a page (see Redirect section below) Markdown Formatting \u00b6 Links must be one of the following formats including all punctuation and brackets: Bare: <https://google.com> <support@listserv.uab.edu> Named with a schema: [website](https://google.com) [email](mailto:support@listserv.uab.edu) note the mailto: schema! Relative internal [relative internal](help/faq.md) All internal links must be relative. For example, use ./file.md not /docs/file.md . Redirects \u00b6 If a page name must change, or the location of a page must change, it is necessary to create a redirect for that page so we don't break bookmarks and incoming links targeting our documentation. Redirecting pages is possible using the plugin at https://github.com/datarobot/mkdocs-redirects . To redirect a page, add a line in mkdocs.yml under the following keys. The line takes the form original page location: new page location , and each side of : must be a full path under docs/ . An example is below. plugins : - redirects : redirect_maps : account_management/uab_researcher.md : account_management/cheaha_account.md Section Index Pages \u00b6 To create a section index page: Create a file called index.md in a reasonable directory location for that section. Add something like the following to that section entry under nav: in mkdocs.yml . nav: ... - Cheaha Guide: - cheaha/index.md # add the index.md here. - Hardware: ... - ... ... When a site visitor clicks Cheaha Guide in the nav pane, the page cheaha/index.md will be loaded. Linting Known Issues \u00b6 There are known issues with the markdown linter and some of our non-standard plugins, especially admonitions (specifically a conflict involving fenced vs indented code blocks). To fix these cases please use one of the following methods. The <lint warning code> can be found by hovering over the yellow squiggles in VSCode to bring up the warning lens. Please do not use these to silence all linter warnings, only for fixing known issues. Please read the warning lenses given by VSCode to identify the cause of the warning. Silence Linter Warning for a Block \u00b6 <!-- markdownlint-disable <lint warning code> --> `linter error here` `maybe multiple lines` <!-- markdownlint-enable <lint warning code> --> Silence Linter Warning for a Single Line \u00b6 We encourage denoting the warning being silenced here by filling out the <lint warning code> , though it isn't required for the single line case. <!-- markdownlint-disable-next-line <lint warning code> --> `linter error here just for this line` False Positive Lint Warnings from Admonitions \u00b6 We allow and encourage the use of admonitions in our documentation, where appropriate. Because these are created using a plugin and are \"non-standard\" markdown , the VSCode markdownlint extension does not recognize admonitions and may produce a false positive warning about inconsistent code block styles. Two styles of code block are allowed in markdown : fenced and indented . To work around the false positive warning about admonitions, we require all code blocks to be fenced . This is enforced by adding an entry to the VSCode settings.json file . Now all admonitions will be consistently assigned the warning MD046 , which can be disabled by placing all admonitions in between the following comment block fences. The comment lines must be indented to the same level as the start of the admonition. <!-- markdownlint-disable MD046 --> <!-- markdownlint-enable MD046 --> The process can be simplified in VSCode using snippets. Bring up the command palette and type snippets and open Preferences: Configure User Snippets . Then type markdown and open it. A json file will be opened. Add the following content between the outermost braces and then save the file. \"Disable Markdown Lint MD046 for a Block\" : { \"prefix\" : \"md046 disable\" , \"body\" : [ \"<!-- markdownlint-disable MD046 -->\" , \"$TM_SELECTED_TEXT\" , \"<!-- markdownlint-enable MD046 -->\" ], \"description\" : \"Disables warning Markdown Lint MD046 for the selected block.\" } The snippet will surround selected text with the appropriate linter disable fencing for MD046. To use the snippet, opne the IntelliSense lens using ctrl + space (or cmd + space ), then type md046 until the prefix shows up as the first entry in the list. Then press tab . The workaround is needed because markdownlint has no plans to add support for admonitions. There is no markdownlint plugin for that support either, and we don't have the ability to develop such a plugin. Slurm Hardware, Partitions, QoS Tables \u00b6 Building hardware tables is a semi-automated script based on a manually curated table. The repository is located here: https://gitlab.rc.uab.edu/rc-data-science/data-science-internal/cluster-fabric-docs . Be warned that the scripts may move to a github.com repo in the future. Building Partition and QoS tables is automated based on scontrol output. The repository is located here: https://github.com/wwarriner/slurm_status_tools . Be warned the link may change in the future when we fork to the UABRC group that houses the docs repo. Accessibility \u00b6 Color vision deficiency checker: https://www.toptal.com/designers/colorfilter/ Contrast checker: https://webaim.org/resources/contrastchecker/ Branding Guidance \u00b6 Brand main page: https://www.uab.edu/toolkit/branding Brand colors: https://www.uab.edu/toolkit/brand-basics/colors Copyright guidance: https://www.uab.edu/toolkit/trademarks-licensing/uab-trademarks Terminology \u00b6 Research Computing (RC) for the IT group supporting campus HPC resources. UAB Campus Network for the hard-wired network physically located on UAB campus, and the UAB Secure wifi. UAB Campus VPN for the VPN to tunnel remote connections through the UAB Campus Network.","title":"Contributing Content"},{"location":"contributing/contributor_guide/#contributor-guide","text":"We welcome contributions from our community. To ensure a high-quality documentation experience, we have some guidelines for contributors who wish to create.","title":"Contributor Guide"},{"location":"contributing/contributor_guide/#prerequisites","text":"We are using Visual Studio Code (VSCode) for development with the following extensions installed. While VSCode is not required, it can help with automating formatting, linting and Anaconda environment management. VSCode may be obtained from Visual Studio Code and documentation is available at VSCode: Docs . It is assumed you have Anaconda installed on your local machine. See https://docs.anaconda.com/anaconda/install/ for more information on installing Anaconda.","title":"Prerequisites"},{"location":"contributing/contributor_guide/#required-vscode-extensions","text":"Python: ms-python.python (for Anaconda environment management) Markdown All in One: yzhang.markdown-all-in-one markdownlint: DavidAnson.vscode-markdownlint Prettier - Code formatter: esbenp.prettier-vscode Search for and install extensions using the extension menu (keyboard shortcut: Ctrl + Shift + X ). More information on managing extensions is available at VSCode: Extension Marketplace .","title":"Required VSCode Extensions"},{"location":"contributing/contributor_guide/#handy-vscode-hotkeys","text":"Open command palette: Ctrl + Shift + P Open extensions: Ctrl + Shift + X Open new terminal: Ctrl + Shift + ` Find in all files: Ctrl + Shift + F Commenting and uncommenting code using Ctrl + / For a single line, click anywhere on that line to place the caret, then press Ctrl + / For commenting a block of text, select the block, then press Ctrl + /","title":"Handy VSCode Hotkeys"},{"location":"contributing/contributor_guide/#vscode-settingsjson-additions","text":"To make the best use of formatting extensions for this project, please add the following block to your settings.json file. These changes can be made: Within the VSCode project file in the .vscode folder, affecting only this project. To make these changes find or create the file .vscode/settings.json --OR-- To the global VSCode settings file, affecting all projects. To change the global file, press Ctrl + Shift + P to open the Command Palette, then search for Preferences: Open Settings (JSON) and append the following content. \"[html]\" : { \"editor.defaultFormatter\" : \"vscode.html-language-features\" }, \"[markdown]\" : { \"editor.defaultFormatter\" : \"yzhang.markdown-all-in-one\" , \"editor.detectIndentation\" : false , \"editor.insertSpaces\" : true , \"editor.tabSize\" : 4 , \"editor.wordWrap\" : \"on\" }, \"[yaml]\" : { \"editor.defaultFormatter\" : \"esbenp.prettier-vscode\" }, \"markdown.extension.list.indentationSize\" : \"inherit\" , \"markdownlint.config\" : { \"MD046\" : { \"style\" : \"fenced\" } }","title":"VSCode settings.json Additions"},{"location":"contributing/contributor_guide/#obtaining-a-working-copy-of-the-code","text":"Before you can get started working on contributions, you'll need a copy of the repository. The first step, done only once, is to fork the repository in GitHub to your personal account. The repository is located at https://github.com/uabrc/uabrc.github.io . More in-depth documentation on forking can be found at GitHub: Fork a Repo . Once the fork has been created, you can clone your fork using the Command Palette ( Ctrl + Shift + P ) and Git: Clone... in VSCode, or at the command line. More information on cloning can be found at GitHub: Cloning a Repository . More information on using git can be found at our git page .","title":"Obtaining a Working Copy of the Code"},{"location":"contributing/contributor_guide/#local-machine-setup-laptopdesktop","text":"The Python extension will activate when you open any Python file. The file test.py has been added for convenience, simply open that file to activate the extension. The extension will show the currently activated environment near the bottom-left corner of the VSCode window. There is no other known way to activate the Python extension at this time. Please create an Anaconda environment using build_env.yml using the following in the VSCode terminal. conda env create -f build_env.yml Activate the environment in VSCode by clicking the currently activated environment in the bottom-left of the VSCode window. A menu will appear allowing you to select from discovered environments. You may need to reload VSCode to get the environment to appear if it was just created. To build the documentation locally, use mkdocs build in the VSCode terminal. Be sure to fix all warnings before submitting a pull request.","title":"Local Machine Setup (Laptop/Desktop)"},{"location":"contributing/contributor_guide/#workflow-for-contributing","text":"The workflow below assumes you are using VSCode and all of the prerequisites listed above. Some familiarity with git and GitHub are assumed.","title":"Workflow for Contributing"},{"location":"contributing/contributor_guide/#create-a-working-branch","text":"You'll need to create a new branch on your local machine (the working branch). The purpose of a branch is to encapsulate a coherent set of changes to add or deprecate a feature, or fix a bug. Typically each bug is associated with one issue, or a set of very closely-related issues. See our Issue Tracker for available issues. A branch should be created to resolve an issue, and lives only until the issue is resolved. Give the branch a short but meaningful name for your intended changes. For bug fixes use names like fix-broken-link or fix-page-formatting . For new features or pages, use names like feat-lts-page or feat-accessibility-improvement . Command to create new branch in terminal: git checkout -b <name> .","title":"Create a working branch"},{"location":"contributing/contributor_guide/#implement-your-changes","text":"You'll need to add, remove or otherwise modify files as appropriate to implement the changes you intend. Stage and commit changes in small units as you go. Be sure you are on the correct branch , i.e. your working branch! VSCode facilitates staging and committing files.","title":"Implement your changes"},{"location":"contributing/contributor_guide/#verify-your-changes","text":"Activate your conda environment. Open the file test.py in the repository to start the Python extension. Select the interpreter using https://code.visualstudio.com/docs/python/environments#_select-and-activate-an-environment Open a VSCode terminal using Ctrl + Shift + ` . Execute the command mkdocs serve If a new browser tab does not open automatically, use your browser to navigate to http://localhost:8000 . Ensure your changes look and function as expected.","title":"Verify your changes"},{"location":"contributing/contributor_guide/#make-a-pull-request","text":"Push your local working branch to your GitHub remote repository. Navigate to the upstream repository at https://github.com/uabrc/uabrc.github.io . Click the \"Pull requests\" tab and click the \"New pull request\" button. Click the link \"compare across forks\". There are four drop-down menus. The left two drop-down menus are for the base repository and should say uabrc/uabrc.github.io and main by default. Be sure that they do. In the third drop-down menu, select your fork. In the fourth drop-down menu, select your working branch. Click the \"Create pull request\" button to open the pull request creation form. Give your pull request a concise and informative name. The name should describe what the pull request changes at a high level. In the description box, give details about what was changed at a conceptual level. The actual details of the changes can be viewed in the \"Commits\" and \"Files changed\" tabs. If you want reviewers to be able to make changes to your pull request (recommended) then leave the \"Allow edits\" checkbox checked.","title":"Make a pull request"},{"location":"contributing/contributor_guide/#wait-for-review","text":"From here your pull request will go through a review process. The following criteria are checked. No linting errors Correct formatting Image alternate text (alt text) Images must use the gallery functionality, formatted as ![!alttext](path/to/file) . Note the leading ! in the alttext. Valid internal and external links. Quality, organization and accuracy of contribution. We will do our best to check information for accuracy, as well as proofread the text. Bear in mind Research Computing staff time is limited and we are not infallible, so please double-check your pull requests! Your audience is your research colleagues at UAB and beyond, and possibly even you at a future date!","title":"Wait for review"},{"location":"contributing/contributor_guide/#reviewing-pull-requests","text":"Note Currently only RC Data Science staff have permissions to review pull requests. Reviewing a pull request means obtaining a copy of the pull request branch and Verifying the Changes on your local machine or on your fork. GitHub provides a facility for obtaining pull request branches directly from the upstream repository.","title":"Reviewing Pull Requests"},{"location":"contributing/contributor_guide/#add-upstream-remote","text":"Add the Upstream Remote using git remote add upstream https://github.com/uabrc/uabrc.github.io.git .","title":"Add upstream remote"},{"location":"contributing/contributor_guide/#pull-the-pull-request","text":"Fetch the pull request with git fetch upstream pull/<id>/head:<branch-name> . Replace <id> with the pull request id number. Replace <branch-name> with the branch name from the pull request source. Checkout the branch using git checkout <branch-name> . Follow the instructions for Verifying Changes (Optional) make modifications to the pull request. Before starting, make sure that the pull request author has allowed edits to their branch. Add the Author's Fork as a Remote . Push changes to the Author's Fork . Be sure to push to the correct remote!","title":"Pull the pull request"},{"location":"contributing/contributor_guide/#file-organization","text":"Main headings are based on UAB Research Computing services Favor placing new pages and information into an existing section over creating Approach documentation from a problem solving angle rather than a technology. Examples: Section title \"Installing Software Yourself with Anaconda\" vs \"Anaconda\" Section title \"Running Analysis Jobs\" vs \"Slurm\" Put redirects for any page moves in case someone has bookmarked a page (see Redirect section below)","title":"File Organization"},{"location":"contributing/contributor_guide/#markdown-formatting","text":"Links must be one of the following formats including all punctuation and brackets: Bare: <https://google.com> <support@listserv.uab.edu> Named with a schema: [website](https://google.com) [email](mailto:support@listserv.uab.edu) note the mailto: schema! Relative internal [relative internal](help/faq.md) All internal links must be relative. For example, use ./file.md not /docs/file.md .","title":"Markdown Formatting"},{"location":"contributing/contributor_guide/#redirects","text":"If a page name must change, or the location of a page must change, it is necessary to create a redirect for that page so we don't break bookmarks and incoming links targeting our documentation. Redirecting pages is possible using the plugin at https://github.com/datarobot/mkdocs-redirects . To redirect a page, add a line in mkdocs.yml under the following keys. The line takes the form original page location: new page location , and each side of : must be a full path under docs/ . An example is below. plugins : - redirects : redirect_maps : account_management/uab_researcher.md : account_management/cheaha_account.md","title":"Redirects"},{"location":"contributing/contributor_guide/#section-index-pages","text":"To create a section index page: Create a file called index.md in a reasonable directory location for that section. Add something like the following to that section entry under nav: in mkdocs.yml . nav: ... - Cheaha Guide: - cheaha/index.md # add the index.md here. - Hardware: ... - ... ... When a site visitor clicks Cheaha Guide in the nav pane, the page cheaha/index.md will be loaded.","title":"Section Index Pages"},{"location":"contributing/contributor_guide/#linting-known-issues","text":"There are known issues with the markdown linter and some of our non-standard plugins, especially admonitions (specifically a conflict involving fenced vs indented code blocks). To fix these cases please use one of the following methods. The <lint warning code> can be found by hovering over the yellow squiggles in VSCode to bring up the warning lens. Please do not use these to silence all linter warnings, only for fixing known issues. Please read the warning lenses given by VSCode to identify the cause of the warning.","title":"Linting Known Issues"},{"location":"contributing/contributor_guide/#silence-linter-warning-for-a-block","text":"<!-- markdownlint-disable <lint warning code> --> `linter error here` `maybe multiple lines` <!-- markdownlint-enable <lint warning code> -->","title":"Silence Linter Warning for a Block"},{"location":"contributing/contributor_guide/#silence-linter-warning-for-a-single-line","text":"We encourage denoting the warning being silenced here by filling out the <lint warning code> , though it isn't required for the single line case. <!-- markdownlint-disable-next-line <lint warning code> --> `linter error here just for this line`","title":"Silence Linter Warning for a Single Line"},{"location":"contributing/contributor_guide/#false-positive-lint-warnings-from-admonitions","text":"We allow and encourage the use of admonitions in our documentation, where appropriate. Because these are created using a plugin and are \"non-standard\" markdown , the VSCode markdownlint extension does not recognize admonitions and may produce a false positive warning about inconsistent code block styles. Two styles of code block are allowed in markdown : fenced and indented . To work around the false positive warning about admonitions, we require all code blocks to be fenced . This is enforced by adding an entry to the VSCode settings.json file . Now all admonitions will be consistently assigned the warning MD046 , which can be disabled by placing all admonitions in between the following comment block fences. The comment lines must be indented to the same level as the start of the admonition. <!-- markdownlint-disable MD046 --> <!-- markdownlint-enable MD046 --> The process can be simplified in VSCode using snippets. Bring up the command palette and type snippets and open Preferences: Configure User Snippets . Then type markdown and open it. A json file will be opened. Add the following content between the outermost braces and then save the file. \"Disable Markdown Lint MD046 for a Block\" : { \"prefix\" : \"md046 disable\" , \"body\" : [ \"<!-- markdownlint-disable MD046 -->\" , \"$TM_SELECTED_TEXT\" , \"<!-- markdownlint-enable MD046 -->\" ], \"description\" : \"Disables warning Markdown Lint MD046 for the selected block.\" } The snippet will surround selected text with the appropriate linter disable fencing for MD046. To use the snippet, opne the IntelliSense lens using ctrl + space (or cmd + space ), then type md046 until the prefix shows up as the first entry in the list. Then press tab . The workaround is needed because markdownlint has no plans to add support for admonitions. There is no markdownlint plugin for that support either, and we don't have the ability to develop such a plugin.","title":"False Positive Lint Warnings from Admonitions"},{"location":"contributing/contributor_guide/#slurm-hardware-partitions-qos-tables","text":"Building hardware tables is a semi-automated script based on a manually curated table. The repository is located here: https://gitlab.rc.uab.edu/rc-data-science/data-science-internal/cluster-fabric-docs . Be warned that the scripts may move to a github.com repo in the future. Building Partition and QoS tables is automated based on scontrol output. The repository is located here: https://github.com/wwarriner/slurm_status_tools . Be warned the link may change in the future when we fork to the UABRC group that houses the docs repo.","title":"Slurm Hardware, Partitions, QoS Tables"},{"location":"contributing/contributor_guide/#accessibility","text":"Color vision deficiency checker: https://www.toptal.com/designers/colorfilter/ Contrast checker: https://webaim.org/resources/contrastchecker/","title":"Accessibility"},{"location":"contributing/contributor_guide/#branding-guidance","text":"Brand main page: https://www.uab.edu/toolkit/branding Brand colors: https://www.uab.edu/toolkit/brand-basics/colors Copyright guidance: https://www.uab.edu/toolkit/trademarks-licensing/uab-trademarks","title":"Branding Guidance"},{"location":"contributing/contributor_guide/#terminology","text":"Research Computing (RC) for the IT group supporting campus HPC resources. UAB Campus Network for the hard-wired network physically located on UAB campus, and the UAB Secure wifi. UAB Campus VPN for the VPN to tunnel remote connections through the UAB Campus Network.","title":"Terminology"},{"location":"contributing/reporting_errors/","text":"Reporting Documentation Errors \u00b6 Important This page is only for reporting errors with the documentation. For issues related to our computing services, please contact Support . Our documentation is hosted on GitHub at https://github.com/uabrc/uabrc.github.io . To report errors, make requests, and contribute to the documentation, you'll need to Create a GitHub Account if you do not have one already. How Do I Report Inaccurate Information? \u00b6 To report inaccurate information please create an Inaccuracy Report . How Do I Report a Bug? \u00b6 Bug reports are intended to be issues with page appearance and feature functionality. They are not intended for inaccurate information. See [Report Inaccurate Information] to report inaccuracies. To report a bug please create a Bug Report . How Do I Request an Article or Section? \u00b6 To request an article or section, please create an Article Request . How Can I Fix It Myself? \u00b6 To get started creating content, please see our Contributor Guide . We assume you are already somewhat familiar with Git, GitHub, Markdown, and writing technical information for an educated audience with diverse backgrounds and expertise.","title":"Reporting Documentation Errors"},{"location":"contributing/reporting_errors/#reporting-documentation-errors","text":"Important This page is only for reporting errors with the documentation. For issues related to our computing services, please contact Support . Our documentation is hosted on GitHub at https://github.com/uabrc/uabrc.github.io . To report errors, make requests, and contribute to the documentation, you'll need to Create a GitHub Account if you do not have one already.","title":"Reporting Documentation Errors"},{"location":"contributing/reporting_errors/#how-do-i-report-inaccurate-information","text":"To report inaccurate information please create an Inaccuracy Report .","title":"How Do I Report Inaccurate Information?"},{"location":"contributing/reporting_errors/#how-do-i-report-a-bug","text":"Bug reports are intended to be issues with page appearance and feature functionality. They are not intended for inaccurate information. See [Report Inaccurate Information] to report inaccuracies. To report a bug please create a Bug Report .","title":"How Do I Report a Bug?"},{"location":"contributing/reporting_errors/#how-do-i-request-an-article-or-section","text":"To request an article or section, please create an Article Request .","title":"How Do I Request an Article or Section?"},{"location":"contributing/reporting_errors/#how-can-i-fix-it-myself","text":"To get started creating content, please see our Contributor Guide . We assume you are already somewhat familiar with Git, GitHub, Markdown, and writing technical information for an educated audience with diverse backgrounds and expertise.","title":"How Can I Fix It Myself?"},{"location":"data_management/code_storage/","text":"Code Storage \u00b6 Unlike with traditional raw data storage, code evolves over time, often rapidly. Rapid changes are inevitable and commonplace in academia. Scientific research demands reproducibility and accountability, especially with methodology. Code you write is part of your methodology, so it should be carefully tracked and documented as it evolves, the same way you keep lab notebooks and document how you performed experiments. Git is the foundational tool on which to build code reproducibility and accountability. GitHub and GitLab provide central, internet-based locations in which to store your code. UAB GitLab vs the third-party GitHub: which should I choose? \u00b6 If your work is private, or internal to UAB operations, it is probably best to store it in our GitLab instance, hosted on-premises as UAB. In contrast, if you must collaborate with external researchers, it might make more sense to store code in GitHub , instead. GitLab has more feature-rich project management tooling in support of code collaboration, but GitHub is more widely known, recognized and can more readily travel with you as you progress in your career. GitLab \u00b6 UAB Research Computing maintains a GitLab instance. To gain access, please visit https://gitlab.rc.uab.edu and create an Account . GitHub \u00b6 Please visit https://github.com . For UAB Research Computing's GitHub repositories, please see our Social Media page. How do I effectively use Git, GitHub, and GitLab for collaboration? \u00b6 Please see our page on Collaborating with Git .","title":"Code Storage"},{"location":"data_management/code_storage/#code-storage","text":"Unlike with traditional raw data storage, code evolves over time, often rapidly. Rapid changes are inevitable and commonplace in academia. Scientific research demands reproducibility and accountability, especially with methodology. Code you write is part of your methodology, so it should be carefully tracked and documented as it evolves, the same way you keep lab notebooks and document how you performed experiments. Git is the foundational tool on which to build code reproducibility and accountability. GitHub and GitLab provide central, internet-based locations in which to store your code.","title":"Code Storage"},{"location":"data_management/code_storage/#uab-gitlab-vs-the-third-party-github-which-should-i-choose","text":"If your work is private, or internal to UAB operations, it is probably best to store it in our GitLab instance, hosted on-premises as UAB. In contrast, if you must collaborate with external researchers, it might make more sense to store code in GitHub , instead. GitLab has more feature-rich project management tooling in support of code collaboration, but GitHub is more widely known, recognized and can more readily travel with you as you progress in your career.","title":"UAB GitLab vs the third-party GitHub: which should I choose?"},{"location":"data_management/code_storage/#gitlab","text":"UAB Research Computing maintains a GitLab instance. To gain access, please visit https://gitlab.rc.uab.edu and create an Account .","title":"GitLab"},{"location":"data_management/code_storage/#github","text":"Please visit https://github.com . For UAB Research Computing's GitHub repositories, please see our Social Media page.","title":"GitHub"},{"location":"data_management/code_storage/#how-do-i-effectively-use-git-github-and-gitlab-for-collaboration","text":"Please see our page on Collaborating with Git .","title":"How do I effectively use Git, GitHub, and GitLab for collaboration?"},{"location":"data_management/storage/","text":"Storage \u00b6 What Type of Storage Do I Need? \u00b6 There are multiple locations for data storage both on and off Cheaha each with a specific purpose. You can look at the table below to help determine the storage platform we provide that best matches your needed use-case. If you need additional assistance, please contact Support . Platform Long-term Storage User Data and Home Directories Project Directories Global Scratch Local Scratch Data Use-Case Data that rarely or never changes. Static data hosting. Acquired data pick-up repository. General-purpose data storage. Data to be shared among collaborator group and ready for analysis. Temporary files created during analysis. Rolling file deletion over time. Small files created during jobs. Must be deleted at the end of a job. Default Quota Individual: 5 TB Group: 75 TB 5 TB 25 TB 100 TB Small Owner Individual: per researcher Group: Lab PI or Core Director Individual Researchers Lab PI or Core Director Individual Researchers Individual Researchers Accessible From - Cheaha - Globus - Cheaha - Globus - Cheaha - Globus Cheaha Cheaha Cheaha Path No path; see LTS Interfaces . /home/<blazerid> ( $HOME ) and /data/user/<blazerid> ( $USER_DATA ) /data/project/<name> /scratch/<blazerid> ( $USER_SCRATCH ) /local/$SLURM_JOB_ID Read/Write (IO) Speed Slower Fast Fast Fast Fastest Responsibilities & Procedures PI/Director responsible for data and access control. PI/Director responsible for data and access control. Data deleted after 30 days. Data deleted as needed. Access Control Bucket policies Self only chmod and ACLs Self only Self only How to Request Upon request. Comes with Cheaha account. Upon request. Comes with Cheaha account. Comes with Cheaha account. What Individual Storage Solutions are Available? \u00b6 Every researcher has personal directories found at /home/$USER (or $HOME ) and /data/user/$USER (or $USER_DATA ), which are created automatically during account registration. These two locations are meant to store general data and can be used during active analysis. While there are no data retention policies in place, these spaces are not intended for long-term storage of data that changes infrequently. Traditionally, $HOME is intended to store scripts, supporting files, software configuration files, and toolboxes such as Anaconda virtual environments or R packages. In contrast, $USER_DATA is intended to store datasets and results for individual research projects. Individual allocations on Long-Term Storage (LTS) are also available upon request. How Do I Request Individual Long-Term Storage? \u00b6 To request individual Long-Term Storage, please first read and understand how Long-Term Storage differs from traditional filesystems, like GPFS on Cheaha. Decide if it is suitable for your needs. Then please feel free to contact Support . What Shared Storage Solutions are Available? \u00b6 Shared Storage is available via two services. We have Project Storage (located in /data/project or Cheaha) and Long-Term Storage (LTS) . The two offerings are suited to different sets of use-cases, so please read on to determine which may be most suitable. Project Storage is best-suited for changing or dynamic data. Specifically: Data needing/undergoing analysis Exploratory data Temporary data needed longer than 30 days In contrast, Long-Term Storage is best-suited for unchanging or static data. Specifically: Instrument-acquired data Completed analyses Hosting data for others to copy Hosting data for the public internet \"Pick-up\" and \"drop-off\" locations for data as part of a workflow Shared Storage is available for labs headed by a PI and for Core facilities headed by a director. Shared Storage is allocated on a per-organization basis, not on a per-person basis. If an individual researcher manages both a lab and a Core, they may request independent storage allocations for each organization. Each organization may request both Project Storage and Long-Term Storage. How Do I Request Shared Storage? \u00b6 To request shared Project Storage or Long-Term Storage, please contact Support . To ensure prompt allocation of Shared Storage, please follow the guidelines below. Requests must be made to support@listserv.uab.edu or via the AskIT HelpDesk . Requests must come from one of the proposed owners. All proposed owners must have created their Research Computing accounts at the time the request is made. Please provide the following information. Missing information can delay allocation of Shared Storage as we either look up the information, or ask followup questions. Responsible Party/Owner: The BlazerID of the person claiming reponsibility for what happens and what is stored in the space. Typically this would be a Principal Investigator (PI) or a Core Director. Multiple responsible parties are allowed. We need one person declared as \"primary\" owner. This person will be the literal owner (in the Linux sense) for Project Storage. Members: A list of BlazerIDs of people to give access to the space. (Note: this only applies to Project Storage. LTS access controls are managed differently.) Type of Organization: Is the Shared Storage request for a lab, core, campus administrative group, or something else? Name of Organization: The specific name of the organization the Shared Storage request is for. Parent Organization: The name of the parent organization for your organization. Please be as detailed as possible. Purpose of Shared Storage: The research purpose for the storage, how do you intend to use it? Please feel free to be as detailed as you like, but please limit to a few sentences at most. Internal UAB Collaborator Organizations: The name(s) of any other UAB organizations participating in the Shared Storage. External Collaborator Organizations: The name(s) of any external organizations participating in the Shared Storage. Regulatory Requirements: List any regulatory requirements or agencies affecting data to be stored in the space. Possibilities include, but are not limited to: IRB, EHR, HIPAA, PHI, FERPA. Name of Shared Storage: Please give us a short, memorable name that is specific to your organization but general to your purpose. For Project Storage, this name will be used in the /data/project/<name> path on Cheaha. For Labs we recommend the format <PI_blazerid>-lab . For Cores we recommend a shortened version of the Core name. If some members have not created their accounts at the time of the request, we will proceed with allocating the Shared Storage. Additional members may be added at a later time in a new service request. How Do I Make Changes to Shared Storage Membership? \u00b6 To request changes in Shared Storage membership, please contact Support . Please take note of the following guidelines to ensure changes can be made promptly. We must have written approval from an owner to make membership changes. The exact name of the Shared Storage. If it is Project Storage, the path to the storage location, i.e., /data/project/... . Please give BlazerIDs of members to add or remove. Project Directory Permissions \u00b6 Every project directory has a group that is unique system-wide, and not used anywhere else on the filesystem. The unique project group will be referred to as <grp> and generally has the same name as the top level project directory. Note Some early group names may not match their project directory, but should be reasonably close. Members of the project directory group have permissions to access that project directory. Adding and removing members from the project directory group is how Research Computing controls access to, and ownership of, project directories. We do not use access control lists (ACLs) to manage permissions ourselves, but use of ACLs is allowed and encouraged for PIs and project administrators who want more fine-grained control. Please see our section on ACLs for more information. Be default, project space permissions are set up in the following way: Top level directory Newly created files Newly created directories Numeric Permissions 2770 0664 2775 Symbolic Permissions drwxrws--- -rw-rw-r-- drwxrwsr-x setgid enabled Yes Yes User owner PI/Admin Creator Creator Group owner <grp> <grp> <grp> Having setgid enabled on directories means new files and directories created within will inherit group ownership and the setgid bit. The setgid bit is reflected by the 2 in the numeric permissions and the s in the symbolic permissions. The setgid bit and per-directory project groups is how Research Computing controls access to each project directory. There are some known issues surrounding project directory permissions when files are put into the project directory. Different commands have different behaviors. The following list describes the behaviors of various commands used to move and copy data, as well as good practices. mv maintains all permissions and ownerships of the source file or directory. For files and directories created outside the project directory, avoid using mv , prefer cp or similar instead. See below for alternatives. For files and directories created within the project directory, mv may work, but be sure the file has correct permissions and group ownership. cp , tar -x , rsync , rclone , sftp and Globus all behave as though creating a new file at the target location, by default. Prefer these commands when it is sensible to do so. Avoid using the -p flag with cp , tar , rsync and sftp . When using the -p flag, files and directories will retain their source permissions. Retaining source permissions in project directories is undesirable behavior and can create headaches for you, your colleagues, and your project directory administrators and PIs. For PIs and project administrators: Please educate your staff and collaborators about the above permission setups, and any additional ACLs you may have in place, to minimize future challenges. If you have issues with permissions, please contact Support . We can guide you through Managing Permissions and Managing Group Ownership . Scratch \u00b6 Two types of scratch space are provided for analyses currently being ran, network-mounted and local. These are spaces shared across users (though one user still cannot access another user's files without permission) and as such, data should be moved out of scratch when the analysis is finished. Important Starting January 2023, scratch data will have limited retention. See Scratch Retention Policy for more information. User Scratch \u00b6 All users have access to a large, temporary, work-in-progress directory for storing data, called a scratch directory in /scratch/$USER or $USER_SCRATCH . Use this directory to store very large datasets or temporary pipeline intermediates for a short period of time while running your jobs. The maximum amount of data a single user can store in network scratch is 100 TB at once. Network scratch is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 1 PB of storage. If using scratch, this should be your jobs' primary working directory, unless the job would benefit from local scratch (see below). Warning Research Computing expects each user to keep their scratch areas clean. The cluster scratch areas are not to be used for archiving data. In order to keep scratch clear and usable for everyone, files older than 30 days will be automatically deleted. Local Scratch \u00b6 Each compute node has a local scratch directory that is accessible via /local/$SLURM_JOB_ID . At this time, you will need to create the directory manually using mkdir -p /local/$SLURM_JOB_ID . If your job performs a lot of file I/O, the job should use /local/$SLURM_JOB_ID rather than $USER_SCRATCH to prevent bogging down the network scratch file system. It's important to recognize that most jobs run on the cluster do not fall under this category. If you are using amperenodes and the A100 GPUs, then it is highly recommended to move your input files to /local/$SLURM_JOB_ID prior to running your workflow, to ensure adequate GPU performance. Using $USER_SCRATCH , or other network file locations, will starve the GPU of data, resulting in poor performance. For more information please see Ensuring IO Performance With A100 GPUs . Be sure to clean up /local/$SLURM_JOB_ID after your job is complete! An example script to automate this process is shown below. #!/bin/bash #SBATCH ... # LOAD MODULES # module load ... # CREATE TEMPORARY DIRECTORY # WARNING! $TMPDIR will be deleted at the end of the script! # Changing the following line can cause permanent, unintended deletion of important data. TMPDIR = \"/local/ $USER / $SLURM_JOB_ID \" mkdir -p \" $TMPDIR \" # COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY # Replace $MY_DATA_DIR with the path to your data folder cp -r \" $MY_DATA_DIR \" \" $TMPDIR \" # YOUR ORIGINAL WORKFLOW GOES HERE # be sure to load files from \"$TMPDIR\"! # CLEAN UP TEMPORARY DIRECTORY # WARNING! # Changing the following line can cause permanent, unintended deletion of important data. rm -rf \" $TMPDIR \" Important Using /local/$SLURM_JOB_ID with MPI jobs takes additional consideration. If you do not need MPI, please use the #SBATCH --nodes=1 slurm directive to specify that all requested cores are on the same node. If you need the performance of /local/$SLURM_JOB_ID in an MPI job, please contact Support and read about the Slurm commands sbcast and sgather . Temporary Files ( tmp ) \u00b6 Please do not use the directory tmp as storage for temporary files. The tmp directory is local to each node, and a full tmp directory harms compute performance on that node for all users. Instead, please use /local/$SLURM_JOB_ID for fast access and $USER_SCRATCH for larger space. Some software defaults to using tmp without any warning or documentation, especially software designed for personal computers. We may reach out to inform you if your software fills tmp , as it can harm performance on that compute node. If that happens we will work with you to identify ways of redirecting temporary storage to one of the scratch spaces. Software Known to Use tmp \u00b6 The following software are known to use tmp by default, and can be worked around by using the listed flags. See Local Scratch for more information about creating a local temporary directory. Java : java * -Djava.io.tmpdir=/local/$SLURM_JOB_ID UMI Tools : umi_tools * --temp-dir=/local/$SLURM_JOB_ID Samtools Sort : samtools sort * -T /local/$SLURM_JOB_ID GATK Tool : gatk --java-options * --tmp-dir /local/$SLURM_JOB_ID Parabricks : pbrun * --tmp-dir=/local/$SLURM_JOB_ID FastQC : fastqc * -d /local/$SLURM_JOB_ID MACS2 : macs2 callpeak * --tempdir /local/$SLURM_JOB_ID Software known to use tmp by default with no know workaround. Keras has /tmp/.keras hardcoded as a fallback cache directory if ~/.keras is inaccessible. See here for a discussion of the issue. How much space do I have left? \u00b6 Individual Storage : use the command quota-report to see usage in /data/user/$USER and /scratch/$USER . Project Storage : use the command proj-quota-report <project> . Replace <project> with the appropriate project directory name, i.e., /data/project/<project> . Be sure to not use a trailing slash. Use proj-quota-report mylab not proj-quota-report mylab/ . Long-Term Storage : please contact Support . Quota reports are updated nightly, so they may be out of date if you move data around before running these commands. Tip Running out of space? Can't afford to remove any data? Please consider using our Long Term Storage (LTS) system . Data Responsibilities and Procedures \u00b6 Archival \u00b6 Important Archival of data is the responsibility of researchers using Cheaha. At this time, Research Computing does not offer a method of archival. If you have need for archival, please feel free to contact Support to start a conversation. A possible external resource for archival is available through University of Oklahoma (OU) Supercomputing Center for Education and Research (OSCER). Please see the following link for details: https://www.ou.edu/oscer/resources/ourrstore--ou---regional-research-store . Backups \u00b6 Important Backups of data are the responsibility of researchers using Cheaha. A good practice for backing up data is to use the 3-2-1 rule, as recommended by US-CERT : 3 : Keep 3 copies of important data. 1 primary copy for use, 2 backup copies. 2 : Store backup copies on 2 different media types to protect from media-specific hazards. 1 : Store 1 backup copy offsite, located geographically distant from the primary copy. What hazards can cause data loss? Accidental file deletion. Example: mistakenly deleting the wrong files when using the shell command rm . Files deleted with rm or any similar command can not be recovered by us under any circumstances. Please restore from a backup. Natural disasters. Examples: tornado; hurricane. All of our data sits in one geographical location at the UAB Technology Innovation Center (TIC). Plans to add geographical data redundancy are being considered. Please restore from an offsite backup. Unusable backups. Examples: backup software bug; media destroyed; natural disaster at offsite location. Regularly test data restoration from all backups. How can I ensure data integrity? Regularly back up your (and your lab's) data in an offsite location. S3 based long-term storage (LTS) can be used for short-term onsite backup. Crashplan licenses are available for automatic offsite backups, please contact Support for more information. HIPAA Compliance \u00b6 Cheaha is HIPAA compliant and can accept Protected Health Information (PHI) data. Currently, long-term storage is NOT HIPAA compliant but will be in the future. For UAB policies surrounding PHI data, please see the following URLs. Data Classification Data Protection and Security Policy Data Access Policy HIPAA Data Policy Important It is the responsibility of researchers to make sure PHI is accessible only to people on the relevant IRB, with a demonstrated need to know. If PHI is stored in a project directory where some researchers are not on the IRB, their access to those files should be restricted using Access Control Lists (ACLs). Access control should be planned in advance of moving PHI data to Cheaha. If you need assistance setting up ACLs properly, please contact Support . Managing PHI data can be challenging. There are experts on Campus who can provide assistance. Please contact Support if you intend to use Research Computing services in combination with PHI and PHI-derived data. Scratch Retention Policy \u00b6 Data stored in /scratch is subject to two limited retention policies. Each user will have a quota of 50 TB of scratch storage. Files will be retained for a maximum of 30 days.","title":"Cheaha Storage"},{"location":"data_management/storage/#storage","text":"","title":"Storage"},{"location":"data_management/storage/#what-type-of-storage-do-i-need","text":"There are multiple locations for data storage both on and off Cheaha each with a specific purpose. You can look at the table below to help determine the storage platform we provide that best matches your needed use-case. If you need additional assistance, please contact Support . Platform Long-term Storage User Data and Home Directories Project Directories Global Scratch Local Scratch Data Use-Case Data that rarely or never changes. Static data hosting. Acquired data pick-up repository. General-purpose data storage. Data to be shared among collaborator group and ready for analysis. Temporary files created during analysis. Rolling file deletion over time. Small files created during jobs. Must be deleted at the end of a job. Default Quota Individual: 5 TB Group: 75 TB 5 TB 25 TB 100 TB Small Owner Individual: per researcher Group: Lab PI or Core Director Individual Researchers Lab PI or Core Director Individual Researchers Individual Researchers Accessible From - Cheaha - Globus - Cheaha - Globus - Cheaha - Globus Cheaha Cheaha Cheaha Path No path; see LTS Interfaces . /home/<blazerid> ( $HOME ) and /data/user/<blazerid> ( $USER_DATA ) /data/project/<name> /scratch/<blazerid> ( $USER_SCRATCH ) /local/$SLURM_JOB_ID Read/Write (IO) Speed Slower Fast Fast Fast Fastest Responsibilities & Procedures PI/Director responsible for data and access control. PI/Director responsible for data and access control. Data deleted after 30 days. Data deleted as needed. Access Control Bucket policies Self only chmod and ACLs Self only Self only How to Request Upon request. Comes with Cheaha account. Upon request. Comes with Cheaha account. Comes with Cheaha account.","title":"What Type of Storage Do I Need?"},{"location":"data_management/storage/#what-individual-storage-solutions-are-available","text":"Every researcher has personal directories found at /home/$USER (or $HOME ) and /data/user/$USER (or $USER_DATA ), which are created automatically during account registration. These two locations are meant to store general data and can be used during active analysis. While there are no data retention policies in place, these spaces are not intended for long-term storage of data that changes infrequently. Traditionally, $HOME is intended to store scripts, supporting files, software configuration files, and toolboxes such as Anaconda virtual environments or R packages. In contrast, $USER_DATA is intended to store datasets and results for individual research projects. Individual allocations on Long-Term Storage (LTS) are also available upon request.","title":"What Individual Storage Solutions are Available?"},{"location":"data_management/storage/#how-do-i-request-individual-long-term-storage","text":"To request individual Long-Term Storage, please first read and understand how Long-Term Storage differs from traditional filesystems, like GPFS on Cheaha. Decide if it is suitable for your needs. Then please feel free to contact Support .","title":"How Do I Request Individual Long-Term Storage?"},{"location":"data_management/storage/#what-shared-storage-solutions-are-available","text":"Shared Storage is available via two services. We have Project Storage (located in /data/project or Cheaha) and Long-Term Storage (LTS) . The two offerings are suited to different sets of use-cases, so please read on to determine which may be most suitable. Project Storage is best-suited for changing or dynamic data. Specifically: Data needing/undergoing analysis Exploratory data Temporary data needed longer than 30 days In contrast, Long-Term Storage is best-suited for unchanging or static data. Specifically: Instrument-acquired data Completed analyses Hosting data for others to copy Hosting data for the public internet \"Pick-up\" and \"drop-off\" locations for data as part of a workflow Shared Storage is available for labs headed by a PI and for Core facilities headed by a director. Shared Storage is allocated on a per-organization basis, not on a per-person basis. If an individual researcher manages both a lab and a Core, they may request independent storage allocations for each organization. Each organization may request both Project Storage and Long-Term Storage.","title":"What Shared Storage Solutions are Available?"},{"location":"data_management/storage/#how-do-i-request-shared-storage","text":"To request shared Project Storage or Long-Term Storage, please contact Support . To ensure prompt allocation of Shared Storage, please follow the guidelines below. Requests must be made to support@listserv.uab.edu or via the AskIT HelpDesk . Requests must come from one of the proposed owners. All proposed owners must have created their Research Computing accounts at the time the request is made. Please provide the following information. Missing information can delay allocation of Shared Storage as we either look up the information, or ask followup questions. Responsible Party/Owner: The BlazerID of the person claiming reponsibility for what happens and what is stored in the space. Typically this would be a Principal Investigator (PI) or a Core Director. Multiple responsible parties are allowed. We need one person declared as \"primary\" owner. This person will be the literal owner (in the Linux sense) for Project Storage. Members: A list of BlazerIDs of people to give access to the space. (Note: this only applies to Project Storage. LTS access controls are managed differently.) Type of Organization: Is the Shared Storage request for a lab, core, campus administrative group, or something else? Name of Organization: The specific name of the organization the Shared Storage request is for. Parent Organization: The name of the parent organization for your organization. Please be as detailed as possible. Purpose of Shared Storage: The research purpose for the storage, how do you intend to use it? Please feel free to be as detailed as you like, but please limit to a few sentences at most. Internal UAB Collaborator Organizations: The name(s) of any other UAB organizations participating in the Shared Storage. External Collaborator Organizations: The name(s) of any external organizations participating in the Shared Storage. Regulatory Requirements: List any regulatory requirements or agencies affecting data to be stored in the space. Possibilities include, but are not limited to: IRB, EHR, HIPAA, PHI, FERPA. Name of Shared Storage: Please give us a short, memorable name that is specific to your organization but general to your purpose. For Project Storage, this name will be used in the /data/project/<name> path on Cheaha. For Labs we recommend the format <PI_blazerid>-lab . For Cores we recommend a shortened version of the Core name. If some members have not created their accounts at the time of the request, we will proceed with allocating the Shared Storage. Additional members may be added at a later time in a new service request.","title":"How Do I Request Shared Storage?"},{"location":"data_management/storage/#how-do-i-make-changes-to-shared-storage-membership","text":"To request changes in Shared Storage membership, please contact Support . Please take note of the following guidelines to ensure changes can be made promptly. We must have written approval from an owner to make membership changes. The exact name of the Shared Storage. If it is Project Storage, the path to the storage location, i.e., /data/project/... . Please give BlazerIDs of members to add or remove.","title":"How Do I Make Changes to Shared Storage Membership?"},{"location":"data_management/storage/#project-directory-permissions","text":"Every project directory has a group that is unique system-wide, and not used anywhere else on the filesystem. The unique project group will be referred to as <grp> and generally has the same name as the top level project directory. Note Some early group names may not match their project directory, but should be reasonably close. Members of the project directory group have permissions to access that project directory. Adding and removing members from the project directory group is how Research Computing controls access to, and ownership of, project directories. We do not use access control lists (ACLs) to manage permissions ourselves, but use of ACLs is allowed and encouraged for PIs and project administrators who want more fine-grained control. Please see our section on ACLs for more information. Be default, project space permissions are set up in the following way: Top level directory Newly created files Newly created directories Numeric Permissions 2770 0664 2775 Symbolic Permissions drwxrws--- -rw-rw-r-- drwxrwsr-x setgid enabled Yes Yes User owner PI/Admin Creator Creator Group owner <grp> <grp> <grp> Having setgid enabled on directories means new files and directories created within will inherit group ownership and the setgid bit. The setgid bit is reflected by the 2 in the numeric permissions and the s in the symbolic permissions. The setgid bit and per-directory project groups is how Research Computing controls access to each project directory. There are some known issues surrounding project directory permissions when files are put into the project directory. Different commands have different behaviors. The following list describes the behaviors of various commands used to move and copy data, as well as good practices. mv maintains all permissions and ownerships of the source file or directory. For files and directories created outside the project directory, avoid using mv , prefer cp or similar instead. See below for alternatives. For files and directories created within the project directory, mv may work, but be sure the file has correct permissions and group ownership. cp , tar -x , rsync , rclone , sftp and Globus all behave as though creating a new file at the target location, by default. Prefer these commands when it is sensible to do so. Avoid using the -p flag with cp , tar , rsync and sftp . When using the -p flag, files and directories will retain their source permissions. Retaining source permissions in project directories is undesirable behavior and can create headaches for you, your colleagues, and your project directory administrators and PIs. For PIs and project administrators: Please educate your staff and collaborators about the above permission setups, and any additional ACLs you may have in place, to minimize future challenges. If you have issues with permissions, please contact Support . We can guide you through Managing Permissions and Managing Group Ownership .","title":"Project Directory Permissions"},{"location":"data_management/storage/#scratch","text":"Two types of scratch space are provided for analyses currently being ran, network-mounted and local. These are spaces shared across users (though one user still cannot access another user's files without permission) and as such, data should be moved out of scratch when the analysis is finished. Important Starting January 2023, scratch data will have limited retention. See Scratch Retention Policy for more information.","title":"Scratch"},{"location":"data_management/storage/#user-scratch","text":"All users have access to a large, temporary, work-in-progress directory for storing data, called a scratch directory in /scratch/$USER or $USER_SCRATCH . Use this directory to store very large datasets or temporary pipeline intermediates for a short period of time while running your jobs. The maximum amount of data a single user can store in network scratch is 100 TB at once. Network scratch is available on the login node and each compute node. This storage is a GPFS high performance file system providing roughly 1 PB of storage. If using scratch, this should be your jobs' primary working directory, unless the job would benefit from local scratch (see below). Warning Research Computing expects each user to keep their scratch areas clean. The cluster scratch areas are not to be used for archiving data. In order to keep scratch clear and usable for everyone, files older than 30 days will be automatically deleted.","title":"User Scratch"},{"location":"data_management/storage/#local-scratch","text":"Each compute node has a local scratch directory that is accessible via /local/$SLURM_JOB_ID . At this time, you will need to create the directory manually using mkdir -p /local/$SLURM_JOB_ID . If your job performs a lot of file I/O, the job should use /local/$SLURM_JOB_ID rather than $USER_SCRATCH to prevent bogging down the network scratch file system. It's important to recognize that most jobs run on the cluster do not fall under this category. If you are using amperenodes and the A100 GPUs, then it is highly recommended to move your input files to /local/$SLURM_JOB_ID prior to running your workflow, to ensure adequate GPU performance. Using $USER_SCRATCH , or other network file locations, will starve the GPU of data, resulting in poor performance. For more information please see Ensuring IO Performance With A100 GPUs . Be sure to clean up /local/$SLURM_JOB_ID after your job is complete! An example script to automate this process is shown below. #!/bin/bash #SBATCH ... # LOAD MODULES # module load ... # CREATE TEMPORARY DIRECTORY # WARNING! $TMPDIR will be deleted at the end of the script! # Changing the following line can cause permanent, unintended deletion of important data. TMPDIR = \"/local/ $USER / $SLURM_JOB_ID \" mkdir -p \" $TMPDIR \" # COPY RESEARCH DATA TO LOCAL TEMPORARY DIRECTORY # Replace $MY_DATA_DIR with the path to your data folder cp -r \" $MY_DATA_DIR \" \" $TMPDIR \" # YOUR ORIGINAL WORKFLOW GOES HERE # be sure to load files from \"$TMPDIR\"! # CLEAN UP TEMPORARY DIRECTORY # WARNING! # Changing the following line can cause permanent, unintended deletion of important data. rm -rf \" $TMPDIR \" Important Using /local/$SLURM_JOB_ID with MPI jobs takes additional consideration. If you do not need MPI, please use the #SBATCH --nodes=1 slurm directive to specify that all requested cores are on the same node. If you need the performance of /local/$SLURM_JOB_ID in an MPI job, please contact Support and read about the Slurm commands sbcast and sgather .","title":"Local Scratch"},{"location":"data_management/storage/#temporary-files-tmp","text":"Please do not use the directory tmp as storage for temporary files. The tmp directory is local to each node, and a full tmp directory harms compute performance on that node for all users. Instead, please use /local/$SLURM_JOB_ID for fast access and $USER_SCRATCH for larger space. Some software defaults to using tmp without any warning or documentation, especially software designed for personal computers. We may reach out to inform you if your software fills tmp , as it can harm performance on that compute node. If that happens we will work with you to identify ways of redirecting temporary storage to one of the scratch spaces.","title":"Temporary Files (tmp)"},{"location":"data_management/storage/#software-known-to-use-tmp","text":"The following software are known to use tmp by default, and can be worked around by using the listed flags. See Local Scratch for more information about creating a local temporary directory. Java : java * -Djava.io.tmpdir=/local/$SLURM_JOB_ID UMI Tools : umi_tools * --temp-dir=/local/$SLURM_JOB_ID Samtools Sort : samtools sort * -T /local/$SLURM_JOB_ID GATK Tool : gatk --java-options * --tmp-dir /local/$SLURM_JOB_ID Parabricks : pbrun * --tmp-dir=/local/$SLURM_JOB_ID FastQC : fastqc * -d /local/$SLURM_JOB_ID MACS2 : macs2 callpeak * --tempdir /local/$SLURM_JOB_ID Software known to use tmp by default with no know workaround. Keras has /tmp/.keras hardcoded as a fallback cache directory if ~/.keras is inaccessible. See here for a discussion of the issue.","title":"Software Known to Use tmp"},{"location":"data_management/storage/#how-much-space-do-i-have-left","text":"Individual Storage : use the command quota-report to see usage in /data/user/$USER and /scratch/$USER . Project Storage : use the command proj-quota-report <project> . Replace <project> with the appropriate project directory name, i.e., /data/project/<project> . Be sure to not use a trailing slash. Use proj-quota-report mylab not proj-quota-report mylab/ . Long-Term Storage : please contact Support . Quota reports are updated nightly, so they may be out of date if you move data around before running these commands. Tip Running out of space? Can't afford to remove any data? Please consider using our Long Term Storage (LTS) system .","title":"How much space do I have left?"},{"location":"data_management/storage/#data-responsibilities-and-procedures","text":"","title":"Data Responsibilities and Procedures"},{"location":"data_management/storage/#archival","text":"Important Archival of data is the responsibility of researchers using Cheaha. At this time, Research Computing does not offer a method of archival. If you have need for archival, please feel free to contact Support to start a conversation. A possible external resource for archival is available through University of Oklahoma (OU) Supercomputing Center for Education and Research (OSCER). Please see the following link for details: https://www.ou.edu/oscer/resources/ourrstore--ou---regional-research-store .","title":"Archival"},{"location":"data_management/storage/#backups","text":"Important Backups of data are the responsibility of researchers using Cheaha. A good practice for backing up data is to use the 3-2-1 rule, as recommended by US-CERT : 3 : Keep 3 copies of important data. 1 primary copy for use, 2 backup copies. 2 : Store backup copies on 2 different media types to protect from media-specific hazards. 1 : Store 1 backup copy offsite, located geographically distant from the primary copy. What hazards can cause data loss? Accidental file deletion. Example: mistakenly deleting the wrong files when using the shell command rm . Files deleted with rm or any similar command can not be recovered by us under any circumstances. Please restore from a backup. Natural disasters. Examples: tornado; hurricane. All of our data sits in one geographical location at the UAB Technology Innovation Center (TIC). Plans to add geographical data redundancy are being considered. Please restore from an offsite backup. Unusable backups. Examples: backup software bug; media destroyed; natural disaster at offsite location. Regularly test data restoration from all backups. How can I ensure data integrity? Regularly back up your (and your lab's) data in an offsite location. S3 based long-term storage (LTS) can be used for short-term onsite backup. Crashplan licenses are available for automatic offsite backups, please contact Support for more information.","title":"Backups"},{"location":"data_management/storage/#hipaa-compliance","text":"Cheaha is HIPAA compliant and can accept Protected Health Information (PHI) data. Currently, long-term storage is NOT HIPAA compliant but will be in the future. For UAB policies surrounding PHI data, please see the following URLs. Data Classification Data Protection and Security Policy Data Access Policy HIPAA Data Policy Important It is the responsibility of researchers to make sure PHI is accessible only to people on the relevant IRB, with a demonstrated need to know. If PHI is stored in a project directory where some researchers are not on the IRB, their access to those files should be restricted using Access Control Lists (ACLs). Access control should be planned in advance of moving PHI data to Cheaha. If you need assistance setting up ACLs properly, please contact Support . Managing PHI data can be challenging. There are experts on Campus who can provide assistance. Please contact Support if you intend to use Research Computing services in combination with PHI and PHI-derived data.","title":"HIPAA Compliance"},{"location":"data_management/storage/#scratch-retention-policy","text":"Data stored in /scratch is subject to two limited retention policies. Each user will have a quota of 50 TB of scratch storage. Files will be retained for a maximum of 30 days.","title":"Scratch Retention Policy"},{"location":"data_management/lts/","text":"Long-Term Storage \u00b6 UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments. Tip Globus may be used to transfer data with LTS. Terminology \u00b6 When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead. object : any unit (i.e. file) stored in LTS. Typical folders and files do not exist in S3. bucket : The root which objects are stored in. Each account can create a certain number of buckets and each bucket can be shared individually with other users. Bucket names are unique across the entire LTS platform (see the section on duplicate names ) prefix : Used in place of a file path to an object, and so can be used to represent an object's place in a typical filesystem. Stored as metadata in each object, prefixes are used for searches in a bucket. policy : sets permissions for whole buckets and individual objects. Policies allow or deny access to buckets for individual accounts. These are controlled by the owner of the bucket. access key : a unique identifier given to each user for access to LTS, similar to a username. A user's access key is preset and given to them after account setup. secret key : a credential string similar to a password given to each user for access to LTS. The secret key is preset and given to the user after account setup Danger Never give access and secret keys for personal or lab accounts to anyone! Bad actors who are given keys to accounts which own important buckets can change access permissions and delete any and all data! If you need to give elevated permissions to other users to view, upload, download, delete, etc. any data from a bucket, those permissions can be changed via bucket policies without giving out keys. Please contact Research Computing for help setting up and applying policies if you need it Note If you lose your access and secret keys, please submit a support ticket to support@listserv.uab.edu to request your keys. Keys will only be given to an account owner as verified by RC staff. This documentation will use the standard file and path terms since those are more easily understood by most users. Just be aware that documentation such as AWS CLI will use terms prefix, object, and others that are not standard in a typical filesystem. Requesting an Account \u00b6 UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to support@listserv.uab.edu . You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS. Avoiding Duplicate Names for Buckets \u00b6 Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. Good practice when naming buckets is to use a short, descriptive and memorable name, then append a universally unique identifier (UUID) to the end. Websites like https://www.uuidgenerator.net/ may be used to generate and copy UUIDs. There are \\(5.3\\times 10^{36}\\) possible UUIDs, which means the chance of duplicating one is virtually zero. Please see Wikipedia for math supporting the low rate of duplication.","title":"Long-Term Storage"},{"location":"data_management/lts/#long-term-storage","text":"UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments. Tip Globus may be used to transfer data with LTS.","title":"Long-Term Storage"},{"location":"data_management/lts/#terminology","text":"When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead. object : any unit (i.e. file) stored in LTS. Typical folders and files do not exist in S3. bucket : The root which objects are stored in. Each account can create a certain number of buckets and each bucket can be shared individually with other users. Bucket names are unique across the entire LTS platform (see the section on duplicate names ) prefix : Used in place of a file path to an object, and so can be used to represent an object's place in a typical filesystem. Stored as metadata in each object, prefixes are used for searches in a bucket. policy : sets permissions for whole buckets and individual objects. Policies allow or deny access to buckets for individual accounts. These are controlled by the owner of the bucket. access key : a unique identifier given to each user for access to LTS, similar to a username. A user's access key is preset and given to them after account setup. secret key : a credential string similar to a password given to each user for access to LTS. The secret key is preset and given to the user after account setup Danger Never give access and secret keys for personal or lab accounts to anyone! Bad actors who are given keys to accounts which own important buckets can change access permissions and delete any and all data! If you need to give elevated permissions to other users to view, upload, download, delete, etc. any data from a bucket, those permissions can be changed via bucket policies without giving out keys. Please contact Research Computing for help setting up and applying policies if you need it Note If you lose your access and secret keys, please submit a support ticket to support@listserv.uab.edu to request your keys. Keys will only be given to an account owner as verified by RC staff. This documentation will use the standard file and path terms since those are more easily understood by most users. Just be aware that documentation such as AWS CLI will use terms prefix, object, and others that are not standard in a typical filesystem.","title":"Terminology"},{"location":"data_management/lts/#requesting-an-account","text":"UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to support@listserv.uab.edu . You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS.","title":"Requesting an Account"},{"location":"data_management/lts/#avoiding-duplicate-names-for-buckets","text":"Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. Good practice when naming buckets is to use a short, descriptive and memorable name, then append a universally unique identifier (UUID) to the end. Websites like https://www.uuidgenerator.net/ may be used to generate and copy UUIDs. There are \\(5.3\\times 10^{36}\\) possible UUIDs, which means the chance of duplicating one is virtually zero. Please see Wikipedia for math supporting the low rate of duplication.","title":"Avoiding Duplicate Names for Buckets"},{"location":"data_management/lts/interfaces/","text":"Connecting to LTS \u00b6 LTS is not available as a mounted filesystem on local computers or Cheaha. You must use an interface to transfer data between LTS and whichever machine you are using. There a variety of interfaces with the following recommendations. Globus \u00b6 Globus is a general file transfer system that operates through a web browser and is recommended for most file transfer needs. UAB has an S3 connector for Globus that can transfer data to and from LTS as long as the user has access to the desired buckets. To connect to the LTS endpoint in Globus, search UAB Research Computing LTS in the search bar and enter your access and secret keys given to you by Research Computing staff. You will be able to see the buckets owned by the account associated with the keys you entered. Important If your LTS account was given permission to access a bucket owned by another account, it will not automatically appear in the Globus file browser. You can access buckets you have s3:ListBucket permissions on by typing /<bucket-name>/ in the Path field under the LTS endpoint. Globus is very useful for single transfers of data either to or from LTS and is available on any computer with an internet connection. However, it is currently not capable of managing buckets. This must be done through a command line interface. Command Line \u00b6 While globus is the recommended tool for most data transfers, command line tools are necessary for planned, regular transfers as well as managing permissions on buckets. We recommend the following two tools for different purposes: s3cmd is a Python tool that we suggest using for managing bucket permissions as well as small transfers. s5cmd is a Go package that transfers data much more quickly than s3cmd, especially as the file size and/or quanitity increases. It does not have full bucket management capabilities. You do not need to install both tools if they aren't necessary. Both are available to install into Anaconda environments. It's suggested to create a single environment named s3 and install both s3cmd and s5cmd into it for easy access to both tools. Specific install and usage commands for each are given in their respective sections. You can create the general environment using the following commands: module load Anaconda3 conda create -n s3 -c conda-forge pip s5cmd conda activate s3 pip install s3cmd Note We manually install pip into the conda environment so that pip will install s3cmd into the conda environment as opposed to $HOME/.local . This way, you do not need to add the .local folder to your path whenever you want to use s3cmd . s3cmd \u00b6 s3cmd is our suggested tool for managing bucket permissions and small, periodic file transfers. See the preceding section for instructions on how to install both it and s5cmd into an Anaconda environment. Once you have s3cmd installed and the environment active, you can start the configuration process like so: s3cmd --configure [ -c $HOME /profile_name ] You can run the configuration either with or without the [-c] option. If you use it, a file named profile_name will be created in your home directory with your login credentials and other information. If you omit the -c option, a file called $HOME/.s3cfg will be created by default. This can be helpful if you have multiple S3 profiles you are using. If you use UAB LTS as your only S3 storage platform and are only managing a single account, it's suggested to omit the -c option. If you are a PI or data manager and are managing both a personal and lab/core LTS account, you will need to make a separate profile for each account. Note After configuration, the s3cmd command will default to using the .s3cfg file for credentials if it exists. If you create a separate named profile file, you will need to add that to the s3cmd call each time you run it. During configuration, you will be asked to enter some information. You can follow the example below, inputting your user-specific information where required. Lines requiring user input are highlighted. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key: <access key> Secret Key: <secret key> Default Region [US]: <leave blank> Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [s3.amazonaws.com]: https://s3.lts.rc.uab.edu Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).s3.lts.rc.uab.edu Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password: <leave blank or enter password> Path to GPG program [/usr/bin/gpg]: <leave blank> When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer Use HTTPS protocol [Yes]: <leave blank> On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name: <leave blank> New settings: Access Key: <access key> Secret Key: <secret key> Default Region: US S3 Endpoint: https://s3.lts.rc.uab.edu DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).s3.lts.rc.uab.edu Encryption password: Path to GPG program: $HOME/bin/gpg Use HTTPS protocol: True HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n] n Save settings? [y/N] y Note If you choose to test access using your credentials, the test may fail. Do not rely on the automatic test results, test access yourself by either creating a bucket or listing files from a existing bucket using the commands listed below. s3cmd Commands \u00b6 # General command structure for s3cmd s3cmd [ -c profile_file ] <command> [ options ] [ -n --dry-run ] The [-c profile_file] is only required if you are using credentials NOT saved in the $HOME/.s3cfg file. Otherwise, you can omit it. To see a list of all available commands, use s3cmd --help . Additionally, if you want to test an action without actually running it (i.e. it prints all actions that would be performed), you can add the -n or --dry-run option. A list of selected commands are provided below for reference # Create a bucket s3cmd mb s3://<bucket> # List a bucket/path within the bucket s3cmd ls [ -r, --recursive ] s3://<bucket/path> # Check bucket or folder size s3cmd du -H s3://<bucket/path/> # transfer a file or folder from local to a bucket s3cmd put <source> s3://<bucket/path/destination/> # transfer a file or folder from a bucket to a local drive s3cmd get s3://<bucket/path/source/> <destination> # transfer between two S3 locations s3cmd cp s3://<bucket/path/> s3://<bucket/path/> # sync an S3 location with a local source. The S3 destination will be made exactly the same as the source including file deletions. # The source is unaltered. The S3 bucket/folder can be either the source or the destination s3cmd sync <source> s3://<bucket/path/destination> # remove a single object or all objects within a given path s3cmd rm s3://<bucket/path/file> [ --recursive ] # remove an entire bucket s3cmd rb s3://<bucket> # get info about the bucket s3cmd info s3://<bucket> Danger Be extremely cautious using sync . If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable. Note When using ls to list buckets, it will only show the buckets you own, not buckets you have been given permissions on. This is a limitation of the S3 system. You can still interact with any buckets you have been given relevant permissions on, but you will need to remember the names of the buckets you don't own. s5cmd \u00b6 s5cmd is a parallel transfer tool suggested for period transfers of large and/or many files at a time. It has options for customizing how many processors are available for transferring data as well as how many chunks files can be broken into during transfer to minimize transfer time. See the preceding section for instructions on how to install both it and s3cmd into an Anaconda environment Configuring s5cmd \u00b6 s5cmd does not use the same authentication file as s3cmd. Instead, it uses official AWS SDK to access S3 including LTS. The default credentials file for AWS CLI would found at ${HOME}/.aws/credentials . This file is then populated with different profiles and their access and secret keys. You can create the necessary file with the following commands. mkdir ${ HOME } /.aws touch ${ HOME } /.aws/credentials Open the credentials file with your favorite editor (i.e. vim , nano , gedit , etc.) and create a default profile by adding the following lines. [default] aws_access_key_id = <access_key> aws_secret_access_key = <secret_key> Note Do not include the <> symbols in the credentials file when saving your keys One of the benefits of this credential method is that multiple sets of credentials can be kept in the same file. For instance, if you have both a lab/core LTS account and a personal account, you could set your personal account as the default profile and then add your lab credentials under a named profile like so: [default] aws_access_key_id = <personal_access_key> aws_secret_access_key = <personal_secret_key> [example-lab] aws_access_key_id = <lab_access_key> aws_secret_access_key = <lab_secret_key> s5cmd Commands \u00b6 s5cmd has the following general form. s5cmd --endpoint-url https://s3.lts.rc.uab.edu [ global_options ] command [ command options ] [ arguments ] Here, global options must be kept separate from command specific options. For instance, the --endpoint-url option is a global option that specifies the URL for the S3 server. This must be included with every s5cmd command to communicate with UAB LTS, otherwise it will default to accessing AWS servers. Other global options include --numworkers and --profile , the number of available CPUs and which account to use in the credentials file, respectively. You can see a list of global options and the list of available commands by running s5cmd --help . A selection of commands are listed below. # copy all files from a local directory to a bucket using a single CPU s5cmd --endpoint-url https://s3.lts.rc.uab.edu cp /path/to/directory/* s3://bucket/ # copy all files from a local directory to a bucket using 10 CPUs and allowing the files to be broken into 5 parts during transfer s5cmd --endpoint-url https://s3.lts.rc.uab.edu --numworkers 10 cp --concurrency 5 /path/to/directory/* s3://bucket/ # sync an S3 bucket (destination) to a local directory (source) s5cmd --endpoint-url https://s3.lts.rc.uab.edu sync /path/to/directory/ s3://bucket/ # remove all objects with a given prefix from a bucket s5cmd --endpoint-url https://s3.lts.rc.uab.edu rm s3://bucket/prefix/* As with s3cmd, be very careful using the sync and rm commands as these can/will delete files either locally or on LTS. There are many more commands s5cmd can use as well as a number of command options that can be used to customize how an operation is performed. Please see the help documentation for a full list. It's important to note that the main functionality of s5cmd over s3cmd is the parallelization options given by the --numworkers global option and the --concurrency local option for cp and sync commands. Choosing not to use these options will result in unoptimized performance. Important When setting the value for --numworkers , do not select a value beyond the number of CPUs you have requested for your job! This can cause high context switching (meaning individual CPUs are switching between multiple running processes) which can affect job performance for all jobs on a node. Alternatives \u00b6 There are other tools for interfacing with LTS such as rclone. Please see our rclone documentation for more details.","title":"Interfacing with LTS"},{"location":"data_management/lts/interfaces/#connecting-to-lts","text":"LTS is not available as a mounted filesystem on local computers or Cheaha. You must use an interface to transfer data between LTS and whichever machine you are using. There a variety of interfaces with the following recommendations.","title":"Connecting to LTS"},{"location":"data_management/lts/interfaces/#globus","text":"Globus is a general file transfer system that operates through a web browser and is recommended for most file transfer needs. UAB has an S3 connector for Globus that can transfer data to and from LTS as long as the user has access to the desired buckets. To connect to the LTS endpoint in Globus, search UAB Research Computing LTS in the search bar and enter your access and secret keys given to you by Research Computing staff. You will be able to see the buckets owned by the account associated with the keys you entered. Important If your LTS account was given permission to access a bucket owned by another account, it will not automatically appear in the Globus file browser. You can access buckets you have s3:ListBucket permissions on by typing /<bucket-name>/ in the Path field under the LTS endpoint. Globus is very useful for single transfers of data either to or from LTS and is available on any computer with an internet connection. However, it is currently not capable of managing buckets. This must be done through a command line interface.","title":"Globus"},{"location":"data_management/lts/interfaces/#command-line","text":"While globus is the recommended tool for most data transfers, command line tools are necessary for planned, regular transfers as well as managing permissions on buckets. We recommend the following two tools for different purposes: s3cmd is a Python tool that we suggest using for managing bucket permissions as well as small transfers. s5cmd is a Go package that transfers data much more quickly than s3cmd, especially as the file size and/or quanitity increases. It does not have full bucket management capabilities. You do not need to install both tools if they aren't necessary. Both are available to install into Anaconda environments. It's suggested to create a single environment named s3 and install both s3cmd and s5cmd into it for easy access to both tools. Specific install and usage commands for each are given in their respective sections. You can create the general environment using the following commands: module load Anaconda3 conda create -n s3 -c conda-forge pip s5cmd conda activate s3 pip install s3cmd Note We manually install pip into the conda environment so that pip will install s3cmd into the conda environment as opposed to $HOME/.local . This way, you do not need to add the .local folder to your path whenever you want to use s3cmd .","title":"Command Line"},{"location":"data_management/lts/interfaces/#s3cmd","text":"s3cmd is our suggested tool for managing bucket permissions and small, periodic file transfers. See the preceding section for instructions on how to install both it and s5cmd into an Anaconda environment. Once you have s3cmd installed and the environment active, you can start the configuration process like so: s3cmd --configure [ -c $HOME /profile_name ] You can run the configuration either with or without the [-c] option. If you use it, a file named profile_name will be created in your home directory with your login credentials and other information. If you omit the -c option, a file called $HOME/.s3cfg will be created by default. This can be helpful if you have multiple S3 profiles you are using. If you use UAB LTS as your only S3 storage platform and are only managing a single account, it's suggested to omit the -c option. If you are a PI or data manager and are managing both a personal and lab/core LTS account, you will need to make a separate profile for each account. Note After configuration, the s3cmd command will default to using the .s3cfg file for credentials if it exists. If you create a separate named profile file, you will need to add that to the s3cmd call each time you run it. During configuration, you will be asked to enter some information. You can follow the example below, inputting your user-specific information where required. Lines requiring user input are highlighted. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key: <access key> Secret Key: <secret key> Default Region [US]: <leave blank> Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [s3.amazonaws.com]: https://s3.lts.rc.uab.edu Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).s3.lts.rc.uab.edu Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password: <leave blank or enter password> Path to GPG program [/usr/bin/gpg]: <leave blank> When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer Use HTTPS protocol [Yes]: <leave blank> On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name: <leave blank> New settings: Access Key: <access key> Secret Key: <secret key> Default Region: US S3 Endpoint: https://s3.lts.rc.uab.edu DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).s3.lts.rc.uab.edu Encryption password: Path to GPG program: $HOME/bin/gpg Use HTTPS protocol: True HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n] n Save settings? [y/N] y Note If you choose to test access using your credentials, the test may fail. Do not rely on the automatic test results, test access yourself by either creating a bucket or listing files from a existing bucket using the commands listed below.","title":"s3cmd"},{"location":"data_management/lts/interfaces/#s3cmd-commands","text":"# General command structure for s3cmd s3cmd [ -c profile_file ] <command> [ options ] [ -n --dry-run ] The [-c profile_file] is only required if you are using credentials NOT saved in the $HOME/.s3cfg file. Otherwise, you can omit it. To see a list of all available commands, use s3cmd --help . Additionally, if you want to test an action without actually running it (i.e. it prints all actions that would be performed), you can add the -n or --dry-run option. A list of selected commands are provided below for reference # Create a bucket s3cmd mb s3://<bucket> # List a bucket/path within the bucket s3cmd ls [ -r, --recursive ] s3://<bucket/path> # Check bucket or folder size s3cmd du -H s3://<bucket/path/> # transfer a file or folder from local to a bucket s3cmd put <source> s3://<bucket/path/destination/> # transfer a file or folder from a bucket to a local drive s3cmd get s3://<bucket/path/source/> <destination> # transfer between two S3 locations s3cmd cp s3://<bucket/path/> s3://<bucket/path/> # sync an S3 location with a local source. The S3 destination will be made exactly the same as the source including file deletions. # The source is unaltered. The S3 bucket/folder can be either the source or the destination s3cmd sync <source> s3://<bucket/path/destination> # remove a single object or all objects within a given path s3cmd rm s3://<bucket/path/file> [ --recursive ] # remove an entire bucket s3cmd rb s3://<bucket> # get info about the bucket s3cmd info s3://<bucket> Danger Be extremely cautious using sync . If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable. Note When using ls to list buckets, it will only show the buckets you own, not buckets you have been given permissions on. This is a limitation of the S3 system. You can still interact with any buckets you have been given relevant permissions on, but you will need to remember the names of the buckets you don't own.","title":"s3cmd Commands"},{"location":"data_management/lts/interfaces/#s5cmd","text":"s5cmd is a parallel transfer tool suggested for period transfers of large and/or many files at a time. It has options for customizing how many processors are available for transferring data as well as how many chunks files can be broken into during transfer to minimize transfer time. See the preceding section for instructions on how to install both it and s3cmd into an Anaconda environment","title":"s5cmd"},{"location":"data_management/lts/interfaces/#configuring-s5cmd","text":"s5cmd does not use the same authentication file as s3cmd. Instead, it uses official AWS SDK to access S3 including LTS. The default credentials file for AWS CLI would found at ${HOME}/.aws/credentials . This file is then populated with different profiles and their access and secret keys. You can create the necessary file with the following commands. mkdir ${ HOME } /.aws touch ${ HOME } /.aws/credentials Open the credentials file with your favorite editor (i.e. vim , nano , gedit , etc.) and create a default profile by adding the following lines. [default] aws_access_key_id = <access_key> aws_secret_access_key = <secret_key> Note Do not include the <> symbols in the credentials file when saving your keys One of the benefits of this credential method is that multiple sets of credentials can be kept in the same file. For instance, if you have both a lab/core LTS account and a personal account, you could set your personal account as the default profile and then add your lab credentials under a named profile like so: [default] aws_access_key_id = <personal_access_key> aws_secret_access_key = <personal_secret_key> [example-lab] aws_access_key_id = <lab_access_key> aws_secret_access_key = <lab_secret_key>","title":"Configuring s5cmd"},{"location":"data_management/lts/interfaces/#s5cmd-commands","text":"s5cmd has the following general form. s5cmd --endpoint-url https://s3.lts.rc.uab.edu [ global_options ] command [ command options ] [ arguments ] Here, global options must be kept separate from command specific options. For instance, the --endpoint-url option is a global option that specifies the URL for the S3 server. This must be included with every s5cmd command to communicate with UAB LTS, otherwise it will default to accessing AWS servers. Other global options include --numworkers and --profile , the number of available CPUs and which account to use in the credentials file, respectively. You can see a list of global options and the list of available commands by running s5cmd --help . A selection of commands are listed below. # copy all files from a local directory to a bucket using a single CPU s5cmd --endpoint-url https://s3.lts.rc.uab.edu cp /path/to/directory/* s3://bucket/ # copy all files from a local directory to a bucket using 10 CPUs and allowing the files to be broken into 5 parts during transfer s5cmd --endpoint-url https://s3.lts.rc.uab.edu --numworkers 10 cp --concurrency 5 /path/to/directory/* s3://bucket/ # sync an S3 bucket (destination) to a local directory (source) s5cmd --endpoint-url https://s3.lts.rc.uab.edu sync /path/to/directory/ s3://bucket/ # remove all objects with a given prefix from a bucket s5cmd --endpoint-url https://s3.lts.rc.uab.edu rm s3://bucket/prefix/* As with s3cmd, be very careful using the sync and rm commands as these can/will delete files either locally or on LTS. There are many more commands s5cmd can use as well as a number of command options that can be used to customize how an operation is performed. Please see the help documentation for a full list. It's important to note that the main functionality of s5cmd over s3cmd is the parallelization options given by the --numworkers global option and the --concurrency local option for cp and sync commands. Choosing not to use these options will result in unoptimized performance. Important When setting the value for --numworkers , do not select a value beyond the number of CPUs you have requested for your job! This can cause high context switching (meaning individual CPUs are switching between multiple running processes) which can affect job performance for all jobs on a node.","title":"s5cmd Commands"},{"location":"data_management/lts/interfaces/#alternatives","text":"There are other tools for interfacing with LTS such as rclone. Please see our rclone documentation for more details.","title":"Alternatives"},{"location":"data_management/lts/lts_faq/","text":"LTS FAQ \u00b6 Can I Share My Account Access Keys With Other People? \u00b6 You should never share access keys with anyone. These should be treated similarly to your BlazerID and password. Sharing keys creates a point of vulnerability and if they fall into a nefarious actor's hands, all buckets that account owns and the data in them can be deleted. In some cases, you may not be actively managing data in a bucket even though you own the account which owns a shared bucket. Instead of sharing keys with a data steward, instead that steward should be given admin-esque permissions on the required bucket via a policy file. How Should I Organize My LTS Shared Account? \u00b6 This is ultimately up to the bucket owner, but there are a couple of single-bucket solutions depending on your specific use-case for LTS: Semi-synced copy of everything in the project space. General permissions: Data stewards and the bucket owner would have permission to delete any files. All other users would be able to upload and download files only. All users would be able to see all files uploaded by all other users to that bucket./ Purpose: This fulfills more of a pure backup role compared to option 2. While all users can upload files Benefits: The policy file for these permissions is much simpler to create and manage. Limits the number of people who can remove files that might be needed down the line. Drawbacks: Needing to ask a steward or the bucket owner to delete individual files introduces friction to data management. Example Policy File Active and collaborative external storage. All users would have a specific prefix/folder they have complete control where they can add or remove data at will. General permissions: Data stewards and the bucket owner would have permission to delete any files. Regular users would only be able to upload to and delete files from their owned prefix/folder. All users would be able to see and download any files from any other user. Purpose: This satisfies the need for expanded storage accessible from Cheaha (via the terminal or Globus). All users have their own space they can use as they see fit within the bucket for extra storage while still being able to access, but not alter, files from other users in cases they need to be shared. Part of the bucket, or a separate bucket entirely, can also be used as a backup for old or current datasets where users only have read permissions. Benefits: How the bucket can be used is much more malleable and up to the individual users. Empowers them to add and remove data from their own prefix/folder without oversight from stewards or the bucket owner. Drawbacks: The policy file is more difficult to craft and manage when researchers needed to be added or removed from the bucket. Allowing users to delete their uploaded data at their discretion may conflict with the owner's view of those data. Example Policy File While these are two simple solutions, a combination of both can be implemented with some clever crafting of the policy file. As well, you could take advantage of both solutions with multiple buckets. Keep in mind that data in all buckets contribute towards the total storage allocation equally. Once an account's storage quota is reached, no files can be added to any bucket owned by that account until files are removed. Are Automatic Backups to LTS Available? \u00b6 Automatic backups are not available by default. If you would like to periodically sync your bucket to a directory on your local machine or Cheaha, you will need to set up a cron task to submit a Slurm job that will run a sync. IF you would like to implement this for your own bucket, please contact us . Why Can I Not Interact With A File In My Bucket? \u00b6 While S3's object storage system does not have POSIX permissions seen in a Linux system entirely, we have found that users who upload files to a shared space have ownership permissions on those objects, and the bucket owner and stewards cannot interact with those objects by default. Instead, owners and stewards need to be given explicit permissions to move or delete all objects in a bucket. This can be dealt with by adding the following sections to the policy file: { \"Sid\" : \"admin\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/example_core\" , \"arn:aws:iam:::user/account_owner@uab.edu\" ] }, \"Action\" : [ \"s3:*\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] }, { \"Sid\" : \"data-steward\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/steward_1@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" , \"s3:GetBucketPolicy\" , \"s3:PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] } How Can I Share A Bucket With All LTS Users? \u00b6 The following policy file will give read permission to all LTS users for all objects in a bucket: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"admin\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/account_owner@uab.edu\" ] }, \"Action\" : [ \"s3:*\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] }, { \"Sid\" : \"read-only-all\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/*\" ] }, \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] } ] } Can I Change Permissions On A Bucket Via Globus? \u00b6 As of now, there is no way to change permissions on a bucket via Globus . The only way to change permissions is via the command line.","title":"FAQ"},{"location":"data_management/lts/lts_faq/#lts-faq","text":"","title":"LTS FAQ"},{"location":"data_management/lts/lts_faq/#can-i-share-my-account-access-keys-with-other-people","text":"You should never share access keys with anyone. These should be treated similarly to your BlazerID and password. Sharing keys creates a point of vulnerability and if they fall into a nefarious actor's hands, all buckets that account owns and the data in them can be deleted. In some cases, you may not be actively managing data in a bucket even though you own the account which owns a shared bucket. Instead of sharing keys with a data steward, instead that steward should be given admin-esque permissions on the required bucket via a policy file.","title":"Can I Share My Account Access Keys With Other People?"},{"location":"data_management/lts/lts_faq/#how-should-i-organize-my-lts-shared-account","text":"This is ultimately up to the bucket owner, but there are a couple of single-bucket solutions depending on your specific use-case for LTS: Semi-synced copy of everything in the project space. General permissions: Data stewards and the bucket owner would have permission to delete any files. All other users would be able to upload and download files only. All users would be able to see all files uploaded by all other users to that bucket./ Purpose: This fulfills more of a pure backup role compared to option 2. While all users can upload files Benefits: The policy file for these permissions is much simpler to create and manage. Limits the number of people who can remove files that might be needed down the line. Drawbacks: Needing to ask a steward or the bucket owner to delete individual files introduces friction to data management. Example Policy File Active and collaborative external storage. All users would have a specific prefix/folder they have complete control where they can add or remove data at will. General permissions: Data stewards and the bucket owner would have permission to delete any files. Regular users would only be able to upload to and delete files from their owned prefix/folder. All users would be able to see and download any files from any other user. Purpose: This satisfies the need for expanded storage accessible from Cheaha (via the terminal or Globus). All users have their own space they can use as they see fit within the bucket for extra storage while still being able to access, but not alter, files from other users in cases they need to be shared. Part of the bucket, or a separate bucket entirely, can also be used as a backup for old or current datasets where users only have read permissions. Benefits: How the bucket can be used is much more malleable and up to the individual users. Empowers them to add and remove data from their own prefix/folder without oversight from stewards or the bucket owner. Drawbacks: The policy file is more difficult to craft and manage when researchers needed to be added or removed from the bucket. Allowing users to delete their uploaded data at their discretion may conflict with the owner's view of those data. Example Policy File While these are two simple solutions, a combination of both can be implemented with some clever crafting of the policy file. As well, you could take advantage of both solutions with multiple buckets. Keep in mind that data in all buckets contribute towards the total storage allocation equally. Once an account's storage quota is reached, no files can be added to any bucket owned by that account until files are removed.","title":"How Should I Organize My LTS Shared Account?"},{"location":"data_management/lts/lts_faq/#are-automatic-backups-to-lts-available","text":"Automatic backups are not available by default. If you would like to periodically sync your bucket to a directory on your local machine or Cheaha, you will need to set up a cron task to submit a Slurm job that will run a sync. IF you would like to implement this for your own bucket, please contact us .","title":"Are Automatic Backups to LTS Available?"},{"location":"data_management/lts/lts_faq/#why-can-i-not-interact-with-a-file-in-my-bucket","text":"While S3's object storage system does not have POSIX permissions seen in a Linux system entirely, we have found that users who upload files to a shared space have ownership permissions on those objects, and the bucket owner and stewards cannot interact with those objects by default. Instead, owners and stewards need to be given explicit permissions to move or delete all objects in a bucket. This can be dealt with by adding the following sections to the policy file: { \"Sid\" : \"admin\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/example_core\" , \"arn:aws:iam:::user/account_owner@uab.edu\" ] }, \"Action\" : [ \"s3:*\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] }, { \"Sid\" : \"data-steward\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/steward_1@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" , \"s3:GetBucketPolicy\" , \"s3:PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] }","title":"Why Can I Not Interact With A File In My Bucket?"},{"location":"data_management/lts/lts_faq/#how-can-i-share-a-bucket-with-all-lts-users","text":"The following policy file will give read permission to all LTS users for all objects in a bucket: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"admin\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/account_owner@uab.edu\" ] }, \"Action\" : [ \"s3:*\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] }, { \"Sid\" : \"read-only-all\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/*\" ] }, \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket\" , \"arn:aws:s3:::bucket/*\" ] } ] }","title":"How Can I Share A Bucket With All LTS Users?"},{"location":"data_management/lts/lts_faq/#can-i-change-permissions-on-a-bucket-via-globus","text":"As of now, there is no way to change permissions on a bucket via Globus . The only way to change permissions is via the command line.","title":"Can I Change Permissions On A Bucket Via Globus?"},{"location":"data_management/lts/policies/","text":"Sharing Buckets \u00b6 A major use for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created. Instead, sharing buckets must be done through the command line using bucket policies . A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to objects within the bucket. If you have not worked with JSON files before, a brief explantion can be found here . It's important to note that the bucket owner will always retain the ability to perform all actions on a bucket and its contents and so do not need to be explicitly granted permissions. Important Your username for LTS could potentially be <blazerid> or <blazerid>@uab.edu depending on when your account was created. It is very important when crafting these policies that the correct username is specified, and these two are not interchangeable. For users with XIAS accounts, your username should be the email address you signed up for the XIAS account with. The usernames are case-sensitive. If you do not remember what your username is, see the email you received with your access key and secret key information or submit a support ticket to support@listserv.uab.edu. Policy Structure \u00b6 Policies files are essentially built as a series of statements expressly allowing or deny access to functions that interact with objects in S3. A skeleton policy file would look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"description\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : []}, \"Action\" : [], \"Resource\" : [] }] } Each statement is made up of a few fields: Sid: a short decription of what the statement is for (i.e. \"bucket-access\") Effect: \"Allow\" or \"Deny\" permissions based on how you want to alter permissions Principal: Essentially a list of users to change permissions for. Have to formatted like arn:aws:iam:::user/<lts_username> . Action: A list of commands to allow or deny permission for, depending on the Effect value. Resource: The name of the bucket or objects to apply permissions to. Must be formatted like arn:aws:s3:::<bucket[/path/objects]> . It is currently suggested to have at least two statements, one statement allowing access to the bucket itself, and another statement dictating permissions for objects in the bucket. For example, if you wanted to give users bob and jane@uab.edu the ability to list objects in your bucket bucket1 , the statement would be: { \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] } Critically, this does not allow the given users the ability to read, download, edit, or delete any objects in the bucket. They will be able to list the objects, see the names, sizes, and directory structure but will not be able to interact with the objects. These permissions should be enumerated in a separate statement like: { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] } This permission set allows bob and jane@uab.edu to download files but not to move, overwrite, delete, or otherwise interact with them. Notice that the Resource value has changed from just bucket1 to bucket1/* . We can set this Resource value to different paths and objects to limit the permissions granted. For example: bucket1/* : Apply permissions to all objects in the entire bucket bucket1/test_folder/* : Apply permissions to all objects in folder test_folder bucket1/test_folder/*jpg : Apply read permissions to only JPGs within test_folder . For the last two examples, bob and jane@uab.edu will not have permission to download any files outside of the test_folder folder. All permissions are implicitly denied unless explicitly given in the policy statements. Common Actions \u00b6 Being able to download a file is only one possible action you may want to give permission for. Uploading files as well as altering the policy of the bucket may also be useful to give permissions for. Here is a short list of common actions you may want to give permissions for: s3:ListBucket : access to see but not interact with objects s3:GetObject : download objects s3:PutObject : upload objects s3:DeleteObject : remove objects s3:GetBucketPolicy : view the current bucket policy s3:PutBucketPolicy : change the current bucket policy A full list of Actions for UAB LTS can be seen on the Ceph docs . Example Policies \u00b6 Read-Only for All Files \u00b6 This will give permissions to bob and jane@uab.edu for bucket bucket1 . The permissions will include bucket access (the list-bucket statement) as well as read-only permissions for all objects in the bucket (the read-only statement). Specifically, they will be able to copy the files from the bucket to another bucket or a local file system. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] } Read-Write Permissions \u00b6 This will give read, write, and delete permissions to bob and jane@uab.edu so they are able to sync directories between a local source folder and the S3 destination. This can be dangerous because of the delete permissions so care should be given in handing out that permission. s3:DeleteObject can be set into another statement field and limited to very select users while giving upload permission to many users. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] } Tiered Permissions \u00b6 In some instances, the bucket owner (i.e. ideally the PI for the lab if this is a shared lab space) will want to allow certain users to have permissions to alter the policies for new or departing lab members. This example will give standard read-write permissions to both our lab members, but only policy altering permissions to jane@uab.edu . { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"change-policy\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetBucketPolicy\" , \"s3:PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] } Applying a Policy \u00b6 Policies can be applied to a bucket either by the owner or by a user who has been given the s3:PutBucketPolicy permission. Use s3cmd to apply policies. Note As of now, rclone has not implemented a way to alter policies, and AWS CLI does not apply them to our system correctly. If you have been using rclone or AWS CLI, you will need to configure s3cmd for policy management instead Applying a policy is fairly straightforward for both tools. First, you should save the policy as a JSON file. The, you can interact with the policy using these commands: # s3cmd set policy s3cmd setpolicy <policy_file> s3://<bucket> # s3cmd view policy s3cmd info s3://<bucket> # s3cmd remove policy s3cmd delpolicy s3://<bucket> Policy Suggestions \u00b6 Important Policies can be very complicated depending on how many people need access to the bucket and how you want to tier permissions (i.e. which people are read-only, read-write, admin-esq priveleges, etc.). If you need help structuring your policy files please visit us during office hours and we will be happy to help structure your policy file to your needs. Admin-esq Priveleges \u00b6 It is suggested to keep the number of people who have permission to delete data and alter policies to a minimum. Inexperience with policies can result in permissions being granted to incorrect users which can potentially lead to irrecoverable consequences. Syncing data without purposeful thought can result in the undesired loss of data. Bucket Ownership \u00b6 For labs using LTS to store data from their Cheaha Project Storage directory, it is highly advised that the PI for the lab creates and owns the bucket and then gives policy changing permissions to another researcher for day-to-day maintenance if desired. For instance, if a lab manager creates the bucket and then leaves the university without giving policy permissions to other users, the lab will not be able to change the policies for those data. Sharing Multiple Datasets with Different Groups \u00b6 Some groups on campus may distribute datasets to other research groups using LTS. If you are distributing data to multiple groups, and those groups should not have access to each other's data, it is highly advised to store those datasets in separate buckets as opposed to separate directories in a single bucket. An idiosyncrasy of buckets involves the fact that all objects are stored in the top level of the bucket, and once permissions are given to someone to see the bucket, they will be able to see all objects within the bucket without restrictions even if they are not given download permissions for some objects. If any identifying or priveleged information is given in file names on LTS, it could constitute an IRB violation. Additionally, managing permissions for groups to access data only from specific folders makes the policy file much more complicated and prone to errors. When sharing multiple datasets with multiple different groups, it's advised to keep these data in separate buckets and have individual policy files for each bucket to make policy management simpler and less prone to error.","title":"Sharing"},{"location":"data_management/lts/policies/#sharing-buckets","text":"A major use for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created. Instead, sharing buckets must be done through the command line using bucket policies . A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to objects within the bucket. If you have not worked with JSON files before, a brief explantion can be found here . It's important to note that the bucket owner will always retain the ability to perform all actions on a bucket and its contents and so do not need to be explicitly granted permissions. Important Your username for LTS could potentially be <blazerid> or <blazerid>@uab.edu depending on when your account was created. It is very important when crafting these policies that the correct username is specified, and these two are not interchangeable. For users with XIAS accounts, your username should be the email address you signed up for the XIAS account with. The usernames are case-sensitive. If you do not remember what your username is, see the email you received with your access key and secret key information or submit a support ticket to support@listserv.uab.edu.","title":"Sharing Buckets"},{"location":"data_management/lts/policies/#policy-structure","text":"Policies files are essentially built as a series of statements expressly allowing or deny access to functions that interact with objects in S3. A skeleton policy file would look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"description\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : []}, \"Action\" : [], \"Resource\" : [] }] } Each statement is made up of a few fields: Sid: a short decription of what the statement is for (i.e. \"bucket-access\") Effect: \"Allow\" or \"Deny\" permissions based on how you want to alter permissions Principal: Essentially a list of users to change permissions for. Have to formatted like arn:aws:iam:::user/<lts_username> . Action: A list of commands to allow or deny permission for, depending on the Effect value. Resource: The name of the bucket or objects to apply permissions to. Must be formatted like arn:aws:s3:::<bucket[/path/objects]> . It is currently suggested to have at least two statements, one statement allowing access to the bucket itself, and another statement dictating permissions for objects in the bucket. For example, if you wanted to give users bob and jane@uab.edu the ability to list objects in your bucket bucket1 , the statement would be: { \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] } Critically, this does not allow the given users the ability to read, download, edit, or delete any objects in the bucket. They will be able to list the objects, see the names, sizes, and directory structure but will not be able to interact with the objects. These permissions should be enumerated in a separate statement like: { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] } This permission set allows bob and jane@uab.edu to download files but not to move, overwrite, delete, or otherwise interact with them. Notice that the Resource value has changed from just bucket1 to bucket1/* . We can set this Resource value to different paths and objects to limit the permissions granted. For example: bucket1/* : Apply permissions to all objects in the entire bucket bucket1/test_folder/* : Apply permissions to all objects in folder test_folder bucket1/test_folder/*jpg : Apply read permissions to only JPGs within test_folder . For the last two examples, bob and jane@uab.edu will not have permission to download any files outside of the test_folder folder. All permissions are implicitly denied unless explicitly given in the policy statements.","title":"Policy Structure"},{"location":"data_management/lts/policies/#common-actions","text":"Being able to download a file is only one possible action you may want to give permission for. Uploading files as well as altering the policy of the bucket may also be useful to give permissions for. Here is a short list of common actions you may want to give permissions for: s3:ListBucket : access to see but not interact with objects s3:GetObject : download objects s3:PutObject : upload objects s3:DeleteObject : remove objects s3:GetBucketPolicy : view the current bucket policy s3:PutBucketPolicy : change the current bucket policy A full list of Actions for UAB LTS can be seen on the Ceph docs .","title":"Common Actions"},{"location":"data_management/lts/policies/#example-policies","text":"","title":"Example Policies"},{"location":"data_management/lts/policies/#read-only-for-all-files","text":"This will give permissions to bob and jane@uab.edu for bucket bucket1 . The permissions will include bucket access (the list-bucket statement) as well as read-only permissions for all objects in the bucket (the read-only statement). Specifically, they will be able to copy the files from the bucket to another bucket or a local file system. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-only\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] }","title":"Read-Only for All Files"},{"location":"data_management/lts/policies/#read-write-permissions","text":"This will give read, write, and delete permissions to bob and jane@uab.edu so they are able to sync directories between a local source folder and the S3 destination. This can be dangerous because of the delete permissions so care should be given in handing out that permission. s3:DeleteObject can be set into another statement field and limited to very select users while giving upload permission to many users. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] }","title":"Read-Write Permissions"},{"location":"data_management/lts/policies/#tiered-permissions","text":"In some instances, the bucket owner (i.e. ideally the PI for the lab if this is a shared lab space) will want to allow certain users to have permissions to alter the policies for new or departing lab members. This example will give standard read-write permissions to both our lab members, but only policy altering permissions to jane@uab.edu . { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"list-bucket\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"change-policy\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetBucketPolicy\" , \"s3:PutBucketPolicy\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1\" ] }, { \"Sid\" : \"read-write\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : [ \"arn:aws:iam:::user/bob\" , \"arn:aws:iam:::user/jane@uab.edu\" ] }, \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::bucket1/*\" ] }] }","title":"Tiered Permissions"},{"location":"data_management/lts/policies/#applying-a-policy","text":"Policies can be applied to a bucket either by the owner or by a user who has been given the s3:PutBucketPolicy permission. Use s3cmd to apply policies. Note As of now, rclone has not implemented a way to alter policies, and AWS CLI does not apply them to our system correctly. If you have been using rclone or AWS CLI, you will need to configure s3cmd for policy management instead Applying a policy is fairly straightforward for both tools. First, you should save the policy as a JSON file. The, you can interact with the policy using these commands: # s3cmd set policy s3cmd setpolicy <policy_file> s3://<bucket> # s3cmd view policy s3cmd info s3://<bucket> # s3cmd remove policy s3cmd delpolicy s3://<bucket>","title":"Applying a Policy"},{"location":"data_management/lts/policies/#policy-suggestions","text":"Important Policies can be very complicated depending on how many people need access to the bucket and how you want to tier permissions (i.e. which people are read-only, read-write, admin-esq priveleges, etc.). If you need help structuring your policy files please visit us during office hours and we will be happy to help structure your policy file to your needs.","title":"Policy Suggestions"},{"location":"data_management/lts/policies/#admin-esq-priveleges","text":"It is suggested to keep the number of people who have permission to delete data and alter policies to a minimum. Inexperience with policies can result in permissions being granted to incorrect users which can potentially lead to irrecoverable consequences. Syncing data without purposeful thought can result in the undesired loss of data.","title":"Admin-esq Priveleges"},{"location":"data_management/lts/policies/#bucket-ownership","text":"For labs using LTS to store data from their Cheaha Project Storage directory, it is highly advised that the PI for the lab creates and owns the bucket and then gives policy changing permissions to another researcher for day-to-day maintenance if desired. For instance, if a lab manager creates the bucket and then leaves the university without giving policy permissions to other users, the lab will not be able to change the policies for those data.","title":"Bucket Ownership"},{"location":"data_management/lts/policies/#sharing-multiple-datasets-with-different-groups","text":"Some groups on campus may distribute datasets to other research groups using LTS. If you are distributing data to multiple groups, and those groups should not have access to each other's data, it is highly advised to store those datasets in separate buckets as opposed to separate directories in a single bucket. An idiosyncrasy of buckets involves the fact that all objects are stored in the top level of the bucket, and once permissions are given to someone to see the bucket, they will be able to see all objects within the bucket without restrictions even if they are not given download permissions for some objects. If any identifying or priveleged information is given in file names on LTS, it could constitute an IRB violation. Additionally, managing permissions for groups to access data only from specific folders makes the policy file much more complicated and prone to errors. When sharing multiple datasets with multiple different groups, it's advised to keep these data in separate buckets and have individual policy files for each bucket to make policy management simpler and less prone to error.","title":"Sharing Multiple Datasets with Different Groups"},{"location":"data_management/transfer/filezilla/","text":"FileZilla \u00b6 Filezilla is a free SFTP platform used to transfer data between a local machine and a remote server, in this case remote storage accessible to Cheaha and UAB Cloud or long-term S3 storage. Filezilla is useful for medium sized data transfers from a local machine as opposed to Globus which can easily connect two different local or remote servers and handle large quantities of data. Installation \u00b6 To download the program, go to the FileZilla site and click Download FileZilla Client. Filezilla is also already installed on Cheaha and can be accessed through the Accessories menu. Using FileZilla \u00b6 Once FileZilla is installed and you open it, you will see the following window A file browser for the local machine FileZilla is running on is on the left while the file system for the remote site will be shown on the right once a remote site has been connected. Creating a Remote Connection \u00b6 You can easily connect to a remote site using the QuickConnect bar near the top of the window. You will need to input the following options: Host: sftp://cheaha.rc.uab.edu Username: your BlazerID or XIAS ID (do NOT include '@uab.edu') Password: your BlazerID or XIAS password Port: 22 Click Quickconnect and you will be connected to the remote storage system. The window should now look like: When connecting in the future, you will be able to select the connection from the dropdown arrow next to the Quickconnect button. In both the local and remote panes, you can navigate to the directories you are transferring from and to. You only have access to directories you can normally access on Cheaha, so your Individual Storage as well as any Project Storage directories you have been added to. Transferring Data \u00b6 From here you can drag and drop whichever files and directories between the local and remote site windows. This will automatically initiate a file transfer. The directory structure is maintained from the source to the destination and is recursive, so all subdirectories and folders within the main directory will be transferred as well. Transfer status will be logged at the bottom of the window. Once all your files have been transferred, you can close FileZilla which will close the SFTP connection.","title":"FileZilla"},{"location":"data_management/transfer/filezilla/#filezilla","text":"Filezilla is a free SFTP platform used to transfer data between a local machine and a remote server, in this case remote storage accessible to Cheaha and UAB Cloud or long-term S3 storage. Filezilla is useful for medium sized data transfers from a local machine as opposed to Globus which can easily connect two different local or remote servers and handle large quantities of data.","title":"FileZilla"},{"location":"data_management/transfer/filezilla/#installation","text":"To download the program, go to the FileZilla site and click Download FileZilla Client. Filezilla is also already installed on Cheaha and can be accessed through the Accessories menu.","title":"Installation"},{"location":"data_management/transfer/filezilla/#using-filezilla","text":"Once FileZilla is installed and you open it, you will see the following window A file browser for the local machine FileZilla is running on is on the left while the file system for the remote site will be shown on the right once a remote site has been connected.","title":"Using FileZilla"},{"location":"data_management/transfer/filezilla/#creating-a-remote-connection","text":"You can easily connect to a remote site using the QuickConnect bar near the top of the window. You will need to input the following options: Host: sftp://cheaha.rc.uab.edu Username: your BlazerID or XIAS ID (do NOT include '@uab.edu') Password: your BlazerID or XIAS password Port: 22 Click Quickconnect and you will be connected to the remote storage system. The window should now look like: When connecting in the future, you will be able to select the connection from the dropdown arrow next to the Quickconnect button. In both the local and remote panes, you can navigate to the directories you are transferring from and to. You only have access to directories you can normally access on Cheaha, so your Individual Storage as well as any Project Storage directories you have been added to.","title":"Creating a Remote Connection"},{"location":"data_management/transfer/filezilla/#transferring-data","text":"From here you can drag and drop whichever files and directories between the local and remote site windows. This will automatically initiate a file transfer. The directory structure is maintained from the source to the destination and is recursive, so all subdirectories and folders within the main directory will be transferred as well. Transfer status will be logged at the bottom of the window. Once all your files have been transferred, you can close FileZilla which will close the SFTP connection.","title":"Transferring Data"},{"location":"data_management/transfer/globus/","text":"Globus \u00b6 Globus is a powerful tool for robustly and securely managing data transfers to and from collaborators and within UAB Research Computing. Globus is recommended for most single-use, day-to-day data transfer use-cases. UAB Research Computing uses High Assurance Endpoints and Collections, meaning there are additional security measures in place to reduce risk and move toward HIPAA compliance. Generally speaking, if you have used Globus in the past, the data transfer interface has not changed, but there are a few new restrictions. You will be prompted to prove authorization each time you access a UAB Research Computing endpoint, collection or attempt to download files to your local machine from such an endpoint or collection. If you are already logged in with Single Sign-On (SSO) the process is simple. If not, you will need to authenticate with SSO. Bookmarks are not allowed in High Assurance endpoints and collections. For more detailed information on High Assurance please see the Globus official pages below: High Assurance Security Overview High Assurance Collections Setting up Globus Connect Personal \u00b6 Globus Connect Personal is software meant to be installed on local machines such as laptops, desktops, workstations and self-owned, local-scale servers. Globus maintains excellent documentation for installation on MacOS , Linux and Windows . To verify your installation is complete, please visit https://app.globus.org and log in. Click \"Endpoints\" in the left-hand navigation pane and then click the \"Administered By You\" tab. Look in the table for the endpoint you just created. Managing Identities \u00b6 Globus Identities is a concept helping to map Globus Accounts (one per person) to institutions (one or more per person). Most UAB researchers will have a single identity, their UAB identity, tied to their blazerid. Some researchers may have external collaborations or appointments that provide additional entities which need access to other endpoints on Globus. To manage your identities, navigate to https://app.globus.org/account/identities and sign in. Important To use UAB Research Computing endpoints and collections, you will need to ensure you are using you UAB identity. Moving Data Between Endpoints \u00b6 Log in to the Globus App online at https://app.globus.org using UAB Single Sign-On (SSO). Start typing \"University of Alabama at Birmingham\" into the \"Use your existing organizational login\" text box and selected it when it appears in the list. Click File Manager in the left-hand navigation pane. Ensure the center icon for the \"Panels\" selection is picked. Click the \"Search\" icon in the \"Collection\" text box near the top-left or top-right of the page to locate an endpoint. There are multiple ways to find an endpoint. For some endpoints you may be asked to log in, which is true of all UAB endpoints. Some UAB endpoints may also require that you be on the UAB Campus VPN. Begin typing in the box to search for an endpoint. To find UAB-related endpoints, search for \"UAB\". There are two Cheaha endpoints Cheaha cluster on-campus (UAB Science DMZ) for machines that are either on the UAB Campus Network, or connected to the UAB Campus VPN. Cheaha cluster off-campus (UAB Science DMZ) for machines that are not on the UAB Campus Network and not on the UAB Campus VPN. The \"Recent\" tab shows endpoints that have most recently been used. The \"Bookmarks\" tab shows a list of endpoint bookmarks. Bookmarks may not reference folders within UAB Research Computing or other High Assurance endpoints or collections. The \"Your Collections\" tab shows all endpoints owned by you. For most researchers this will be one or more Globus Connect Personal endpoints. The \"Shared With You\" tab shows any private endpoints that have been shared with you by other users, possibly collaborators. The \"More Options\" tab will show a brief text on installing Globus Connect Personal. When an endpoint has been selected you will see a list of folders and files on the default path for that endpoint in the bottom box. You can use the \"Path\" box to type a path to find the files you are looking for. Repeat the process of selecting an endpoint for the other \"Collection\" text box. When both endpoints have been selected and you have chosen the correct paths for each endpoint, select files and/or folders on the side you wish to transfer FROM. We will call this side the source endpoint, and the other side the target endpoint. Selections may be made by clicking the checkboxes that appear when you hover over each file or folder. When all files and folders have been selected from the source endpoint, click the \"Start\" button on the source endpoint side. This will start a transfer process from source to target. The files will be placed in the currently open path on the target endpoint. A green pop-up notification will appear indicating the transfer has started. Click \"View details >\" to be taken to the status of the transfer. You can also check on the status of any transfers by clicking the \"Activity\" button in the left-hand navigation pane. Note File permissions from the source will not be copied to the destination. Please read more at this ask.ci FAQ . Transfer and Sync Options \u00b6 Between the two \"Start\" buttons on the \"File Manager\" page is a \"Transfer & Sync Options\" drop down menu. Click that button to change the options. More information on each option. A brief summary of the options are... sync - Sync files only, rather than create new files. delete files - Delete any files on the target that are not on the source. Useful for forcing identical filesystems when syncing. preserve source - Copies file \"modified time\" metadata. verify integrity - Verifies that checksums are identical on source and target after transfer completes. Highly recommended to have this checked. encrypt transfer - Encrypts data before leaving source and decrypts after arriving at destination. Recommended for all transfers, required and enforced for all UAB endpoints. skip files - Skips source files that cause errors during the transfer. Otherwise the entire transfer will stop when an error is encountered. quota fail - Fails instead of retries when the target storage quota is exceeded. Common Errors \u00b6 File Not Found - This may mean that a file was not readable by Globus. Check that the file hasn't moved or changed names during the transfer. It is recommended to not modify files while they are being transferred by Globus. Permission Denied - Globus is not able to access the files because permissions do not allow it. For Globus Connect Personal, be sure the containing folder is on the \"Accessible Folders\" list. Be sure that your Cheaha account has access to read the file. Project Space Permissions \u00b6 Globus does not preserve permissions nor ownership when data is transferred, instead using whatever permissions are default at the target location, and making the owner the authenticated user who initiated the transfer. Typically this is not an issue, but may cause problems for Project Storage directories . Please see our Project Directory Permissions Section for more information. More Information \u00b6 A Globus FAQ is available for additional information on endpoints and transfers. Connectors \u00b6 UAB Researcher Computing has subscriptions to connectors for cloud services and other types of filesystems. UAB Box Connector \u00b6 To use the UAB Box Connector, search for an endpoint like usual and enter \"UAB Box\" into the search box. Select the endpoint labeled \"UAB Box\". You should see a list of files and folders that are available to you at https://uab.app.box.com . File transfers work as they would with any other endpoint or collection. Long-term Storage S3 (LTS) Connector \u00b6 Important LTS behaves differently from other file systems and comes with a few possible pitfalls. Keep in mind the following three rules: (1) all data must be in buckets, (2) buckets are only allowed in the root folder, and (3) buckets must have unique names. To use the UAB LTS Connector, search for an endpoint like usual and enter \"UAB LTS\" into the search box. Select the endpoint labeled \"UAB Research Computing LTS (Long Term Storage aka S3)\". If you have stored data within LTS already you should see a list of folders, otherwise you will see an empty space where folders may be placed. Each folder corresponds to a bucket in LTS. To create a bucket, click \"New Folder\" in the \"File Manager\" window in Globus. Note that buckets must have globally unique names. Read on for more information about possible pitfalls. Data Must be in Buckets \u00b6 All data transferred to LTS must be placed in a bucket, and may not be placed directly into the root directory. Attempting to move data to the root directory will result in an unhelpful error message in the \"Activity\" window. Clicking on the \"view event log\" link shows the following. Error (transfer) Endpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3) Server: m-a201b5.9ad93.a567.data.globus.org:443 Command: STOR /test.log Message: The connection to the server was broken --- Details: an end-of-file was reached\\nglobus_xio: An end of file occurred\\n Buckets Must Have Globally Unique Names \u00b6 When creating new buckets, the name must be unique across all buckets on the system. At first this may sound very restrictive, but it is quite simple to deal with in practice. See our LTS section on good naming practice for how to avoid duplicate names. If a duplicate bucket name is entered, a long error message will appear in a small space next to the new bucket name. The message reads like the following, expanded for readability. Bad Gateway: Endpoint Error, Error (mkdir) Endpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3) Server: m-b81a79.9ad93.a567.data.globus.org:443 Command: MKD /test/ Message: Fatal FTP Response --- Details: 553- GlobusError: v=1 c=PATH_EXISTS\\r\\n553- GridFTP-Path: (null)\\r\\n553-globus_gridftp_server_s3_base: S3 Error accessing \"\": ErrorBucketAlreadyExists: ErrorBucketAlreadyExists: \\r\\n553 End.\\r\\n Using Bookmarks \u00b6 To save a bookmark, use the File Manager interface to select an endpoint and navigate to a path on that endpoint. Then click the bookmark button as shown below. To manage bookmarks, click \"Bookmarks\" in the left-hand navigation pane. Click the \"Pencil\" icon to edit a bookmark. Click the \"Trash Bin\" icon to delete a bookmark. Note It is not possible to create bookmarks within High Assurance Endpoints. Managing Shared Collections from a Globus Connect Personal Endpoint \u00b6 It is NOT RECOMMENDED to make Globus Connect Personal endpoints public as this is insecure. It is more difficult to manage access controls for the entire Globus Connect Personal endpoint than for a shared collection. Shared collections make it simpler to share different data with distinct collaborators, and to manage who has access to what data. Be secure, use shared collections! Creating a Shared Collection \u00b6 Click \"Endpoints\" in the left-hand navigation pane. Click the \"Administered By You\" tab. In the table, find the endpoint you wish to share data from and click its name. You will be taken to the page for that endpoint. Click the \"Collections\" tab. Click the \"Add a Guest Collection\" button. Fill out the form. Manually enter a path or click the Browse button to select a folder. Give a short but memorable name for your shared collection. This information will be useful for your collaborators. Optionally fill in a more detailed description of the shared collection for your records. Optionally fill in searchable keywords. Click \"Create Share\" to move to the next step. You will be taken to the page for the newly created collection, which is now a full-fledged endpoint. Any further references to \"an endpoint\" will be about the newly created, shared collection. Make sure you are on the \"Permissions\" tab. You should see a permissions table with your name in the first row. Click \"Add Permissions -- Share With\" to share your endpoint with other users. Fill out the form. Optionally enter a path within the shared endpoint or use the Browse button. If you leave the path as just a slash, the entire shared endpoint will be shared with these users. Select who to share with. User - One or more users. Group - All members of a group. All Users - All globus users. Danger This will expose information to everyone on Globus! Search for users to add, or a group, depending on your choice above. You should be able to find any globus user using the search box. Warning Be certain of which user you are selecting! Check the email address domain. If adding users, optionally enter a message so they know why they are being added. Select permissions. Read is automatically selected and cannot be changed. Write permissions are optional. Click \"Add Permission\" to add permissions for these users or groups. You will be returned to the page for the shared endpoint and should be on the \"Permissions\" tab and should see the user or group in the table. Deleting a Shared Collection \u00b6 Click \"Endpoints\" in the left-hand navigation pane, then c Click the \"Administered By You\" tab. Click the right caret \">\" icon at the right side of the row with the endpoint you wish to delete. You will be taken to the information page for that endpoint. Click \"X Delete Endpoint\" and a confirmation dialog will open at the top of the page. Respond to the dialog to delete the endpoint, or to cancel.","title":"Globus"},{"location":"data_management/transfer/globus/#globus","text":"Globus is a powerful tool for robustly and securely managing data transfers to and from collaborators and within UAB Research Computing. Globus is recommended for most single-use, day-to-day data transfer use-cases. UAB Research Computing uses High Assurance Endpoints and Collections, meaning there are additional security measures in place to reduce risk and move toward HIPAA compliance. Generally speaking, if you have used Globus in the past, the data transfer interface has not changed, but there are a few new restrictions. You will be prompted to prove authorization each time you access a UAB Research Computing endpoint, collection or attempt to download files to your local machine from such an endpoint or collection. If you are already logged in with Single Sign-On (SSO) the process is simple. If not, you will need to authenticate with SSO. Bookmarks are not allowed in High Assurance endpoints and collections. For more detailed information on High Assurance please see the Globus official pages below: High Assurance Security Overview High Assurance Collections","title":"Globus"},{"location":"data_management/transfer/globus/#setting-up-globus-connect-personal","text":"Globus Connect Personal is software meant to be installed on local machines such as laptops, desktops, workstations and self-owned, local-scale servers. Globus maintains excellent documentation for installation on MacOS , Linux and Windows . To verify your installation is complete, please visit https://app.globus.org and log in. Click \"Endpoints\" in the left-hand navigation pane and then click the \"Administered By You\" tab. Look in the table for the endpoint you just created.","title":"Setting up Globus Connect Personal"},{"location":"data_management/transfer/globus/#managing-identities","text":"Globus Identities is a concept helping to map Globus Accounts (one per person) to institutions (one or more per person). Most UAB researchers will have a single identity, their UAB identity, tied to their blazerid. Some researchers may have external collaborations or appointments that provide additional entities which need access to other endpoints on Globus. To manage your identities, navigate to https://app.globus.org/account/identities and sign in. Important To use UAB Research Computing endpoints and collections, you will need to ensure you are using you UAB identity.","title":"Managing Identities"},{"location":"data_management/transfer/globus/#moving-data-between-endpoints","text":"Log in to the Globus App online at https://app.globus.org using UAB Single Sign-On (SSO). Start typing \"University of Alabama at Birmingham\" into the \"Use your existing organizational login\" text box and selected it when it appears in the list. Click File Manager in the left-hand navigation pane. Ensure the center icon for the \"Panels\" selection is picked. Click the \"Search\" icon in the \"Collection\" text box near the top-left or top-right of the page to locate an endpoint. There are multiple ways to find an endpoint. For some endpoints you may be asked to log in, which is true of all UAB endpoints. Some UAB endpoints may also require that you be on the UAB Campus VPN. Begin typing in the box to search for an endpoint. To find UAB-related endpoints, search for \"UAB\". There are two Cheaha endpoints Cheaha cluster on-campus (UAB Science DMZ) for machines that are either on the UAB Campus Network, or connected to the UAB Campus VPN. Cheaha cluster off-campus (UAB Science DMZ) for machines that are not on the UAB Campus Network and not on the UAB Campus VPN. The \"Recent\" tab shows endpoints that have most recently been used. The \"Bookmarks\" tab shows a list of endpoint bookmarks. Bookmarks may not reference folders within UAB Research Computing or other High Assurance endpoints or collections. The \"Your Collections\" tab shows all endpoints owned by you. For most researchers this will be one or more Globus Connect Personal endpoints. The \"Shared With You\" tab shows any private endpoints that have been shared with you by other users, possibly collaborators. The \"More Options\" tab will show a brief text on installing Globus Connect Personal. When an endpoint has been selected you will see a list of folders and files on the default path for that endpoint in the bottom box. You can use the \"Path\" box to type a path to find the files you are looking for. Repeat the process of selecting an endpoint for the other \"Collection\" text box. When both endpoints have been selected and you have chosen the correct paths for each endpoint, select files and/or folders on the side you wish to transfer FROM. We will call this side the source endpoint, and the other side the target endpoint. Selections may be made by clicking the checkboxes that appear when you hover over each file or folder. When all files and folders have been selected from the source endpoint, click the \"Start\" button on the source endpoint side. This will start a transfer process from source to target. The files will be placed in the currently open path on the target endpoint. A green pop-up notification will appear indicating the transfer has started. Click \"View details >\" to be taken to the status of the transfer. You can also check on the status of any transfers by clicking the \"Activity\" button in the left-hand navigation pane. Note File permissions from the source will not be copied to the destination. Please read more at this ask.ci FAQ .","title":"Moving Data Between Endpoints"},{"location":"data_management/transfer/globus/#transfer-and-sync-options","text":"Between the two \"Start\" buttons on the \"File Manager\" page is a \"Transfer & Sync Options\" drop down menu. Click that button to change the options. More information on each option. A brief summary of the options are... sync - Sync files only, rather than create new files. delete files - Delete any files on the target that are not on the source. Useful for forcing identical filesystems when syncing. preserve source - Copies file \"modified time\" metadata. verify integrity - Verifies that checksums are identical on source and target after transfer completes. Highly recommended to have this checked. encrypt transfer - Encrypts data before leaving source and decrypts after arriving at destination. Recommended for all transfers, required and enforced for all UAB endpoints. skip files - Skips source files that cause errors during the transfer. Otherwise the entire transfer will stop when an error is encountered. quota fail - Fails instead of retries when the target storage quota is exceeded.","title":"Transfer and Sync Options"},{"location":"data_management/transfer/globus/#common-errors","text":"File Not Found - This may mean that a file was not readable by Globus. Check that the file hasn't moved or changed names during the transfer. It is recommended to not modify files while they are being transferred by Globus. Permission Denied - Globus is not able to access the files because permissions do not allow it. For Globus Connect Personal, be sure the containing folder is on the \"Accessible Folders\" list. Be sure that your Cheaha account has access to read the file.","title":"Common Errors"},{"location":"data_management/transfer/globus/#project-space-permissions","text":"Globus does not preserve permissions nor ownership when data is transferred, instead using whatever permissions are default at the target location, and making the owner the authenticated user who initiated the transfer. Typically this is not an issue, but may cause problems for Project Storage directories . Please see our Project Directory Permissions Section for more information.","title":"Project Space Permissions"},{"location":"data_management/transfer/globus/#more-information","text":"A Globus FAQ is available for additional information on endpoints and transfers.","title":"More Information"},{"location":"data_management/transfer/globus/#connectors","text":"UAB Researcher Computing has subscriptions to connectors for cloud services and other types of filesystems.","title":"Connectors"},{"location":"data_management/transfer/globus/#uab-box-connector","text":"To use the UAB Box Connector, search for an endpoint like usual and enter \"UAB Box\" into the search box. Select the endpoint labeled \"UAB Box\". You should see a list of files and folders that are available to you at https://uab.app.box.com . File transfers work as they would with any other endpoint or collection.","title":"UAB Box Connector"},{"location":"data_management/transfer/globus/#long-term-storage-s3-lts-connector","text":"Important LTS behaves differently from other file systems and comes with a few possible pitfalls. Keep in mind the following three rules: (1) all data must be in buckets, (2) buckets are only allowed in the root folder, and (3) buckets must have unique names. To use the UAB LTS Connector, search for an endpoint like usual and enter \"UAB LTS\" into the search box. Select the endpoint labeled \"UAB Research Computing LTS (Long Term Storage aka S3)\". If you have stored data within LTS already you should see a list of folders, otherwise you will see an empty space where folders may be placed. Each folder corresponds to a bucket in LTS. To create a bucket, click \"New Folder\" in the \"File Manager\" window in Globus. Note that buckets must have globally unique names. Read on for more information about possible pitfalls.","title":"Long-term Storage S3 (LTS) Connector"},{"location":"data_management/transfer/globus/#data-must-be-in-buckets","text":"All data transferred to LTS must be placed in a bucket, and may not be placed directly into the root directory. Attempting to move data to the root directory will result in an unhelpful error message in the \"Activity\" window. Clicking on the \"view event log\" link shows the following. Error (transfer) Endpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3) Server: m-a201b5.9ad93.a567.data.globus.org:443 Command: STOR /test.log Message: The connection to the server was broken --- Details: an end-of-file was reached\\nglobus_xio: An end of file occurred\\n","title":"Data Must be in Buckets"},{"location":"data_management/transfer/globus/#buckets-must-have-globally-unique-names","text":"When creating new buckets, the name must be unique across all buckets on the system. At first this may sound very restrictive, but it is quite simple to deal with in practice. See our LTS section on good naming practice for how to avoid duplicate names. If a duplicate bucket name is entered, a long error message will appear in a small space next to the new bucket name. The message reads like the following, expanded for readability. Bad Gateway: Endpoint Error, Error (mkdir) Endpoint: UAB Research Computing LTS (Long Term Storage aka S3) (184408b4-d04b-4513-9912-8feeb6adcab3) Server: m-b81a79.9ad93.a567.data.globus.org:443 Command: MKD /test/ Message: Fatal FTP Response --- Details: 553- GlobusError: v=1 c=PATH_EXISTS\\r\\n553- GridFTP-Path: (null)\\r\\n553-globus_gridftp_server_s3_base: S3 Error accessing \"\": ErrorBucketAlreadyExists: ErrorBucketAlreadyExists: \\r\\n553 End.\\r\\n","title":"Buckets Must Have Globally Unique Names"},{"location":"data_management/transfer/globus/#using-bookmarks","text":"To save a bookmark, use the File Manager interface to select an endpoint and navigate to a path on that endpoint. Then click the bookmark button as shown below. To manage bookmarks, click \"Bookmarks\" in the left-hand navigation pane. Click the \"Pencil\" icon to edit a bookmark. Click the \"Trash Bin\" icon to delete a bookmark. Note It is not possible to create bookmarks within High Assurance Endpoints.","title":"Using Bookmarks"},{"location":"data_management/transfer/globus/#managing-shared-collections-from-a-globus-connect-personal-endpoint","text":"It is NOT RECOMMENDED to make Globus Connect Personal endpoints public as this is insecure. It is more difficult to manage access controls for the entire Globus Connect Personal endpoint than for a shared collection. Shared collections make it simpler to share different data with distinct collaborators, and to manage who has access to what data. Be secure, use shared collections!","title":"Managing Shared Collections from a Globus Connect Personal Endpoint"},{"location":"data_management/transfer/globus/#creating-a-shared-collection","text":"Click \"Endpoints\" in the left-hand navigation pane. Click the \"Administered By You\" tab. In the table, find the endpoint you wish to share data from and click its name. You will be taken to the page for that endpoint. Click the \"Collections\" tab. Click the \"Add a Guest Collection\" button. Fill out the form. Manually enter a path or click the Browse button to select a folder. Give a short but memorable name for your shared collection. This information will be useful for your collaborators. Optionally fill in a more detailed description of the shared collection for your records. Optionally fill in searchable keywords. Click \"Create Share\" to move to the next step. You will be taken to the page for the newly created collection, which is now a full-fledged endpoint. Any further references to \"an endpoint\" will be about the newly created, shared collection. Make sure you are on the \"Permissions\" tab. You should see a permissions table with your name in the first row. Click \"Add Permissions -- Share With\" to share your endpoint with other users. Fill out the form. Optionally enter a path within the shared endpoint or use the Browse button. If you leave the path as just a slash, the entire shared endpoint will be shared with these users. Select who to share with. User - One or more users. Group - All members of a group. All Users - All globus users. Danger This will expose information to everyone on Globus! Search for users to add, or a group, depending on your choice above. You should be able to find any globus user using the search box. Warning Be certain of which user you are selecting! Check the email address domain. If adding users, optionally enter a message so they know why they are being added. Select permissions. Read is automatically selected and cannot be changed. Write permissions are optional. Click \"Add Permission\" to add permissions for these users or groups. You will be returned to the page for the shared endpoint and should be on the \"Permissions\" tab and should see the user or group in the table.","title":"Creating a Shared Collection"},{"location":"data_management/transfer/globus/#deleting-a-shared-collection","text":"Click \"Endpoints\" in the left-hand navigation pane, then c Click the \"Administered By You\" tab. Click the right caret \">\" icon at the right side of the row with the endpoint you wish to delete. You will be taken to the information page for that endpoint. Click \"X Delete Endpoint\" and a confirmation dialog will open at the top of the page. Respond to the dialog to delete the endpoint, or to cancel.","title":"Deleting a Shared Collection"},{"location":"data_management/transfer/rclone/","text":"RClone \u00b6 RClone is a powerful command line tool for transferring and synchronizing files over the internet between various machines, servers and cloud storage services. It is highly recommended for small to moderate amounts of data. For very large amounts of data consider using Globus for increased robustness against failure. Where Globus is not available, rclone is still suitable. RClone requires a modest amount of setup time on local machines, but once setup can be used fairly easily. RClone uses the concepts of \"remotes\", which is an abstract term for any storage service or device that is not physically part of the local machine. Many remotes are offered, including SFTP and various UAB Cloud Storage Solutions . SFTP may be used to access Cheaha, cloud.rc and other laptop and desktop computers. To use RClone effectively, you'll need to setup remotes before using the various commands. Most file manipulation commands on Linux can be found in the RClone commands, but may have slightly different names, e.g. cp is rclone copy . RClone is very powerful and, as such, has a wide variety of configuration options and flags to fine tune behavior. We will only cover the basics needed to install the software, setup remotes relevant to work at UAB, and some basic usage commands. Quick Tutorial for Cheaha \u00b6 To use RClone for simple data transfer to and from Cheaha, follow these steps: Load the module using module load rclone . Set up a remote storage provider Copy files using the rclone cp command . To transfer from Cheaha, use rclone cp :path/on/cheaha remote:path/on/dest To transfer to Cheaha, use rclone cp remote:/path/on/remote :path/on/cheaha . Be sure to replace remote with the name of the remote you set up in step 2. Installing \u00b6 Installing on Cheaha \u00b6 On Cheaha, RClone is already installed as a Module . Use module load rclone to load it. Installing on Linux and cloud.rc \u00b6 See Installing Software for general good practice on installing software, then use the following command. curl https://rclone.org/install.sh | sudo bash Open a new terminal and enter rclone to verify installation. Installing on Windows \u00b6 It is highly recommended to install rclone in Windows Subsystem for Linux (WSL). To instead install natively on Windows, you will need to use the following instructions. Download the appropriate version from the downloads page. Extract rclone.exe into a memorable folder on your system. Do not put it into Program Files . In the Start Menu type env and look for the application \"Edit the system Environment Variables\" to open the System Properties dialog. Click the \"Environment Variables...\" button. Under \"User variables for $USER\" find the variable \"Path\". Click \"Path\" to select it. Click the \"Edit...\" button to open a new dialog. Click the \"New\" button. Type in the folder path to rclone.exe as C:/path/to/rclone_folder , modified appropriately. Click the \"OK\" button to confirm and close the dialog box. Click the \"OK\" button to confirm and close the Environment Variables dialog box. Click the \"OK\" button to confirm and close the System Properties dialog box. Verify the installation by typing \"cmd\" in the Start Menu and opening the Command Prompt application. Type rclone and you should see the rclone help text. MacOS \u00b6 Follow the online instructions for installing with brew . Setting up Remotes \u00b6 RClone is capable of interfacing with many remote cloud services, as well as using sftp for connecting two personal computers or servers. We will only cover those cloud services relevant to UAB use. We will not cover how to connect to any other cloud services using RClone. More detailed information is available at the RClone documentation Important Cloud access tokens are always supplied with an expiration date for security reasons. You will need to repeat the setup process periodically to regain access via RClone. Note RClone has an unusual user interface, using alternating red and green blocks to differentiate list items. The colors do not convey any particular meaning beyond differentiation. Setting up an SFTP Remote \u00b6 RClone connects two personal computers or servers using SFTP which is built on SSH, so a lot of these instructions mirror what would be done with an SSH configuration. Generate a Key Pair for use with the remote machine. At the terminal enter rclone config . Follow the prompts to choose sftp . Enter the following values as they come up, using defaults for other values. name> Name of the remote for future reference and configuration host> Remote IP address or cheaha.rc.uab.edu for Cheaha user> The user you will log into on the remote machine key_file> The absolute path to the private key file on the local machine, something like ~/.ssh/private_key_ed25519 key_file_pass> The passphrase used to secure the private key file (optional, but highly recommended) Verify by using rclone lsd <name> . The official docuemntation for rclone sftp is here . Setting up UAB Cloud Remotes \u00b6 The setup process for UAB cloud remotes is generally the same, except for the specifics of authentication. The instruction template is outlined below and will point you to the authentication section specific to each remote when it becomes relevant. As you step through the process, you will ultimately open two terminal windows and a browser window, and will need to copy text between the terminal windows. The first terminal window will be used to setup the RClone cloud remote. The second terminal will be used to authenticate to that cloud service and gain a token that will be passed back to the first terminal. Authentication will happen by logging into the service in a browser window. This setup method is necessary for any machine where a browser is not readily available, such as a cloud.rc virtual machine. To facilitate setup on these machines, the second terminal will be opened on a machine with RClone and a browser. An example of what this setup might look like is given below. Important If you are using RClone in Windows Subsystem for Linux (WSL), you won't be able to open a browser using WSL. Instead, you will need to Install RClone on Windows and use the Windows Command Prompt terminal to use rclone authorize . Open a terminal on the device you wish to authorize to access the chosen cloud service provider using RClone. This terminal will be referred to as terminal-1. At terminal-1 enter rclone config . Follow the prompts to choose one of the following. The selection here will be used later and will be referred to as <remote> . UAB Box: select Box . <remote> will be replaced by box . UAB SharePoint Site: select Microsoft OneDrive . <remote> will be replaced by onedrive . UAB OneDrive: select Microsoft OneDrive . <remote> will be replaced by onedrive . Enter a short, memorable name for future reference when prompted with name> . Keep this <name> in mind as it will be how you access the remote when Using Commands . Press enter to leave all additional prompts blank until \"Use auto config?\". Type \"n\", for no, and press enter. The prompt should now read config_token> . On a machine with a browser, such as your personal computer, open a new terminal and enter rclone authorize \"<remote>\" . Replacing <remote> with the value from step (3). This terminal will be referred to as terminal-2. When the browser window opens, use it to authenticate to your selected service. Authenticate to UAB Box . Authenticate to Microsoft OneDrive . Other services not officially supported by UAB IT are possible, but are not documented here. You will need to provide your own credentials for these services. Terminal-2 will print a secret token, which will appear like in the following image. You will need to copy the portion highlighted in the image, between the lines with ---> and <--- . Copy and paste the token from the terminal-2 to terminal-1. Follow the remaining prompts. Verify success by using rclone lsd <name>: in terminal-1. Authenticating to Cloud Remotes \u00b6 Authenticating to UAB Box \u00b6 Click \"Use Single Sign On (SSO)\". Type in your UAB email address (not your @uabmc.edu email!). Click \"Authorize\". You will be redirected to the UAB SSO page. Authenticate with your blazerid credentials. You will be asked to grant permission to the RClone software. Click \"Grant access to Box\" if you want the software to work with Box. If you do not grant permission, you will not be able to use RClone with Box. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Return to Setting up UAB Cloud Remotes . Warning Tokens are set to expire after some time of disuse to decrease risk of a data breach. If your token expires, you can Reconnect to an Existing Remote rather than recreate the remote configuration completely from scratch. Authenticating to Microsoft OneDrive \u00b6 Type in your UAB email address (not your @uabmc.edu email!). Click \"Next\". If prompted, click \"Work or school account\". You will be asked to grant permission to the RClone software. Click \"Accept\" if you want the software to work with OneDrive. If you do not grant permission, you will not be able to use RClone with OneDrive. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Next you will return to the general instructions. Before you do, note that you'll be asked to choose which type of OneDrive service to access. The prompt will look like the image below. For UAB, the two relevant selections will be (1) to access your personal OneDrive space and (3) for a SharePoint Site, e.g. for a lab or department. With your selection in mind, return to Setting up UAB Cloud Remotes . Setting Up an S3 LTS Remote \u00b6 The full S3 configuration process can be done from a single command line terminal. Open a terminal and enter rclone config to begin the configuration process. Note The locations where you will need to input either a command or select an option are preceded with a $ for easier navigation. $ rclone config 2022 /02/22 13 :02:15 NOTICE: Config file \"/home/mdefende/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config # select 'n' to create a new remote $ n/s/q> n # name the new remote $ name> uablts At this point, you've created a new remote configuration called uablts. This will be the remote name used in further commands. You can name the remote whatever you would like, but will need to replace uablts in the instructions with whichever name you chose, if you chose a different name. ... 4 / Amazon Drive \\ ( amazon cloud drive ) 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS \\ ( s3 ) 6 / Backblaze B2 \\ ( b2 ) ... $ Storage> 5 ... 2 / Alibaba Cloud Object Storage System ( OSS ) formerly Aliyun \\ ( Alibaba ) 3 / Ceph Object Storage \\ ( Ceph ) 4 / Digital Ocean Spaces \\ ( DigitalOcean ) ... $ provider> 3 Option env_auth. Get AWS credentials from runtime ( environment variables or EC2/ECS meta data if no env vars ) . Only applies if access_key_id and secret_access_key is blank. Choose a number from below, or type in your own boolean value ( true or false ) . Press Enter for the default ( false ) . 1 / Enter AWS credentials in the next step. \\ ( false ) 2 / Get AWS credentials from the environment ( env vars or IAM ) . \\ ( true ) $ env_auth> 1 ( or leave blank ) Option access_key_id. AWS Access Key ID. Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ access_key_id> ( Enter your access key given to you by research computing ) Option secret_access_key. AWS Secret Access Key ( password ) . Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ secret_access_key> ( Enter your secret access key given to you by research computing here ) Option region. Region to connect to. Leave blank if you are using an S3 clone and you don 't have a region. Choose a number from below, or type in your own value. Press Enter to leave empty. / Use this if unsure. 1 | Will use v4 signatures and an empty region. \\ () / Use this only if v4 signatures don' t work. 2 | E.g. pre Jewel/v10 CEPH. \\ ( other-v2-signature ) $ region> ( Leave empty ) Option endpoint. Endpoint for S3 API. Required when using an S3 clone. Enter a value. Press Enter to leave empty. $ endpoint> s3.lts.rc.uab.edu From here, press Enter to accept default options until it gives you a summary of your connection [ uablts ] type = s3 provider = Ceph access_key_id = ****************** # these will be filled in on your screen secret_access_key = ******************************** endpoint = s3.lts.rc.uab.edu -------------------- y ) Yes this is OK ( default ) e ) Edit this remote d ) Delete this remote y/e/d> Make sure everything looks correct here, then press Enter. At this point, it will bring you back to the main configuration menu. You can choose the Quit Config option and exit back to a basic terminal. Reconnecting to an Existing Remote \u00b6 When your tokens expire, rather than recreate the remote from scratch, simply use the following command with your existing remote <name> . rclone config reconnect <name>: If you already have a token, you will be asked if you want to refresh it. Choose yes if so, then continue. You will be prompted with Use auto config? . If you are on a machine with no access to a browser, respond n , as in the original setup. Follow the steps in the appropriate section under Authenticating to Cloud Remotes , as in the original setup. Usage \u00b6 RClone is a powerful tool with many commands available. We will only cover a small subset of the available commands, as most are beyond typical usage, so please see the RClone documentation for more information. All commands have access to the global flags . An important global flag is --dry-run to show what will happen without actually executing the command, which can be helpful to prevent costly mistakes. Other Helpful Global Flags are also available. The various remotes each have their own individual page with their own specific flags, and are linked in the relevant Setting up Remotes section above. Important Remote paths are always prefixed by the name of the remote like cheaha:/path/to/files . The colon character : is required for all remote paths. Local paths have no prefix like /path/to/local/files . RClone can thus be used between any two machines that are configured where rclone is being used, including from the local machine to itself. In the following instructions, replace <remote:> by the appropriate remote name from configuration. To access local files, leave <remote:> off entirely. Important Remember to use quotes \" around paths with spaces like \"path\\to\\a folder with spaces\" Usage Concept \u00b6 All rclone commands follow the same general patterns outlined below. source is the name of the source remote and destination is the name of the destination remote. Remotes must be set up before using commands. To work with local files use the format :path/to/data instead of remote:path/to/data . The colon is necessary to access local files. Single-source commands like ls : rclone ls <flags...> source:path/to/data Transfer commands like cp : rclone cp <flags...> source:path/to/data destination:path/to/data Source always comes before destination. To change the direction of file transfer, swap the order of source and destination . Creating a Directory \u00b6 To create a directory use rclone mkdir <remote:><path> . Example: rclone mkdir box:manuscript . Listing Files and Directories \u00b6 To list files on a machine use rclone ls <remote:><path> . Example rclone ls box: . To list directories on a machine use rclone lsd <remote:><path> . Example: rclone lsd box: should show manuscript . Copying Files \u00b6 To copy files without changing their name, or to recursively copy directory content, use rclone copy <source:><path> <destination:><path> . Note that the directory contents are copied, so when copying a directory, be sure that directory exists on the remote and that you are copying into it. Example: rclone copy \"C:\\users\\Name\\My Documents\" box:manuscript To copy a single file and change its name, use rclone copyto <source:><path/oldname> <destination:><path/newname> . Example rclone copyto \"C:\\users\\Name\\My Documents\\manuscript.docx\" box:manuscript\\newest.docx Syncing Between Two Devices \u00b6 To make a destination directory's contents identical to a source directory, use rclone sync <source:><path> <destination:><path> Example: rclone sync cheaha:\"C:\\users\\Name\\My Documents\" box:manuscript . Danger rclone sync is a destructive operation and cannot be undone! If files exist on the destination that do not exist on the source, then they will be deleted permanently from the destination. To avoid accidental destruction of files use the --immutable flag . Other Helpful Global Flags \u00b6 Flag used with any RClone command are called global flags. Below are some useful global flags. -C or --checksum : Skip syncing files based on checksum instead of last modified time. --dry-run : Show what will happen if the command were executed. No changes are made. --immutable : Do not allow any files to be modified. Helpful to avoid unintended deletions and overwrites. --max-depth <integer> : Only recurse to <integer> depth within directory tree. Using rclone ls --max-depth 1 means only show top-level files in the current directory. -P or --progress : Show progress of command as it runs. --quiet : Print as little as possible. Useful in scripts. -u or --update : Skips files that are newer on the remote.","title":"RClone"},{"location":"data_management/transfer/rclone/#rclone","text":"RClone is a powerful command line tool for transferring and synchronizing files over the internet between various machines, servers and cloud storage services. It is highly recommended for small to moderate amounts of data. For very large amounts of data consider using Globus for increased robustness against failure. Where Globus is not available, rclone is still suitable. RClone requires a modest amount of setup time on local machines, but once setup can be used fairly easily. RClone uses the concepts of \"remotes\", which is an abstract term for any storage service or device that is not physically part of the local machine. Many remotes are offered, including SFTP and various UAB Cloud Storage Solutions . SFTP may be used to access Cheaha, cloud.rc and other laptop and desktop computers. To use RClone effectively, you'll need to setup remotes before using the various commands. Most file manipulation commands on Linux can be found in the RClone commands, but may have slightly different names, e.g. cp is rclone copy . RClone is very powerful and, as such, has a wide variety of configuration options and flags to fine tune behavior. We will only cover the basics needed to install the software, setup remotes relevant to work at UAB, and some basic usage commands.","title":"RClone"},{"location":"data_management/transfer/rclone/#quick-tutorial-for-cheaha","text":"To use RClone for simple data transfer to and from Cheaha, follow these steps: Load the module using module load rclone . Set up a remote storage provider Copy files using the rclone cp command . To transfer from Cheaha, use rclone cp :path/on/cheaha remote:path/on/dest To transfer to Cheaha, use rclone cp remote:/path/on/remote :path/on/cheaha . Be sure to replace remote with the name of the remote you set up in step 2.","title":"Quick Tutorial for Cheaha"},{"location":"data_management/transfer/rclone/#installing","text":"","title":"Installing"},{"location":"data_management/transfer/rclone/#installing-on-cheaha","text":"On Cheaha, RClone is already installed as a Module . Use module load rclone to load it.","title":"Installing on Cheaha"},{"location":"data_management/transfer/rclone/#installing-on-linux-and-cloudrc","text":"See Installing Software for general good practice on installing software, then use the following command. curl https://rclone.org/install.sh | sudo bash Open a new terminal and enter rclone to verify installation.","title":"Installing on Linux and cloud.rc"},{"location":"data_management/transfer/rclone/#installing-on-windows","text":"It is highly recommended to install rclone in Windows Subsystem for Linux (WSL). To instead install natively on Windows, you will need to use the following instructions. Download the appropriate version from the downloads page. Extract rclone.exe into a memorable folder on your system. Do not put it into Program Files . In the Start Menu type env and look for the application \"Edit the system Environment Variables\" to open the System Properties dialog. Click the \"Environment Variables...\" button. Under \"User variables for $USER\" find the variable \"Path\". Click \"Path\" to select it. Click the \"Edit...\" button to open a new dialog. Click the \"New\" button. Type in the folder path to rclone.exe as C:/path/to/rclone_folder , modified appropriately. Click the \"OK\" button to confirm and close the dialog box. Click the \"OK\" button to confirm and close the Environment Variables dialog box. Click the \"OK\" button to confirm and close the System Properties dialog box. Verify the installation by typing \"cmd\" in the Start Menu and opening the Command Prompt application. Type rclone and you should see the rclone help text.","title":"Installing on Windows"},{"location":"data_management/transfer/rclone/#macos","text":"Follow the online instructions for installing with brew .","title":"MacOS"},{"location":"data_management/transfer/rclone/#setting-up-remotes","text":"RClone is capable of interfacing with many remote cloud services, as well as using sftp for connecting two personal computers or servers. We will only cover those cloud services relevant to UAB use. We will not cover how to connect to any other cloud services using RClone. More detailed information is available at the RClone documentation Important Cloud access tokens are always supplied with an expiration date for security reasons. You will need to repeat the setup process periodically to regain access via RClone. Note RClone has an unusual user interface, using alternating red and green blocks to differentiate list items. The colors do not convey any particular meaning beyond differentiation.","title":"Setting up Remotes"},{"location":"data_management/transfer/rclone/#setting-up-an-sftp-remote","text":"RClone connects two personal computers or servers using SFTP which is built on SSH, so a lot of these instructions mirror what would be done with an SSH configuration. Generate a Key Pair for use with the remote machine. At the terminal enter rclone config . Follow the prompts to choose sftp . Enter the following values as they come up, using defaults for other values. name> Name of the remote for future reference and configuration host> Remote IP address or cheaha.rc.uab.edu for Cheaha user> The user you will log into on the remote machine key_file> The absolute path to the private key file on the local machine, something like ~/.ssh/private_key_ed25519 key_file_pass> The passphrase used to secure the private key file (optional, but highly recommended) Verify by using rclone lsd <name> . The official docuemntation for rclone sftp is here .","title":"Setting up an SFTP Remote"},{"location":"data_management/transfer/rclone/#setting-up-uab-cloud-remotes","text":"The setup process for UAB cloud remotes is generally the same, except for the specifics of authentication. The instruction template is outlined below and will point you to the authentication section specific to each remote when it becomes relevant. As you step through the process, you will ultimately open two terminal windows and a browser window, and will need to copy text between the terminal windows. The first terminal window will be used to setup the RClone cloud remote. The second terminal will be used to authenticate to that cloud service and gain a token that will be passed back to the first terminal. Authentication will happen by logging into the service in a browser window. This setup method is necessary for any machine where a browser is not readily available, such as a cloud.rc virtual machine. To facilitate setup on these machines, the second terminal will be opened on a machine with RClone and a browser. An example of what this setup might look like is given below. Important If you are using RClone in Windows Subsystem for Linux (WSL), you won't be able to open a browser using WSL. Instead, you will need to Install RClone on Windows and use the Windows Command Prompt terminal to use rclone authorize . Open a terminal on the device you wish to authorize to access the chosen cloud service provider using RClone. This terminal will be referred to as terminal-1. At terminal-1 enter rclone config . Follow the prompts to choose one of the following. The selection here will be used later and will be referred to as <remote> . UAB Box: select Box . <remote> will be replaced by box . UAB SharePoint Site: select Microsoft OneDrive . <remote> will be replaced by onedrive . UAB OneDrive: select Microsoft OneDrive . <remote> will be replaced by onedrive . Enter a short, memorable name for future reference when prompted with name> . Keep this <name> in mind as it will be how you access the remote when Using Commands . Press enter to leave all additional prompts blank until \"Use auto config?\". Type \"n\", for no, and press enter. The prompt should now read config_token> . On a machine with a browser, such as your personal computer, open a new terminal and enter rclone authorize \"<remote>\" . Replacing <remote> with the value from step (3). This terminal will be referred to as terminal-2. When the browser window opens, use it to authenticate to your selected service. Authenticate to UAB Box . Authenticate to Microsoft OneDrive . Other services not officially supported by UAB IT are possible, but are not documented here. You will need to provide your own credentials for these services. Terminal-2 will print a secret token, which will appear like in the following image. You will need to copy the portion highlighted in the image, between the lines with ---> and <--- . Copy and paste the token from the terminal-2 to terminal-1. Follow the remaining prompts. Verify success by using rclone lsd <name>: in terminal-1.","title":"Setting up UAB Cloud Remotes"},{"location":"data_management/transfer/rclone/#authenticating-to-cloud-remotes","text":"","title":"Authenticating to Cloud Remotes"},{"location":"data_management/transfer/rclone/#authenticating-to-uab-box","text":"Click \"Use Single Sign On (SSO)\". Type in your UAB email address (not your @uabmc.edu email!). Click \"Authorize\". You will be redirected to the UAB SSO page. Authenticate with your blazerid credentials. You will be asked to grant permission to the RClone software. Click \"Grant access to Box\" if you want the software to work with Box. If you do not grant permission, you will not be able to use RClone with Box. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Return to Setting up UAB Cloud Remotes . Warning Tokens are set to expire after some time of disuse to decrease risk of a data breach. If your token expires, you can Reconnect to an Existing Remote rather than recreate the remote configuration completely from scratch.","title":"Authenticating to UAB Box"},{"location":"data_management/transfer/rclone/#authenticating-to-microsoft-onedrive","text":"Type in your UAB email address (not your @uabmc.edu email!). Click \"Next\". If prompted, click \"Work or school account\". You will be asked to grant permission to the RClone software. Click \"Accept\" if you want the software to work with OneDrive. If you do not grant permission, you will not be able to use RClone with OneDrive. You will be redirected to a \"Success!\" page. Return to Terminal (5) to find the authentication token. Next you will return to the general instructions. Before you do, note that you'll be asked to choose which type of OneDrive service to access. The prompt will look like the image below. For UAB, the two relevant selections will be (1) to access your personal OneDrive space and (3) for a SharePoint Site, e.g. for a lab or department. With your selection in mind, return to Setting up UAB Cloud Remotes .","title":"Authenticating to Microsoft OneDrive"},{"location":"data_management/transfer/rclone/#setting-up-an-s3-lts-remote","text":"The full S3 configuration process can be done from a single command line terminal. Open a terminal and enter rclone config to begin the configuration process. Note The locations where you will need to input either a command or select an option are preceded with a $ for easier navigation. $ rclone config 2022 /02/22 13 :02:15 NOTICE: Config file \"/home/mdefende/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config # select 'n' to create a new remote $ n/s/q> n # name the new remote $ name> uablts At this point, you've created a new remote configuration called uablts. This will be the remote name used in further commands. You can name the remote whatever you would like, but will need to replace uablts in the instructions with whichever name you chose, if you chose a different name. ... 4 / Amazon Drive \\ ( amazon cloud drive ) 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS \\ ( s3 ) 6 / Backblaze B2 \\ ( b2 ) ... $ Storage> 5 ... 2 / Alibaba Cloud Object Storage System ( OSS ) formerly Aliyun \\ ( Alibaba ) 3 / Ceph Object Storage \\ ( Ceph ) 4 / Digital Ocean Spaces \\ ( DigitalOcean ) ... $ provider> 3 Option env_auth. Get AWS credentials from runtime ( environment variables or EC2/ECS meta data if no env vars ) . Only applies if access_key_id and secret_access_key is blank. Choose a number from below, or type in your own boolean value ( true or false ) . Press Enter for the default ( false ) . 1 / Enter AWS credentials in the next step. \\ ( false ) 2 / Get AWS credentials from the environment ( env vars or IAM ) . \\ ( true ) $ env_auth> 1 ( or leave blank ) Option access_key_id. AWS Access Key ID. Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ access_key_id> ( Enter your access key given to you by research computing ) Option secret_access_key. AWS Secret Access Key ( password ) . Leave blank for anonymous access or runtime credentials. Enter a value. Press Enter to leave empty. $ secret_access_key> ( Enter your secret access key given to you by research computing here ) Option region. Region to connect to. Leave blank if you are using an S3 clone and you don 't have a region. Choose a number from below, or type in your own value. Press Enter to leave empty. / Use this if unsure. 1 | Will use v4 signatures and an empty region. \\ () / Use this only if v4 signatures don' t work. 2 | E.g. pre Jewel/v10 CEPH. \\ ( other-v2-signature ) $ region> ( Leave empty ) Option endpoint. Endpoint for S3 API. Required when using an S3 clone. Enter a value. Press Enter to leave empty. $ endpoint> s3.lts.rc.uab.edu From here, press Enter to accept default options until it gives you a summary of your connection [ uablts ] type = s3 provider = Ceph access_key_id = ****************** # these will be filled in on your screen secret_access_key = ******************************** endpoint = s3.lts.rc.uab.edu -------------------- y ) Yes this is OK ( default ) e ) Edit this remote d ) Delete this remote y/e/d> Make sure everything looks correct here, then press Enter. At this point, it will bring you back to the main configuration menu. You can choose the Quit Config option and exit back to a basic terminal.","title":"Setting Up an S3 LTS Remote"},{"location":"data_management/transfer/rclone/#reconnecting-to-an-existing-remote","text":"When your tokens expire, rather than recreate the remote from scratch, simply use the following command with your existing remote <name> . rclone config reconnect <name>: If you already have a token, you will be asked if you want to refresh it. Choose yes if so, then continue. You will be prompted with Use auto config? . If you are on a machine with no access to a browser, respond n , as in the original setup. Follow the steps in the appropriate section under Authenticating to Cloud Remotes , as in the original setup.","title":"Reconnecting to an Existing Remote"},{"location":"data_management/transfer/rclone/#usage","text":"RClone is a powerful tool with many commands available. We will only cover a small subset of the available commands, as most are beyond typical usage, so please see the RClone documentation for more information. All commands have access to the global flags . An important global flag is --dry-run to show what will happen without actually executing the command, which can be helpful to prevent costly mistakes. Other Helpful Global Flags are also available. The various remotes each have their own individual page with their own specific flags, and are linked in the relevant Setting up Remotes section above. Important Remote paths are always prefixed by the name of the remote like cheaha:/path/to/files . The colon character : is required for all remote paths. Local paths have no prefix like /path/to/local/files . RClone can thus be used between any two machines that are configured where rclone is being used, including from the local machine to itself. In the following instructions, replace <remote:> by the appropriate remote name from configuration. To access local files, leave <remote:> off entirely. Important Remember to use quotes \" around paths with spaces like \"path\\to\\a folder with spaces\"","title":"Usage"},{"location":"data_management/transfer/rclone/#usage-concept","text":"All rclone commands follow the same general patterns outlined below. source is the name of the source remote and destination is the name of the destination remote. Remotes must be set up before using commands. To work with local files use the format :path/to/data instead of remote:path/to/data . The colon is necessary to access local files. Single-source commands like ls : rclone ls <flags...> source:path/to/data Transfer commands like cp : rclone cp <flags...> source:path/to/data destination:path/to/data Source always comes before destination. To change the direction of file transfer, swap the order of source and destination .","title":"Usage Concept"},{"location":"data_management/transfer/rclone/#creating-a-directory","text":"To create a directory use rclone mkdir <remote:><path> . Example: rclone mkdir box:manuscript .","title":"Creating a Directory"},{"location":"data_management/transfer/rclone/#listing-files-and-directories","text":"To list files on a machine use rclone ls <remote:><path> . Example rclone ls box: . To list directories on a machine use rclone lsd <remote:><path> . Example: rclone lsd box: should show manuscript .","title":"Listing Files and Directories"},{"location":"data_management/transfer/rclone/#copying-files","text":"To copy files without changing their name, or to recursively copy directory content, use rclone copy <source:><path> <destination:><path> . Note that the directory contents are copied, so when copying a directory, be sure that directory exists on the remote and that you are copying into it. Example: rclone copy \"C:\\users\\Name\\My Documents\" box:manuscript To copy a single file and change its name, use rclone copyto <source:><path/oldname> <destination:><path/newname> . Example rclone copyto \"C:\\users\\Name\\My Documents\\manuscript.docx\" box:manuscript\\newest.docx","title":"Copying Files"},{"location":"data_management/transfer/rclone/#syncing-between-two-devices","text":"To make a destination directory's contents identical to a source directory, use rclone sync <source:><path> <destination:><path> Example: rclone sync cheaha:\"C:\\users\\Name\\My Documents\" box:manuscript . Danger rclone sync is a destructive operation and cannot be undone! If files exist on the destination that do not exist on the source, then they will be deleted permanently from the destination. To avoid accidental destruction of files use the --immutable flag .","title":"Syncing Between Two Devices"},{"location":"data_management/transfer/rclone/#other-helpful-global-flags","text":"Flag used with any RClone command are called global flags. Below are some useful global flags. -C or --checksum : Skip syncing files based on checksum instead of last modified time. --dry-run : Show what will happen if the command were executed. No changes are made. --immutable : Do not allow any files to be modified. Helpful to avoid unintended deletions and overwrites. --max-depth <integer> : Only recurse to <integer> depth within directory tree. Using rclone ls --max-depth 1 means only show top-level files in the current directory. -P or --progress : Show progress of command as it runs. --quiet : Print as little as possible. Useful in scripts. -u or --update : Skips files that are newer on the remote.","title":"Other Helpful Global Flags"},{"location":"education/case_studies/","text":"Case Studies \u00b6 Parabricks for Performing GPU-accelerated Genome Sequencing Analysis \u00b6 A GPU-accelerated genome sequencing analysis with high speedup and more accurate results can be achieved with NVIDIA Clara Parabricks. Pararbricks is a software suite for genomic analysis. Parabricks delivers accelerated analysis of next generation sequencing (NGS) data for researchers, RNA-seq, population studies, and many more usecases. More insights on its performance can be found here . For more information on Cheaha GPUs, please see our GPU Page Licensing Policy \u00b6 A license is no longer required to use Clara Parabricks 4.x and later versions, and is free for the following groups, Academic Research. Research by non-profit institutions. For development, test and evaluation purposes without use in production. Minimum Hardware requirements to run Parabricks on Cheaha GPUs \u00b6 Access to the internet. Any GPU that supports CUDA architecture/compute capability 7.0, 7.5, 8.0, 8.6, 8.9 or 9.0. The GPU has 16 GB of GPU RAM or more. It has been tested on NVIDIA V100, NVIDIA A100, and NVIDIA T4 GPUs. For more information on Cheaha GPUs, please see our GPU Page . An NVIDIA driver with version 525.60.13 or greater. Note The recent versions of Parabricks requires 16GB of GPU RAM or more. If this requirement is not satisfied, it will lead to out of memory error. Therefore, Pascalnodes partition are not recommended to run Parabricks pipeline as it does not meet the hardware requirement. System Requirements \u00b6 2 GPU server should have 100GB CPU RAM, at least 24 CPU threads. 4 GPU server should have 196GB CPU RAM, at least 32 CPU threads. 8 GPU server should have 392GB CPU RAM, at least 48 CPU threads. Parabricks Testing on Cheaha \u00b6 Parabricks software can be installed and used in the Cheaha platform on amperenodes partition. Parabricks 4.x Installation on Cheaha \u00b6 Parbaricks 4.x are available as containers in the NGC Catalog . It has generic container that comprises all the analyses pipeline that are referred in the Nvidia Documentation . It also has containers for specific tool category. The recent Parabricks 4.x documentation is available here . Parabricks 4.x container image can be installed on Cheaha using a Singularity container. More details on usage of Singularity container on Cheaha can be found in the Containers Page . To install Parabricks using Singulairty, load the Singularity 3.x module from Cheaha as, module load Singularity/3.5.2-GCC-5.4.0-2.26 Go to the NGC catalog page and copy the image path to pull the desired containers of Parabricks using Singularity. Here, the generic container is pulled using Singularity. The image path is in \u201cnvcr.io/nvidia/clara/clara-parabricks\" and the tag is 4.2.0-1. The container image name parabricks-4.2.0-1.sif is an user-derived name. singularity pull parabricks-4.2.0-1.sif docker://nvcr.io/nvidia/clara/clara-parabricks:4.2.0-1 After the image parabricks-4.2.0-1.sif is successfully created, you can run singularity image parabricks-4.2.0-1.sif with all input and output parameters. Various ways of running singularity image can be found in the Containers Page . Running singularity shell helps to navigate through the containers directory to verify the path of the software executable and later use the path outside the container to run the software. Following are the commands to run the container using singularity shell and traverse through the directories inside the contianer. singularity shell parabricks-4.2.0-1.sif Singularity> ls /bin/pbrun /bin/pbrun Singularity> /bin/pbrun version Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentation pbrun: 4 .2.0-1 If the above commands are successfully executed, then the Parabricks software is installed correctly. Downloading Parabricks Sample Use Case \u00b6 Sample test case for Parabricks can be found here . Download the sample data using wget , wget -O parabricks_sample.tar.gz https://s3.amazonaws.com/parabricks.sample/parabricks_sample.tar.gz Untar the parabricks_sample.tar.gz file, tar -xzvf parabricks_sample.tar.gz Parabricks Testing on amperenodes on Cheaha \u00b6 Once the sample data is downloaded, you can execute the pipeline using the executable pbrun which is located in /bin/pbrun within the container. You will have to load the compatible CUDA module to access GPUs as below. module load CUDA/11.6.0 In the below script, the --nv option enables the use of NVIDIA GPUs within the container. The singualrity container parabricks-4.2.0-1.sif is executed using the command singualrity run over the executable /bin/pbrun . singularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\ --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\ --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\ --out-bam output.bam You can execute Parabricks on Cheaha using amperenodes partition. Maximum number of GPUs you can request in amperenodes partition to run Parabricks is 2. Below is a sample job script to run Parabricks on amperenodes partition on 2 GPUs. #!/bin/bash #SBATCH --ntasks=24 #SBATCH --mem=100G #SBATCH --time=2:00:00 #SBATCH --partition=amperenodes #SBATCH --job-name=parabricks-ampere #SBATCH --gres=gpu:2 #SBATCH --error=%x-%j_gpu2.ampere.err #SBATCH --output=%x-%j_gpu2.ampere.out #SBATCH --mail-user=$USER@uab.edu #Load the Singularity and CUDA Toolkit modules module load Singularity/3.5.2-GCC-5.4.0-2.26 module load CUDA/11.6.0 #Run the \"pbrun\" executable from the singularity image \"parabricks-4.2.0-1.sif\", and pass the CUDA lib path to make it accessible within the container singularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\ --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\ --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\ --out-bam output.bam You can also request the required resources using srun using the below command, and execute the commands required to run Parabricks. srun --ntasks = 24 --mem = 100G --time = 1 :00:00 --partition = amperenodes --job-name = parabricks-ampere --gres = gpu:2 --pty /bin/bash Illustration on fq2bam tool analyses \u00b6 The above execution script performs fq2bam pipeline analyses. The fq2bam tool aligns, sorts (by coordinate), and marks duplicates in pair-ended FASTQ file data. The data files used in this example are taken from the sample data downloaded in the previous section. If you execute the above batch script using Parabricks sample data on amperenodes with 2 GPUs, the results will be reported as below. [ PB Info 2023 -Nov-03 11 :54:50 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :54:50 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :54:50 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :54:50 ] || GPU-BWA mem, Sorting Phase-I || [ PB Info 2023 -Nov-03 11 :54:50 ] ------------------------------------------------------------------------------ [ M::bwa_idx_load_from_disk ] read 0 ALT contigs [ PB Info 2023 -Nov-03 11 :54:55 ] GPU-BWA mem [ PB Info 2023 -Nov-03 11 :54:55 ] ProgressMeter Reads Base Pairs Aligned [ PB Info 2023 -Nov-03 11 :55:09 ] 5043564 600000000 [ PB Info 2023 -Nov-03 11 :55:13 ] 10087128 1180000000 [ PB Info 2023 -Nov-03 11 :55:18 ] 15130692 1740000000 [ PB Info 2023 -Nov-03 11 :55:22 ] 20174256 2320000000 [ PB Info 2023 -Nov-03 11 :55:26 ] 25217820 2900000000 [ PB Info 2023 -Nov-03 11 :55:30 ] 30261384 3490000000 [ PB Info 2023 -Nov-03 11 :55:33 ] 35304948 4050000000 [ PB Info 2023 -Nov-03 11 :55:37 ] 40348512 4640000000 [ PB Info 2023 -Nov-03 11 :55:41 ] 45392076 5230000000 [ PB Info 2023 -Nov-03 11 :55:45 ] 50435640 5790000000 [ PB Info 2023 -Nov-03 11 :57:59 ] GPU-BWA Mem time: 184 .615934 seconds [ PB Info 2023 -Nov-03 11 :57:59 ] GPU-BWA Mem is finished. [ main ] CMD: /usr/local/parabricks/binaries//bin/bwa mem -Z ./pbOpts.txt -F 0 /home/prema/projects/parabricks_testing/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_1.fq.gz /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_2.fq.gz @RG \\t ID:HK3TJBCX2.1 \\t LB:lib1 \\t PL:bar \\t SM:sample \\t PU:HK3TJBCX2.1 [ main ] Real time: 188 .762 sec ; CPU: 2037 .057 sec [ PB Info 2023 -Nov-03 11 :57:59 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :57:59 ] || Program: GPU-BWA mem, Sorting Phase-I || [ PB Info 2023 -Nov-03 11 :57:59 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :57:59 ] || Start Time: Fri Nov 3 11 :54:50 2023 || [ PB Info 2023 -Nov-03 11 :57:59 ] || End Time: Fri Nov 3 11 :57:59 2023 || [ PB Info 2023 -Nov-03 11 :57:59 ] || Total Time: 3 minutes 9 seconds || [ PB Info 2023 -Nov-03 11 :57:59 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :58:00 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:00 ] || Sorting Phase-II || [ PB Info 2023 -Nov-03 11 :58:00 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] progressMeter - Percentage [ PB Info 2023 -Nov-03 11 :58:00 ] 0 .0 0 .00 GB [ PB Info 2023 -Nov-03 11 :58:05 ] 52 .8 0 .00 GB [ PB Info 2023 -Nov-03 11 :58:10 ] Sorting and Marking: 10 .001 seconds [ PB Info 2023 -Nov-03 11 :58:10 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:10 ] || Program: Sorting Phase-II || [ PB Info 2023 -Nov-03 11 :58:10 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:10 ] || Start Time: Fri Nov 3 11 :58:00 2023 || [ PB Info 2023 -Nov-03 11 :58:10 ] || End Time: Fri Nov 3 11 :58:10 2023 || [ PB Info 2023 -Nov-03 11 :58:10 ] || Total Time: 10 seconds || [ PB Info 2023 -Nov-03 11 :58:10 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :58:11 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:11 ] || Marking Duplicates, BQSR || [ PB Info 2023 -Nov-03 11 :58:11 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] Using PBBinBamFile for BAM writing [ PB Info 2023 -Nov-03 11 :58:11 ] progressMeter - Percentage [ PB Info 2023 -Nov-03 11 :58:21 ] 24 .9 0 .08 GB [ PB Info 2023 -Nov-03 11 :58:31 ] 48 .1 0 .07 GB [ PB Info 2023 -Nov-03 11 :58:41 ] 69 .8 0 .09 GB [ PB Info 2023 -Nov-03 11 :58:51 ] 85 .9 0 .11 GB [ PB Info 2023 -Nov-03 11 :59:01 ] 100 .0 0 .00 GB [ PB Info 2023 -Nov-03 11 :59:01 ] BQSR and writing final BAM: 50 .044 seconds [ PB Info 2023 -Nov-03 11 :59:01 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :59:01 ] || Program: Marking Duplicates, BQSR || [ PB Info 2023 -Nov-03 11 :59:01 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :59:01 ] || Start Time: Fri Nov 3 11 :58:11 2023 || [ PB Info 2023 -Nov-03 11 :59:01 ] || End Time: Fri Nov 3 11 :59:01 2023 || [ PB Info 2023 -Nov-03 11 :59:01 ] || Total Time: 50 seconds || [ PB Info 2023 -Nov-03 11 :59:01 ] ------------------------------------------------------------------------------ Monitoring GPU Usage During Runtime on Cheaha \u00b6 The nvidia-smi command helps to montior the GPU usage during runtime. You need to ssh to the assigned GPU node, and type in the following command. ssh GPU_node module load CUDA/11.6.0 nvidia-smi The nvidia-smi reports the GPU memory usage and the 2 GPU process running details as shown below. $ module load CUDA/11.6.0 $ nvidia-smi Fri Nov 3 12 :38:24 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535 .86.10 Driver Version: 535 .86.10 CUDA Version: 12 .2 | | -----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | ========================================= + ====================== + ====================== | | 0 NVIDIA A100 80GB PCIe Off | 00000000 :25:00.0 Off | 0 | | N/A 35C P0 165W / 300W | 16631MiB / 81920MiB | 97 % Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA A100 80GB PCIe Off | 00000000 :81:00.0 Off | 0 | | N/A 32C P0 112W / 300W | 16631MiB / 81920MiB | 96 % Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | | ======================================================================================= | | 0 N/A N/A 65905 C .../local/parabricks/binaries//bin/bwa 16594MiB | | 1 N/A N/A 65905 C .../local/parabricks/binaries//bin/bwa 16594MiB | +---------------------------------------------------------------------------------------+ Runtime Evaluation of Parabricks Sample Test Case on amperenodes Partition \u00b6 Parabricks is tested and works with CUDA version >= 11.6.0 on Cheaha. Empirical results on running Parabricks sample test case on amperendoes partitions are illustrated in the below table. For this test case, runtime of 1 GPU is better than 2 GPU. Generally, large-scale data scales well with increase in number of GPUs, and the real-world science simulations may vary in their speedup. Partitions No. of GPUs Execution Time (s) amperenodes 1 239 amperenodes 2 270 Applications show 2x performance with Parabricks > 4.0 version. You can refer here to performance tuning ideas to achieve best performance with Parabricks.","title":"Case Studies"},{"location":"education/case_studies/#case-studies","text":"","title":"Case Studies"},{"location":"education/case_studies/#parabricks-for-performing-gpu-accelerated-genome-sequencing-analysis","text":"A GPU-accelerated genome sequencing analysis with high speedup and more accurate results can be achieved with NVIDIA Clara Parabricks. Pararbricks is a software suite for genomic analysis. Parabricks delivers accelerated analysis of next generation sequencing (NGS) data for researchers, RNA-seq, population studies, and many more usecases. More insights on its performance can be found here . For more information on Cheaha GPUs, please see our GPU Page","title":"Parabricks for Performing GPU-accelerated Genome Sequencing Analysis"},{"location":"education/case_studies/#licensing-policy","text":"A license is no longer required to use Clara Parabricks 4.x and later versions, and is free for the following groups, Academic Research. Research by non-profit institutions. For development, test and evaluation purposes without use in production.","title":"Licensing Policy"},{"location":"education/case_studies/#minimum-hardware-requirements-to-run-parabricks-on-cheaha-gpus","text":"Access to the internet. Any GPU that supports CUDA architecture/compute capability 7.0, 7.5, 8.0, 8.6, 8.9 or 9.0. The GPU has 16 GB of GPU RAM or more. It has been tested on NVIDIA V100, NVIDIA A100, and NVIDIA T4 GPUs. For more information on Cheaha GPUs, please see our GPU Page . An NVIDIA driver with version 525.60.13 or greater. Note The recent versions of Parabricks requires 16GB of GPU RAM or more. If this requirement is not satisfied, it will lead to out of memory error. Therefore, Pascalnodes partition are not recommended to run Parabricks pipeline as it does not meet the hardware requirement.","title":"Minimum Hardware requirements to run Parabricks on Cheaha GPUs"},{"location":"education/case_studies/#system-requirements","text":"2 GPU server should have 100GB CPU RAM, at least 24 CPU threads. 4 GPU server should have 196GB CPU RAM, at least 32 CPU threads. 8 GPU server should have 392GB CPU RAM, at least 48 CPU threads.","title":"System Requirements"},{"location":"education/case_studies/#parabricks-testing-on-cheaha","text":"Parabricks software can be installed and used in the Cheaha platform on amperenodes partition.","title":"Parabricks Testing on Cheaha"},{"location":"education/case_studies/#parabricks-4x-installation-on-cheaha","text":"Parbaricks 4.x are available as containers in the NGC Catalog . It has generic container that comprises all the analyses pipeline that are referred in the Nvidia Documentation . It also has containers for specific tool category. The recent Parabricks 4.x documentation is available here . Parabricks 4.x container image can be installed on Cheaha using a Singularity container. More details on usage of Singularity container on Cheaha can be found in the Containers Page . To install Parabricks using Singulairty, load the Singularity 3.x module from Cheaha as, module load Singularity/3.5.2-GCC-5.4.0-2.26 Go to the NGC catalog page and copy the image path to pull the desired containers of Parabricks using Singularity. Here, the generic container is pulled using Singularity. The image path is in \u201cnvcr.io/nvidia/clara/clara-parabricks\" and the tag is 4.2.0-1. The container image name parabricks-4.2.0-1.sif is an user-derived name. singularity pull parabricks-4.2.0-1.sif docker://nvcr.io/nvidia/clara/clara-parabricks:4.2.0-1 After the image parabricks-4.2.0-1.sif is successfully created, you can run singularity image parabricks-4.2.0-1.sif with all input and output parameters. Various ways of running singularity image can be found in the Containers Page . Running singularity shell helps to navigate through the containers directory to verify the path of the software executable and later use the path outside the container to run the software. Following are the commands to run the container using singularity shell and traverse through the directories inside the contianer. singularity shell parabricks-4.2.0-1.sif Singularity> ls /bin/pbrun /bin/pbrun Singularity> /bin/pbrun version Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentation pbrun: 4 .2.0-1 If the above commands are successfully executed, then the Parabricks software is installed correctly.","title":"Parabricks 4.x Installation on Cheaha"},{"location":"education/case_studies/#downloading-parabricks-sample-use-case","text":"Sample test case for Parabricks can be found here . Download the sample data using wget , wget -O parabricks_sample.tar.gz https://s3.amazonaws.com/parabricks.sample/parabricks_sample.tar.gz Untar the parabricks_sample.tar.gz file, tar -xzvf parabricks_sample.tar.gz","title":"Downloading Parabricks Sample Use Case"},{"location":"education/case_studies/#parabricks-testing-on-amperenodes-on-cheaha","text":"Once the sample data is downloaded, you can execute the pipeline using the executable pbrun which is located in /bin/pbrun within the container. You will have to load the compatible CUDA module to access GPUs as below. module load CUDA/11.6.0 In the below script, the --nv option enables the use of NVIDIA GPUs within the container. The singualrity container parabricks-4.2.0-1.sif is executed using the command singualrity run over the executable /bin/pbrun . singularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\ --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\ --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\ --out-bam output.bam You can execute Parabricks on Cheaha using amperenodes partition. Maximum number of GPUs you can request in amperenodes partition to run Parabricks is 2. Below is a sample job script to run Parabricks on amperenodes partition on 2 GPUs. #!/bin/bash #SBATCH --ntasks=24 #SBATCH --mem=100G #SBATCH --time=2:00:00 #SBATCH --partition=amperenodes #SBATCH --job-name=parabricks-ampere #SBATCH --gres=gpu:2 #SBATCH --error=%x-%j_gpu2.ampere.err #SBATCH --output=%x-%j_gpu2.ampere.out #SBATCH --mail-user=$USER@uab.edu #Load the Singularity and CUDA Toolkit modules module load Singularity/3.5.2-GCC-5.4.0-2.26 module load CUDA/11.6.0 #Run the \"pbrun\" executable from the singularity image \"parabricks-4.2.0-1.sif\", and pass the CUDA lib path to make it accessible within the container singularity run --nv parabricks-4.2.0-1.sif /bin/pbrun fq2bam \\ --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta \\ --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz \\ --out-bam output.bam You can also request the required resources using srun using the below command, and execute the commands required to run Parabricks. srun --ntasks = 24 --mem = 100G --time = 1 :00:00 --partition = amperenodes --job-name = parabricks-ampere --gres = gpu:2 --pty /bin/bash","title":"Parabricks Testing on amperenodes on Cheaha"},{"location":"education/case_studies/#illustration-on-fq2bam-tool-analyses","text":"The above execution script performs fq2bam pipeline analyses. The fq2bam tool aligns, sorts (by coordinate), and marks duplicates in pair-ended FASTQ file data. The data files used in this example are taken from the sample data downloaded in the previous section. If you execute the above batch script using Parabricks sample data on amperenodes with 2 GPUs, the results will be reported as below. [ PB Info 2023 -Nov-03 11 :54:50 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :54:50 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :54:50 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :54:50 ] || GPU-BWA mem, Sorting Phase-I || [ PB Info 2023 -Nov-03 11 :54:50 ] ------------------------------------------------------------------------------ [ M::bwa_idx_load_from_disk ] read 0 ALT contigs [ PB Info 2023 -Nov-03 11 :54:55 ] GPU-BWA mem [ PB Info 2023 -Nov-03 11 :54:55 ] ProgressMeter Reads Base Pairs Aligned [ PB Info 2023 -Nov-03 11 :55:09 ] 5043564 600000000 [ PB Info 2023 -Nov-03 11 :55:13 ] 10087128 1180000000 [ PB Info 2023 -Nov-03 11 :55:18 ] 15130692 1740000000 [ PB Info 2023 -Nov-03 11 :55:22 ] 20174256 2320000000 [ PB Info 2023 -Nov-03 11 :55:26 ] 25217820 2900000000 [ PB Info 2023 -Nov-03 11 :55:30 ] 30261384 3490000000 [ PB Info 2023 -Nov-03 11 :55:33 ] 35304948 4050000000 [ PB Info 2023 -Nov-03 11 :55:37 ] 40348512 4640000000 [ PB Info 2023 -Nov-03 11 :55:41 ] 45392076 5230000000 [ PB Info 2023 -Nov-03 11 :55:45 ] 50435640 5790000000 [ PB Info 2023 -Nov-03 11 :57:59 ] GPU-BWA Mem time: 184 .615934 seconds [ PB Info 2023 -Nov-03 11 :57:59 ] GPU-BWA Mem is finished. [ main ] CMD: /usr/local/parabricks/binaries//bin/bwa mem -Z ./pbOpts.txt -F 0 /home/prema/projects/parabricks_testing/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_1.fq.gz /home/prema/projects/parabricks_testing/parabricks_sample/Data/sample_2.fq.gz @RG \\t ID:HK3TJBCX2.1 \\t LB:lib1 \\t PL:bar \\t SM:sample \\t PU:HK3TJBCX2.1 [ main ] Real time: 188 .762 sec ; CPU: 2037 .057 sec [ PB Info 2023 -Nov-03 11 :57:59 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :57:59 ] || Program: GPU-BWA mem, Sorting Phase-I || [ PB Info 2023 -Nov-03 11 :57:59 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :57:59 ] || Start Time: Fri Nov 3 11 :54:50 2023 || [ PB Info 2023 -Nov-03 11 :57:59 ] || End Time: Fri Nov 3 11 :57:59 2023 || [ PB Info 2023 -Nov-03 11 :57:59 ] || Total Time: 3 minutes 9 seconds || [ PB Info 2023 -Nov-03 11 :57:59 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :58:00 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:00 ] || Sorting Phase-II || [ PB Info 2023 -Nov-03 11 :58:00 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:00 ] progressMeter - Percentage [ PB Info 2023 -Nov-03 11 :58:00 ] 0 .0 0 .00 GB [ PB Info 2023 -Nov-03 11 :58:05 ] 52 .8 0 .00 GB [ PB Info 2023 -Nov-03 11 :58:10 ] Sorting and Marking: 10 .001 seconds [ PB Info 2023 -Nov-03 11 :58:10 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:10 ] || Program: Sorting Phase-II || [ PB Info 2023 -Nov-03 11 :58:10 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:10 ] || Start Time: Fri Nov 3 11 :58:00 2023 || [ PB Info 2023 -Nov-03 11 :58:10 ] || End Time: Fri Nov 3 11 :58:10 2023 || [ PB Info 2023 -Nov-03 11 :58:10 ] || Total Time: 10 seconds || [ PB Info 2023 -Nov-03 11 :58:10 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] || Parabricks accelerated Genomics Pipeline || [ PB Info 2023 -Nov-03 11 :58:11 ] || Version 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :58:11 ] || Marking Duplicates, BQSR || [ PB Info 2023 -Nov-03 11 :58:11 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :58:11 ] Using PBBinBamFile for BAM writing [ PB Info 2023 -Nov-03 11 :58:11 ] progressMeter - Percentage [ PB Info 2023 -Nov-03 11 :58:21 ] 24 .9 0 .08 GB [ PB Info 2023 -Nov-03 11 :58:31 ] 48 .1 0 .07 GB [ PB Info 2023 -Nov-03 11 :58:41 ] 69 .8 0 .09 GB [ PB Info 2023 -Nov-03 11 :58:51 ] 85 .9 0 .11 GB [ PB Info 2023 -Nov-03 11 :59:01 ] 100 .0 0 .00 GB [ PB Info 2023 -Nov-03 11 :59:01 ] BQSR and writing final BAM: 50 .044 seconds [ PB Info 2023 -Nov-03 11 :59:01 ] ------------------------------------------------------------------------------ [ PB Info 2023 -Nov-03 11 :59:01 ] || Program: Marking Duplicates, BQSR || [ PB Info 2023 -Nov-03 11 :59:01 ] || Version: 4 .2.0-1 || [ PB Info 2023 -Nov-03 11 :59:01 ] || Start Time: Fri Nov 3 11 :58:11 2023 || [ PB Info 2023 -Nov-03 11 :59:01 ] || End Time: Fri Nov 3 11 :59:01 2023 || [ PB Info 2023 -Nov-03 11 :59:01 ] || Total Time: 50 seconds || [ PB Info 2023 -Nov-03 11 :59:01 ] ------------------------------------------------------------------------------","title":"Illustration on fq2bam tool analyses"},{"location":"education/case_studies/#monitoring-gpu-usage-during-runtime-on-cheaha","text":"The nvidia-smi command helps to montior the GPU usage during runtime. You need to ssh to the assigned GPU node, and type in the following command. ssh GPU_node module load CUDA/11.6.0 nvidia-smi The nvidia-smi reports the GPU memory usage and the 2 GPU process running details as shown below. $ module load CUDA/11.6.0 $ nvidia-smi Fri Nov 3 12 :38:24 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535 .86.10 Driver Version: 535 .86.10 CUDA Version: 12 .2 | | -----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | ========================================= + ====================== + ====================== | | 0 NVIDIA A100 80GB PCIe Off | 00000000 :25:00.0 Off | 0 | | N/A 35C P0 165W / 300W | 16631MiB / 81920MiB | 97 % Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA A100 80GB PCIe Off | 00000000 :81:00.0 Off | 0 | | N/A 32C P0 112W / 300W | 16631MiB / 81920MiB | 96 % Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | | ======================================================================================= | | 0 N/A N/A 65905 C .../local/parabricks/binaries//bin/bwa 16594MiB | | 1 N/A N/A 65905 C .../local/parabricks/binaries//bin/bwa 16594MiB | +---------------------------------------------------------------------------------------+","title":"Monitoring GPU Usage During Runtime on Cheaha"},{"location":"education/case_studies/#runtime-evaluation-of-parabricks-sample-test-case-on-amperenodes-partition","text":"Parabricks is tested and works with CUDA version >= 11.6.0 on Cheaha. Empirical results on running Parabricks sample test case on amperendoes partitions are illustrated in the below table. For this test case, runtime of 1 GPU is better than 2 GPU. Generally, large-scale data scales well with increase in number of GPUs, and the real-world science simulations may vary in their speedup. Partitions No. of GPUs Execution Time (s) amperenodes 1 239 amperenodes 2 270 Applications show 2x performance with Parabricks > 4.0 version. You can refer here to performance tuning ideas to achieve best performance with Parabricks.","title":"Runtime Evaluation of Parabricks Sample Test Case on amperenodes Partition"},{"location":"education/courses/","text":"Courses \u00b6 Data Science Journal Club Course \u00b6 Intended for students interested in learning more about data science and its applications to research, the course is 1 credit hour pass/fail and offered every session (Fall, Spring, Summer). The course is listed as GBSC 720-VTR JC- Data Science Club and sometimes also as IDNE 790-VTR JC- Data Science Club . Students are largely expected to lead their own journey, with instructors facilitating and offering advice. We expect a good-faith effort to learn and grow in the course and the course has most value when students step outside their comfort zones. There are three student-led demonstrations required to pass, where we expect students to demonstrate how they have learned and grown from their exploration of the material. Some of the topics covered by past students include: Learning to use High Performance Computing (HPC) resources Learning software development good practices Learning git and github Learning data organization good practices Jupyter notebooks R Markdown notebooks Data visualization Data analytics Data processing pipelines Machine learning Deep learning The only prerequisite for the course is feeling comfortable using a computer. We are flexible with topics and welcome students of all levels of data science experience. If you have questions about the course please feel free to reach out to Support . Syllabus \u00b6 The syllabus is available as a public webpage at https://s3.lts.rc.uab.edu/uab-rc-dsjc/syllabus.html , hosted on LTS .","title":"Offered Courses"},{"location":"education/courses/#courses","text":"","title":"Courses"},{"location":"education/courses/#data-science-journal-club-course","text":"Intended for students interested in learning more about data science and its applications to research, the course is 1 credit hour pass/fail and offered every session (Fall, Spring, Summer). The course is listed as GBSC 720-VTR JC- Data Science Club and sometimes also as IDNE 790-VTR JC- Data Science Club . Students are largely expected to lead their own journey, with instructors facilitating and offering advice. We expect a good-faith effort to learn and grow in the course and the course has most value when students step outside their comfort zones. There are three student-led demonstrations required to pass, where we expect students to demonstrate how they have learned and grown from their exploration of the material. Some of the topics covered by past students include: Learning to use High Performance Computing (HPC) resources Learning software development good practices Learning git and github Learning data organization good practices Jupyter notebooks R Markdown notebooks Data visualization Data analytics Data processing pipelines Machine learning Deep learning The only prerequisite for the course is feeling comfortable using a computer. We are flexible with topics and welcome students of all levels of data science experience. If you have questions about the course please feel free to reach out to Support .","title":"Data Science Journal Club Course"},{"location":"education/courses/#syllabus","text":"The syllabus is available as a public webpage at https://s3.lts.rc.uab.edu/uab-rc-dsjc/syllabus.html , hosted on LTS .","title":"Syllabus"},{"location":"education/research_computing_days/","text":"UAB Research Computing Day \u00b6 Research Computing Day is a dialog within the UAB research communityabout leveraging the power of computers to grow the depth of our investigation into the nature of the world that surrounds us. The annual event welcomes discussions on science, engineering, the arts and humanities focused on the drive to open new research frontiers with advances in technology. Whether computers are used to increase the accuracy of a model, interpret the ever-growing stream of data from new image collections and instruments, or engage with peers around the globe, UAB's status as a leading research community depends on the ability to incorporate these capabilities into the research process. By participating in the dialog of Research Computing Day at UAB, researchers can share how they are using these methods to enhance their research, gain new insights from peers, and contribute their voices to the growth of research at UAB. Background \u00b6 Since 2007, The Office of the Vice President for Information Technology has sponsored an annual dialog on the role of technology in research. These events joined UAB with national dialogs on the role of Cyberinfrastructure in research held at campuses across the country. Previous UAB Research Computing Days \u00b6 2007 -- Co-hosted along with the ASA site visit, providing an overview of new services and upcoming launch of the UABgrid pilot. (No web record) 2008 -- Focus on grid computing and collaboration technologies, in particular the caBIG program with guest speakers from Booz Allen Hamilton who managed the NCI caBIG program and SURA (agenda currently offline) 2010 -- Featured introduction to Galaxy platform for genetic sequencing by Dell staff scientist (agenda currently offline) 2011 -- Understanding growth of research computing support at peer institutions UNC and Emory 2012 -- Growing data sciences at UAB 2013 -- OpenStack at UAB 2016 -- HPC Expansion 2017 -- GPU expansion Research Computing Day 2018","title":"Research Computing Days"},{"location":"education/research_computing_days/#uab-research-computing-day","text":"Research Computing Day is a dialog within the UAB research communityabout leveraging the power of computers to grow the depth of our investigation into the nature of the world that surrounds us. The annual event welcomes discussions on science, engineering, the arts and humanities focused on the drive to open new research frontiers with advances in technology. Whether computers are used to increase the accuracy of a model, interpret the ever-growing stream of data from new image collections and instruments, or engage with peers around the globe, UAB's status as a leading research community depends on the ability to incorporate these capabilities into the research process. By participating in the dialog of Research Computing Day at UAB, researchers can share how they are using these methods to enhance their research, gain new insights from peers, and contribute their voices to the growth of research at UAB.","title":"UAB Research Computing Day"},{"location":"education/research_computing_days/#background","text":"Since 2007, The Office of the Vice President for Information Technology has sponsored an annual dialog on the role of technology in research. These events joined UAB with national dialogs on the role of Cyberinfrastructure in research held at campuses across the country.","title":"Background"},{"location":"education/research_computing_days/#previous-uab-research-computing-days","text":"2007 -- Co-hosted along with the ASA site visit, providing an overview of new services and upcoming launch of the UABgrid pilot. (No web record) 2008 -- Focus on grid computing and collaboration technologies, in particular the caBIG program with guest speakers from Booz Allen Hamilton who managed the NCI caBIG program and SURA (agenda currently offline) 2010 -- Featured introduction to Galaxy platform for genetic sequencing by Dell staff scientist (agenda currently offline) 2011 -- Understanding growth of research computing support at peer institutions UNC and Emory 2012 -- Growing data sciences at UAB 2013 -- OpenStack at UAB 2016 -- HPC Expansion 2017 -- GPU expansion Research Computing Day 2018","title":"Previous UAB Research Computing Days"},{"location":"education/training_resources/","text":"Training Resources \u00b6 Internal Resources \u00b6 Informatics Club : A UAB group for students and trainees to learn more about informatics. Code, Chat & Collab : A recurring meet-up hosted by the Informatics Club where students, staff and faculty can meet to discuss informatics, data analysis, software development, and Cheaha use. Events and Resources : A collection of useful resources provided by the Informatics club. LinkedIn Learning : Online courses available for free to UAB employees. Topics include programming, data science, and more. External Resources \u00b6 The Carpentries \u00b6 The Carpentries is a 501(c)3 non-profit organization dedicated to educating researchers on software and data skills. Using inclusive and accessibly best practices, they build, maintaint and promote high-quality instructional material for a range of software and data-oriented concepts. Software Carpentries Lessons : Primary page for the Software Carpentry lessons. The following lessons, and more, are available. The Unix Shell Version Control with Git Programming with Python Programming with R Data Carpentries Lessons : Primary page for the Data Carpentry lessons. Instructional material is available for a variety of fields. Genomics Curriculum Image Processing Curriculum Other External Resources \u00b6 Rosalind.info : A platform for learning bioinformatics and programming. Our Social Media Presence \u00b6 YouTube GitHub : Contains several repos used on the YouTube channel.","title":"Training Resources"},{"location":"education/training_resources/#training-resources","text":"","title":"Training Resources"},{"location":"education/training_resources/#internal-resources","text":"Informatics Club : A UAB group for students and trainees to learn more about informatics. Code, Chat & Collab : A recurring meet-up hosted by the Informatics Club where students, staff and faculty can meet to discuss informatics, data analysis, software development, and Cheaha use. Events and Resources : A collection of useful resources provided by the Informatics club. LinkedIn Learning : Online courses available for free to UAB employees. Topics include programming, data science, and more.","title":"Internal Resources"},{"location":"education/training_resources/#external-resources","text":"","title":"External Resources"},{"location":"education/training_resources/#the-carpentries","text":"The Carpentries is a 501(c)3 non-profit organization dedicated to educating researchers on software and data skills. Using inclusive and accessibly best practices, they build, maintaint and promote high-quality instructional material for a range of software and data-oriented concepts. Software Carpentries Lessons : Primary page for the Software Carpentry lessons. The following lessons, and more, are available. The Unix Shell Version Control with Git Programming with Python Programming with R Data Carpentries Lessons : Primary page for the Data Carpentry lessons. Instructional material is available for a variety of fields. Genomics Curriculum Image Processing Curriculum","title":"The Carpentries"},{"location":"education/training_resources/#other-external-resources","text":"Rosalind.info : A platform for learning bioinformatics and programming.","title":"Other External Resources"},{"location":"education/training_resources/#our-social-media-presence","text":"YouTube GitHub : Contains several repos used on the YouTube channel.","title":"Our Social Media Presence"},{"location":"grants/budgets/","text":"Grant Budgets \u00b6 Construction This page is a stub and is under construction. Research Computing Buy-in Program \u00b6 Computation \u00b6 Additional Storage \u00b6","title":"Budgeting"},{"location":"grants/budgets/#grant-budgets","text":"Construction This page is a stub and is under construction.","title":"Grant Budgets"},{"location":"grants/budgets/#research-computing-buy-in-program","text":"","title":"Research Computing Buy-in Program"},{"location":"grants/budgets/#computation","text":"","title":"Computation"},{"location":"grants/budgets/#additional-storage","text":"","title":"Additional Storage"},{"location":"grants/facilities/","text":"UAB IT Research Computing (UABRC) Resources and Cybersecurity Facilities Document \u00b6 The link below contains a plain-text facilities document designed to support research grant applications. Click Here To Download The Research Computing Facilities Grant Document. If you believe that information within is inaccurate or there are missing details, please feel free to contact Support","title":"Facilities and Plan"},{"location":"grants/facilities/#uab-it-research-computing-uabrc-resources-and-cybersecurity-facilities-document","text":"The link below contains a plain-text facilities document designed to support research grant applications. Click Here To Download The Research Computing Facilities Grant Document. If you believe that information within is inaccurate or there are missing details, please feel free to contact Support","title":"UAB IT Research Computing (UABRC) Resources and Cybersecurity Facilities Document"},{"location":"grants/overview/","text":"Overview of Research Computing Grant Resources \u00b6 Construction This page is a stub and is under construction.","title":"Overview"},{"location":"grants/overview/#overview-of-research-computing-grant-resources","text":"Construction This page is a stub and is under construction.","title":"Overview of Research Computing Grant Resources"},{"location":"grants/publications/","text":"Grant Descriptions \u00b6 Short \u00b6 UAB IT Research Computing maintains high performance compute (HPC) and storage resources for investigators. The Cheaha high performance compute cluster provides 8192 CPU cores and 72 GPUs interconnected via an InfiniBand network, providing over 619 TFLOP/s of aggregate theoretical peak performance. A high-performance, 12PB raw GPFS storage on DDN SFA12KX hardware is also connected to these compute nodes via the Infiniband fabric, available to all UAB investigators. Detailed \u00b6 Construction Under construction. Acknoweldgement in Publications \u00b6 To acknowledge the use of Cheaha in published work, for compute time or substantial technical assistance, please consider adding the following to the acknowledgements section of your publication: The authors gratefully acknowledge the resources provided by the University of Alabama at Birmingham IT-Research Computing group for high performance computing (HPC) support and CPU time on the Cheaha compute cluster. If Globus was used to transfer data to/from Cheaha, please consider adding the following to the acknowledgements section of your publication: This work was supported in part by the National Science Foundation under Grants Nos. OAC-1541310, the University of Alabama at Birmingham, and the Alabama Innovation Fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the University of Alabama at Birmingham. Detailed Hardware Information \u00b6 For more detailed information on compute hardware please see: Detailed Hardware Information","title":"Descriptions"},{"location":"grants/publications/#grant-descriptions","text":"","title":"Grant Descriptions"},{"location":"grants/publications/#short","text":"UAB IT Research Computing maintains high performance compute (HPC) and storage resources for investigators. The Cheaha high performance compute cluster provides 8192 CPU cores and 72 GPUs interconnected via an InfiniBand network, providing over 619 TFLOP/s of aggregate theoretical peak performance. A high-performance, 12PB raw GPFS storage on DDN SFA12KX hardware is also connected to these compute nodes via the Infiniband fabric, available to all UAB investigators.","title":"Short"},{"location":"grants/publications/#detailed","text":"Construction Under construction.","title":"Detailed"},{"location":"grants/publications/#acknoweldgement-in-publications","text":"To acknowledge the use of Cheaha in published work, for compute time or substantial technical assistance, please consider adding the following to the acknowledgements section of your publication: The authors gratefully acknowledge the resources provided by the University of Alabama at Birmingham IT-Research Computing group for high performance computing (HPC) support and CPU time on the Cheaha compute cluster. If Globus was used to transfer data to/from Cheaha, please consider adding the following to the acknowledgements section of your publication: This work was supported in part by the National Science Foundation under Grants Nos. OAC-1541310, the University of Alabama at Birmingham, and the Alabama Innovation Fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the University of Alabama at Birmingham.","title":"Acknoweldgement in Publications"},{"location":"grants/publications/#detailed-hardware-information","text":"For more detailed information on compute hardware please see: Detailed Hardware Information","title":"Detailed Hardware Information"},{"location":"help/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 We have moved our FAQ page to a new home at Ask.Cyberinfrastructure . Feel free to subscribe for updates.","title":"FAQ"},{"location":"help/faq/#frequently-asked-questions-faq","text":"We have moved our FAQ page to a new home at Ask.Cyberinfrastructure . Feel free to subscribe for updates.","title":"Frequently Asked Questions (FAQ)"},{"location":"help/support/","text":"How to Request Support \u00b6 Before reaching out to us, try searching this documentation for keywords related to your issue. If you aren't able to find anything, please try checking our FAQ located on ask.cyberinfrastructure . If you still need help, please read on for how to send in a ticket and how to work with our ticketing system. How Do I Create a Support Ticket? \u00b6 To Create a support ticket, send a descriptive email to support@listserv.uab.edu to create a ticket. Bonus points for including the following details. For general issues: What is your goal? What steps were taken? What was expected? What actually happened? How was the cluster accessed? Web Portal, SSH, VNC, etc.? What software were you using? Please be as specific as possible. The command module list can be helpful here. For outages: What part of the cluster is affected? Please list any relevant affected nodes or other hardware that is not accessible. If you are unable to access the cluster please state that instead. What were you working on when you noticed the outage? How were you accessing the cluster? Web Portal, SSH, VNC, etc.? How Do I Work With Tickets I've Created? \u00b6 UAB IT and Research Computing use ServiceNow for ticket management. When any email is sent to support@listserv.uab.edu , AskIT is notified and a ticket is created in ServiceNow and the ticket details are forwarded by email to every member of Research Computing staff. When any email is sent to support@listserv.uab.edu the following happens: An RITM ticket is created in ServiceNow and assigned to Research Computing. The email is routed to Research Computing staff and the ticket creator from the support@listserv.uab.edu email list. Please do not reply . Replies create additional tickets and cause service delays. A monitoring email is sent from support-watch@listserv.uab.edu to Research Computing staff and the ticket creator. Please do not reply to this email. Your email will not make it through, because the email address is closed to Research Computing staff. A ticket creation email is sent from askit@uab.edu to Research Computing staff and the ticket creator. Please do reply to this email. Replies to askit@uab.edu with the correct subject line format will add your reply as a comment to the ticket. Before replying, please delete all previous quoted replies to avoid the accumulation of noise in the ticket. The images below show how to, and how not to, reply to askit@uab.edu emails. Please reply like this: Not like this: If you prefer to use email to manage your ticket, please use the method of replying to emails from askit@uab.edu . Note that you will not be able to see any attachments this way. They are stripped off and added to the ticket in the ServiceNow web interface, available through the RITM0000000 link in the emails you've received. If you prefer to use the ServiceNow web interface, please click the RITM0000000 link in the email replies. Important The UAB email server strips all potentially executable files from emails. This includes attachments with .log , .sh , .py and .exe suffixes. Zip files containing those files will also be stripped. If you need to send script examples to us please rename the files to have a .txt suffix and inform us of their original nature. What Types of Files can I Attach to Tickets? \u00b6 Certain common file extensions are filtered when sent via email or attached to tickets. Generally, any file that is potentially executable is filtered. Below is a list of file extensions known to be filtered, and there may be others. Executable files: .exe , .dat , .cab Windows registry files: .reg Code and script files: .sh , .bash , .py , .m , .r Log files: .log Rich document files containing macros. Essentially, any file extension from Microsoft Office ending with m , e.g., .docm . Files with special characters in the filename. To ensure your files are attached to tickets as expected, try the following. Images, use .png or .jpg . Log files, rename output.log to output.txt Code files, rename code.sh or code.py , etc., to code.txt . Change the name to only use ASCII letters, numbers, hyphen, and underscore. Please do not try to attach executable files to tickets. If you have a legitimate need to give us executable files, mention it in the ticket and we can find a path forward on a case-by-case basis. How Do I Request Or Change Shared Storage? \u00b6 Please see our Storage page for more information. How do I request new software installed? \u00b6 Before making a request for new software on Cheaha, please try searching our modules or searching for packages on Anaconda . If you are not able to find a suitable module or package and would like software installed on Cheaha, please create a ticket with the name of the software, the version number, and a link to the installation instructions. Office Hours \u00b6 For our office hours links please see Contact Us . Status Updates \u00b6 For status updates affecting our systems or services please visit https://uabstatus.statuscast.com/#!/incidentlist?componentId=34990 . At this page you can subscribe to notifications using the bell icon next to the name \"Research Computing\" near the top-left of the page.","title":"Support"},{"location":"help/support/#how-to-request-support","text":"Before reaching out to us, try searching this documentation for keywords related to your issue. If you aren't able to find anything, please try checking our FAQ located on ask.cyberinfrastructure . If you still need help, please read on for how to send in a ticket and how to work with our ticketing system.","title":"How to Request Support"},{"location":"help/support/#how-do-i-create-a-support-ticket","text":"To Create a support ticket, send a descriptive email to support@listserv.uab.edu to create a ticket. Bonus points for including the following details. For general issues: What is your goal? What steps were taken? What was expected? What actually happened? How was the cluster accessed? Web Portal, SSH, VNC, etc.? What software were you using? Please be as specific as possible. The command module list can be helpful here. For outages: What part of the cluster is affected? Please list any relevant affected nodes or other hardware that is not accessible. If you are unable to access the cluster please state that instead. What were you working on when you noticed the outage? How were you accessing the cluster? Web Portal, SSH, VNC, etc.?","title":"How Do I Create a Support Ticket?"},{"location":"help/support/#how-do-i-work-with-tickets-ive-created","text":"UAB IT and Research Computing use ServiceNow for ticket management. When any email is sent to support@listserv.uab.edu , AskIT is notified and a ticket is created in ServiceNow and the ticket details are forwarded by email to every member of Research Computing staff. When any email is sent to support@listserv.uab.edu the following happens: An RITM ticket is created in ServiceNow and assigned to Research Computing. The email is routed to Research Computing staff and the ticket creator from the support@listserv.uab.edu email list. Please do not reply . Replies create additional tickets and cause service delays. A monitoring email is sent from support-watch@listserv.uab.edu to Research Computing staff and the ticket creator. Please do not reply to this email. Your email will not make it through, because the email address is closed to Research Computing staff. A ticket creation email is sent from askit@uab.edu to Research Computing staff and the ticket creator. Please do reply to this email. Replies to askit@uab.edu with the correct subject line format will add your reply as a comment to the ticket. Before replying, please delete all previous quoted replies to avoid the accumulation of noise in the ticket. The images below show how to, and how not to, reply to askit@uab.edu emails. Please reply like this: Not like this: If you prefer to use email to manage your ticket, please use the method of replying to emails from askit@uab.edu . Note that you will not be able to see any attachments this way. They are stripped off and added to the ticket in the ServiceNow web interface, available through the RITM0000000 link in the emails you've received. If you prefer to use the ServiceNow web interface, please click the RITM0000000 link in the email replies. Important The UAB email server strips all potentially executable files from emails. This includes attachments with .log , .sh , .py and .exe suffixes. Zip files containing those files will also be stripped. If you need to send script examples to us please rename the files to have a .txt suffix and inform us of their original nature.","title":"How Do I Work With Tickets I've Created?"},{"location":"help/support/#what-types-of-files-can-i-attach-to-tickets","text":"Certain common file extensions are filtered when sent via email or attached to tickets. Generally, any file that is potentially executable is filtered. Below is a list of file extensions known to be filtered, and there may be others. Executable files: .exe , .dat , .cab Windows registry files: .reg Code and script files: .sh , .bash , .py , .m , .r Log files: .log Rich document files containing macros. Essentially, any file extension from Microsoft Office ending with m , e.g., .docm . Files with special characters in the filename. To ensure your files are attached to tickets as expected, try the following. Images, use .png or .jpg . Log files, rename output.log to output.txt Code files, rename code.sh or code.py , etc., to code.txt . Change the name to only use ASCII letters, numbers, hyphen, and underscore. Please do not try to attach executable files to tickets. If you have a legitimate need to give us executable files, mention it in the ticket and we can find a path forward on a case-by-case basis.","title":"What Types of Files can I Attach to Tickets?"},{"location":"help/support/#how-do-i-request-or-change-shared-storage","text":"Please see our Storage page for more information.","title":"How Do I Request Or Change Shared Storage?"},{"location":"help/support/#how-do-i-request-new-software-installed","text":"Before making a request for new software on Cheaha, please try searching our modules or searching for packages on Anaconda . If you are not able to find a suitable module or package and would like software installed on Cheaha, please create a ticket with the name of the software, the version number, and a link to the installation instructions.","title":"How do I request new software installed?"},{"location":"help/support/#office-hours","text":"For our office hours links please see Contact Us .","title":"Office Hours"},{"location":"help/support/#status-updates","text":"For status updates affecting our systems or services please visit https://uabstatus.statuscast.com/#!/incidentlist?componentId=34990 . At this page you can subscribe to notifications using the bell icon next to the name \"Research Computing\" near the top-left of the page.","title":"Status Updates"},{"location":"kubernetes/startup/","text":"UAB Kubernetes \u00b6 Construction This page is a stub and is under construction. Kubernetes (K8s, named because there are 8 letters between 'K' and 's') can be used to automate workflows of all kinds. Researchers can use KSoftware engineers can use it to create GitLab runners to execute continuous integration workflows to deploy code updates seemlessly. UAB Kubernetes Computation Resources \u00b6 4 DGX A100 nodes GPUs aren\u2019t virtualized, but one DGX node can be split into 1/7th of a GPU 8 A100s per DGX node * 7/7ths = 56/7ths of a GPU possible Can be used for classes learning to use GPUs Finding containers to use K8s will search DockerHub and other container registries automagically. Can add additional registries","title":"Startup"},{"location":"kubernetes/startup/#uab-kubernetes","text":"Construction This page is a stub and is under construction. Kubernetes (K8s, named because there are 8 letters between 'K' and 's') can be used to automate workflows of all kinds. Researchers can use KSoftware engineers can use it to create GitLab runners to execute continuous integration workflows to deploy code updates seemlessly.","title":"UAB Kubernetes"},{"location":"kubernetes/startup/#uab-kubernetes-computation-resources","text":"4 DGX A100 nodes GPUs aren\u2019t virtualized, but one DGX node can be split into 1/7th of a GPU 8 A100s per DGX node * 7/7ths = 56/7ths of a GPU possible Can be used for classes learning to use GPUs Finding containers to use K8s will search DockerHub and other container registries automagically. Can add additional registries","title":"UAB Kubernetes Computation Resources"},{"location":"national_ci/","text":"National Cyberinfrastructre \u00b6 Construction This page is a stub and is under construction.","title":"National Cyberinfrastructre"},{"location":"national_ci/#national-cyberinfrastructre","text":"Construction This page is a stub and is under construction.","title":"National Cyberinfrastructre"},{"location":"national_ci/nih/","text":"National Institutes of Health (NIH) Cyberinfrastructure \u00b6 Construction This page is a stub and is under construction.","title":"NIH Cyberinfrastructure"},{"location":"national_ci/nih/#national-institutes-of-health-nih-cyberinfrastructure","text":"Construction This page is a stub and is under construction.","title":"National Institutes of Health (NIH) Cyberinfrastructure"},{"location":"national_ci/nsf_access/","text":"National Science Foundation (NSF) Cyberinfrastructure \u00b6 Construction This page is a stub and is under construction.","title":"NSF Cyberinfrastructure"},{"location":"national_ci/nsf_access/#national-science-foundation-nsf-cyberinfrastructure","text":"Construction This page is a stub and is under construction.","title":"National Science Foundation (NSF) Cyberinfrastructure"},{"location":"national_ci/osg/","text":"The Open Science Grid (OSG) \u00b6 Construction This page is a stub and is under construction.","title":"The Open Science Grid"},{"location":"national_ci/osg/#the-open-science-grid-osg","text":"Construction This page is a stub and is under construction.","title":"The Open Science Grid (OSG)"},{"location":"uab_cloud/","text":"UAB Cloud \u00b6 Our cloud.rc portal, based on OpenStack cloud software, provides a home for more permanent research applications such as web pages and database hosting, as well as a place where researchers can more fluidly develop applications for high performance compute. Resource quotas are set to ensure that every researcher has a fair share. Please be sure to free up resources when they are no longer needed by deleting instances and volumes. Currently, access to cloud.rc must be made while on the UAB Campus Network or on the UAB Campus Virtual Private Network (VPN). For more information about using the UAB Campus VPN, please visit VPN - UAB IT . The VPN requires Duo 2FA . First Steps \u00b6 To get started using cloud.rc, please navigate to https://dashboard.cloud.rc.uab.edu/ . You will be taken to a login page that looks like below. To login you will need an account, please request one by contacting Support . When requesting an account, please tell us how you intend to use the service. Some reasonable use-cases are listed in Cloud Usage Philosophy below. There are three fields that must be filled out: Domain: must always be uab , lowercase. User Name: your BlazerID or XIAS email. Password: whatever your current password is, not necessarily the same as your Single Sign-On password. If you have forgotten your password please contact Support . Once these fields are filled, click \"Sign In\" to login. Once logged in, you will see the OpenStack dashboard. An example is shown below. To get the most out of cloud.rc, you'll want to make sure you have a working familiarity with the Linux terminal . Cloud.rc runs on Openstack. If you are new to Openstack or to cloud.rc, it is highly recommended to follow our Tutorial to learn how to set up all of the necessary components of a virtual machine (VM) setup. The tutorial is intended to be followed in order. Doing it out of order may result in errors and issues. If you encounter any unexpected issues, unclear instructions or have questions or comments, please contact Support . Cloud Usage Philosophy \u00b6 An important philosophy of cloud services is that virtual machines are disposable, and cloud platforms are designed to facilitate this philosophy. If a virtual machine fails, breaks or becomes misconfigured, it is designed to be destroyed and recreated from scratch. With that in mind, there are many possible use-cases for cloud.rc. Broadly speaking, a few common ones: Experimental workflow development prior to Containerization and batch processing on Cheaha or a National Cyberinfrastructure platform . Packaging software into Containers . Temporary hosting of server software for scientific development or workflows. The downside to disposable machines is losing configuration specifics. Software exists that can assist with reproducible virtual machine configuration, including Ansible , or even just a custom shell script. We are unable to provide assistance with deployment of virtual machine internals. Naming Conventions \u00b6 Entities on cloud.rc must be named a certain way or difficult-to-diagnose errors may occur. Entities includes instances, volumes, networks, routers, and anything else that you are allowed to give a name to. Please use the following rules when naming entities: Must: use only letters, numbers, dash - and underscore _ . Must: have the first character in the name be a letter. Should: use short, descriptive, memorable names.","title":"UAB Cloud"},{"location":"uab_cloud/#uab-cloud","text":"Our cloud.rc portal, based on OpenStack cloud software, provides a home for more permanent research applications such as web pages and database hosting, as well as a place where researchers can more fluidly develop applications for high performance compute. Resource quotas are set to ensure that every researcher has a fair share. Please be sure to free up resources when they are no longer needed by deleting instances and volumes. Currently, access to cloud.rc must be made while on the UAB Campus Network or on the UAB Campus Virtual Private Network (VPN). For more information about using the UAB Campus VPN, please visit VPN - UAB IT . The VPN requires Duo 2FA .","title":"UAB Cloud"},{"location":"uab_cloud/#first-steps","text":"To get started using cloud.rc, please navigate to https://dashboard.cloud.rc.uab.edu/ . You will be taken to a login page that looks like below. To login you will need an account, please request one by contacting Support . When requesting an account, please tell us how you intend to use the service. Some reasonable use-cases are listed in Cloud Usage Philosophy below. There are three fields that must be filled out: Domain: must always be uab , lowercase. User Name: your BlazerID or XIAS email. Password: whatever your current password is, not necessarily the same as your Single Sign-On password. If you have forgotten your password please contact Support . Once these fields are filled, click \"Sign In\" to login. Once logged in, you will see the OpenStack dashboard. An example is shown below. To get the most out of cloud.rc, you'll want to make sure you have a working familiarity with the Linux terminal . Cloud.rc runs on Openstack. If you are new to Openstack or to cloud.rc, it is highly recommended to follow our Tutorial to learn how to set up all of the necessary components of a virtual machine (VM) setup. The tutorial is intended to be followed in order. Doing it out of order may result in errors and issues. If you encounter any unexpected issues, unclear instructions or have questions or comments, please contact Support .","title":"First Steps"},{"location":"uab_cloud/#cloud-usage-philosophy","text":"An important philosophy of cloud services is that virtual machines are disposable, and cloud platforms are designed to facilitate this philosophy. If a virtual machine fails, breaks or becomes misconfigured, it is designed to be destroyed and recreated from scratch. With that in mind, there are many possible use-cases for cloud.rc. Broadly speaking, a few common ones: Experimental workflow development prior to Containerization and batch processing on Cheaha or a National Cyberinfrastructure platform . Packaging software into Containers . Temporary hosting of server software for scientific development or workflows. The downside to disposable machines is losing configuration specifics. Software exists that can assist with reproducible virtual machine configuration, including Ansible , or even just a custom shell script. We are unable to provide assistance with deployment of virtual machine internals.","title":"Cloud Usage Philosophy"},{"location":"uab_cloud/#naming-conventions","text":"Entities on cloud.rc must be named a certain way or difficult-to-diagnose errors may occur. Entities includes instances, volumes, networks, routers, and anything else that you are allowed to give a name to. Please use the following rules when naming entities: Must: use only letters, numbers, dash - and underscore _ . Must: have the first character in the name be a letter. Should: use short, descriptive, memorable names.","title":"Naming Conventions"},{"location":"uab_cloud/installing_software/","text":"Installing Software on Instances \u00b6 An important part of managing instances is the installation of software. This page assumes you have a working instance and can SSH into it. This page assumes you are using an Ubuntu image. We highly recommend building your research software stack into a Container . While there is a learning curve and some setup time, containers make replicating and sharing environments simpler. Everything you develop is packaged into a self-contained unit that can be run on virtually any modern Linux system. A particular command sudo will be used extensively. Be warned that sudo grants any commands used administrator privileges. If you use sudo with untrustworthy software, you may be allowing an attacker to compromise your system. Danger The sudo command should be used carefully and judiciously, as it creates security risks. Use with caution. Important Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources. Before Installing Software \u00b6 Before installing software, good practice is updating and upgrading operating system packages. For some software this is required. These updates often include critical security and bug fixes. To update the instance operating system, enter the following at the command line. sudo apt update sudo apt upgrade Installing Software \u00b6 Most common software packages and NVIDIA drivers are available as apt packages. Some packages are available using custom installers. Be sure the website and installer author are trustworthy before installing! Finding Packages \u00b6 Try using Google to locate the name of the package with something like ubuntu apt <keywords> Try using https://packages.ubuntu.com Try apt-cache search <keyword> Ask Support for help Installing Packages \u00b6 If the software is available via apt then use sudo apt install <package> . An example would be sudo apt install git to install git software. If the software uses a custom installer, then follow the instructions provided by the software's documentation. An example would be Miniconda , where a shell script is downloaded and then executed using bash installer.sh . Installing Server Software \u00b6 If you wish to set up server software, you'll need to open ports for that software by creating Security Groups . Then you may want to test the ports are open and verify your software is listening. It is highly recommended to verify the ports are configured properly and remotely accessible before you spend time setting up and configuring the server software. Testing Server Ports \u00b6 If you intend to use your instance as a server host, you'll likely need to set up additional Security Groups for any ports the server expects to communicate on. It can be helpful to verify that those ports are open before configuring the server software. Assuming you know which ports are needed, the simplest way to do this is outlined below. Set up Security Groups for the ports your server will need to communicate on. SSH into the instance. Prepare the netcat software command nc : For Ubuntu, the command nc should already be available. For other OSes, you may need to Install nc or netcat . For one of your <port> of interest, start a TCP listener with nc -l <port> . Note nc -l <port> will only listen for a single connection attempt, and then close. To emulate indefinite listening behavior, use it within a loop like so. while true ; do nc -l <port> ; done Open a new terminal on your local machine. Probe the <port> : Using the Windows command prompt: Enter the command telnet <floating-ip> <port> . If the terminal window goes blank, then the connection was successful. Otherwise you will see a message like Could not open connection to the host . To exit telnet press Ctrl + ] , then type q , then press Enter . Using any Linux-based prompt, MacOS, or Git Bash on Windows: Ensure nc is installed locally. Enter the command nc -nvz <floating-ip> <port> . n uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster. v uses verbose output, i.e., print the output we care about. z scans for listeners on the remote. If the connection is successful you should see something close to the following, with <floating-ip> and <port> replaced by the values you supplied earlier. (UNKNOWN) [<floating-ip>] <port> (?) open If the connection is unsuccessful you will see Connection refused instead of open . Now you should have more information on whether your VM port configuration was successful. Feel free to repeat the steps above for each port, as needed. Verify Server Software is Listening \u00b6 Once you have the server set up, you can check which processes are listening on which ports using the following command. sudo ss -lnptu The program is ss for \"socket statistics\", which can display information linking ports and processes. The flags are l displays only listening sockets n uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster. p shows which process is using each socket. t displays tcp sockets. u displays udp sockets. An example of the output is shown below. The most useful columns for us are Local Address:Port , to verify they match the configured Security Group ports, and Process , to verify the server software is listening on the correct ports. Common Examples \u00b6 Below are a few examples of installing certain common softwares that may be useful to scientific applications. We are not able to provide diagnostic or troubleshooting support for installation of any software. If you believe these instructions are outdated or in error, please reach out and let us know . Installing NVidia Drivers \u00b6 Run the commands in Before Installing Software . sudo apt install ubuntu-drivers-common ubuntu-drivers devices Find the line with \"recommended\" and install the package on that line with sudo apt install nvidia-driver-### Reboot the instance Installing Miniconda \u00b6 We recommend installing Miniconda on cloud.rc instances, as opposed to Anaconda, to conserve storage space. Run the commands in Before Installing Software . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Tip Consider installing Mamba to speed up environment installation. Installing Singularity \u00b6 Follow the instructions located at https://sylabs.io/guides/3.9/user-guide/quick_start.html#install-system-dependencies under \"Debian-based systems\". Run the commands in Before Installing Software . Run the following sudo apt-get install -y \\ build-essential \\ libseccomp-dev \\ pkg-config \\ squashfs-tools \\ cryptsetup Install Go language using the following export VERSION = 1 .17.2 OS = linux ARCH = amd64 && \\ # Replace the values as needed wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && \\ # Downloads the required Go package sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && \\ # Extracts the archive rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc Download SingularityCE export VERSION = 3 .9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v ${ VERSION } /singularity-ce- ${ VERSION } .tar.gz && \\ tar -xzf singularity-ce- ${ VERSION } .tar.gz && \\ cd singularity-ce- ${ VERSION } Compile SingularityCE ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Note For other versions of the Singularity documentation, visit https://sylabs.io/docs/ . Installing Jupyter Notebook Server \u00b6 Jupyter Notebooks are a staple of modern research computing, especially when developing new workflows or evaluating the usefulness of software packages. The setup process for cloud.rc is more involved than for Cheaha . Before using cloud.rc for Jupyter Notebooks, be sure that Open OnDemand on Cheaha does not meet your needs. To install, you will need the following pre-requisites. If you are unfamiliar with the terminology or new to cloud.rc, it is highly recommended to first start with our Introduction and follow the tutorial completely. Run the commands in Before Installing Software . A Cloud Instance with attached [Floating IP]network_setup_basic.md#floating-ips). A Security Group for the intended Jupyter Server port. For the purposes of this tutorial, the port will be set to 9999 . Miniconda installed on the instance. Miniconda is a lightweight version of Anaconda. Once the prerequisites are complete, the following steps must be performed to install and setup Jupyter Notebook Server. It is highly recommended to build an Anaconda Environment using a reproducible Environment File . The steps below belong to the official Jupyter documentation available at https://jupyter-notebook.readthedocs.io/en/stable/public_server.html# . Warning Leaving your Jupyter Notebook Server unsecured may mean that other people on the UAB Campus Network are able to access your notebooks and other files stored on that cloud instance. Install Jupyter Notebook Server using Miniconda . You will need the following packages. conda-forge channel notebook nb_conda_kernels [Optional] jupyter_contrib_nbextensions anaconda channel ipykernel for python users r-irkernel for R users [Optional] pip Because floating IPs are, by default, reachable by anyone on the campus network, you'll need to secure the server using the steps below. Generate a notebook config file using jupyter notebook --generate-config . [ official docs ] Prepare a password using jupyter notebook password . [ official docs ] Set up SSL for an encrypted connection. For now create a self-signed certificate using the following command. [ official docs ] openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem Warning When you connect to your Jupyter Server, your browser will warn you that the connection may be insecure. This is because self-signed certificates are not trusted by your operating system's root certificates. It is possible to fix this with some additional work using notes at the official docs . Generally the security warning can be bypassed without issue in this case . Configure the notebook server by locating lines like the following in ~/.jupyter/jupyter_notebook_config.py and updating them with the right-hand side of each variable assignment (equals sign = ). This file was created as part of the first step of these instructions. [ official docs ] Note If you used jupyter notebook password the hashed password will be located in jupyter_notebook_config.json instead of .py . Note The lines below may not appear together depending on the version of Jupyter installed. The file jupyter_notebook_config.py contains over a thousand lines. You may need to search using a text editor search or find feature. If you are using nano please visit our nano page to learn how to search. c . NotebookApp . certfile = u '/absolute/path/to/your/certificate/mycert.pem' c . NotebookApp . keyfile = u '/absolute/path/to/your/certificate/mykey.key' c . NotebookApp . ip = '*' c . NotebookApp . password = u 'sha1:bcd259ccf...<your hashed password here>' c . NotebookApp . open_browser = False # It is a good idea to set a known, fixed port for server access c . NotebookApp . port = 9999 Start the server with jupyter notebook . Access the server with the browser on your local machine by navigating to https://<floating-ip>:<port> . In this case the port was set to be 9999 , and <floating-ip> comes from the prerequisites for this section. The port must match that used for the security group to allow traffic between your local machine and the cloud instance. You must also be on the UAB Campus VPN. Important Some browsers may default to using http instead of https when given a raw IP address. Make sure to fully type out https://<floating-ip>:<port> .","title":"Installing Software"},{"location":"uab_cloud/installing_software/#installing-software-on-instances","text":"An important part of managing instances is the installation of software. This page assumes you have a working instance and can SSH into it. This page assumes you are using an Ubuntu image. We highly recommend building your research software stack into a Container . While there is a learning curve and some setup time, containers make replicating and sharing environments simpler. Everything you develop is packaged into a self-contained unit that can be run on virtually any modern Linux system. A particular command sudo will be used extensively. Be warned that sudo grants any commands used administrator privileges. If you use sudo with untrustworthy software, you may be allowing an attacker to compromise your system. Danger The sudo command should be used carefully and judiciously, as it creates security risks. Use with caution. Important Much of the information and examples on this page require a working knowledge of terminal commands and the shell. If you are unfamiliar with the terminal then please see our Shell page for more information and educational resources.","title":"Installing Software on Instances"},{"location":"uab_cloud/installing_software/#before-installing-software","text":"Before installing software, good practice is updating and upgrading operating system packages. For some software this is required. These updates often include critical security and bug fixes. To update the instance operating system, enter the following at the command line. sudo apt update sudo apt upgrade","title":"Before Installing Software"},{"location":"uab_cloud/installing_software/#installing-software","text":"Most common software packages and NVIDIA drivers are available as apt packages. Some packages are available using custom installers. Be sure the website and installer author are trustworthy before installing!","title":"Installing Software"},{"location":"uab_cloud/installing_software/#finding-packages","text":"Try using Google to locate the name of the package with something like ubuntu apt <keywords> Try using https://packages.ubuntu.com Try apt-cache search <keyword> Ask Support for help","title":"Finding Packages"},{"location":"uab_cloud/installing_software/#installing-packages","text":"If the software is available via apt then use sudo apt install <package> . An example would be sudo apt install git to install git software. If the software uses a custom installer, then follow the instructions provided by the software's documentation. An example would be Miniconda , where a shell script is downloaded and then executed using bash installer.sh .","title":"Installing Packages"},{"location":"uab_cloud/installing_software/#installing-server-software","text":"If you wish to set up server software, you'll need to open ports for that software by creating Security Groups . Then you may want to test the ports are open and verify your software is listening. It is highly recommended to verify the ports are configured properly and remotely accessible before you spend time setting up and configuring the server software.","title":"Installing Server Software"},{"location":"uab_cloud/installing_software/#testing-server-ports","text":"If you intend to use your instance as a server host, you'll likely need to set up additional Security Groups for any ports the server expects to communicate on. It can be helpful to verify that those ports are open before configuring the server software. Assuming you know which ports are needed, the simplest way to do this is outlined below. Set up Security Groups for the ports your server will need to communicate on. SSH into the instance. Prepare the netcat software command nc : For Ubuntu, the command nc should already be available. For other OSes, you may need to Install nc or netcat . For one of your <port> of interest, start a TCP listener with nc -l <port> . Note nc -l <port> will only listen for a single connection attempt, and then close. To emulate indefinite listening behavior, use it within a loop like so. while true ; do nc -l <port> ; done Open a new terminal on your local machine. Probe the <port> : Using the Windows command prompt: Enter the command telnet <floating-ip> <port> . If the terminal window goes blank, then the connection was successful. Otherwise you will see a message like Could not open connection to the host . To exit telnet press Ctrl + ] , then type q , then press Enter . Using any Linux-based prompt, MacOS, or Git Bash on Windows: Ensure nc is installed locally. Enter the command nc -nvz <floating-ip> <port> . n uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster. v uses verbose output, i.e., print the output we care about. z scans for listeners on the remote. If the connection is successful you should see something close to the following, with <floating-ip> and <port> replaced by the values you supplied earlier. (UNKNOWN) [<floating-ip>] <port> (?) open If the connection is unsuccessful you will see Connection refused instead of open . Now you should have more information on whether your VM port configuration was successful. Feel free to repeat the steps above for each port, as needed.","title":"Testing Server Ports"},{"location":"uab_cloud/installing_software/#verify-server-software-is-listening","text":"Once you have the server set up, you can check which processes are listening on which ports using the following command. sudo ss -lnptu The program is ss for \"socket statistics\", which can display information linking ports and processes. The flags are l displays only listening sockets n uses numeric output, which minimizes unhelpful warnings about hostname lookups. It is also faster. p shows which process is using each socket. t displays tcp sockets. u displays udp sockets. An example of the output is shown below. The most useful columns for us are Local Address:Port , to verify they match the configured Security Group ports, and Process , to verify the server software is listening on the correct ports.","title":"Verify Server Software is Listening"},{"location":"uab_cloud/installing_software/#common-examples","text":"Below are a few examples of installing certain common softwares that may be useful to scientific applications. We are not able to provide diagnostic or troubleshooting support for installation of any software. If you believe these instructions are outdated or in error, please reach out and let us know .","title":"Common Examples"},{"location":"uab_cloud/installing_software/#installing-nvidia-drivers","text":"Run the commands in Before Installing Software . sudo apt install ubuntu-drivers-common ubuntu-drivers devices Find the line with \"recommended\" and install the package on that line with sudo apt install nvidia-driver-### Reboot the instance","title":"Installing NVidia Drivers"},{"location":"uab_cloud/installing_software/#installing-miniconda","text":"We recommend installing Miniconda on cloud.rc instances, as opposed to Anaconda, to conserve storage space. Run the commands in Before Installing Software . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Tip Consider installing Mamba to speed up environment installation.","title":"Installing Miniconda"},{"location":"uab_cloud/installing_software/#installing-singularity","text":"Follow the instructions located at https://sylabs.io/guides/3.9/user-guide/quick_start.html#install-system-dependencies under \"Debian-based systems\". Run the commands in Before Installing Software . Run the following sudo apt-get install -y \\ build-essential \\ libseccomp-dev \\ pkg-config \\ squashfs-tools \\ cryptsetup Install Go language using the following export VERSION = 1 .17.2 OS = linux ARCH = amd64 && \\ # Replace the values as needed wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && \\ # Downloads the required Go package sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && \\ # Extracts the archive rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc Download SingularityCE export VERSION = 3 .9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v ${ VERSION } /singularity-ce- ${ VERSION } .tar.gz && \\ tar -xzf singularity-ce- ${ VERSION } .tar.gz && \\ cd singularity-ce- ${ VERSION } Compile SingularityCE ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Note For other versions of the Singularity documentation, visit https://sylabs.io/docs/ .","title":"Installing Singularity"},{"location":"uab_cloud/installing_software/#installing-jupyter-notebook-server","text":"Jupyter Notebooks are a staple of modern research computing, especially when developing new workflows or evaluating the usefulness of software packages. The setup process for cloud.rc is more involved than for Cheaha . Before using cloud.rc for Jupyter Notebooks, be sure that Open OnDemand on Cheaha does not meet your needs. To install, you will need the following pre-requisites. If you are unfamiliar with the terminology or new to cloud.rc, it is highly recommended to first start with our Introduction and follow the tutorial completely. Run the commands in Before Installing Software . A Cloud Instance with attached [Floating IP]network_setup_basic.md#floating-ips). A Security Group for the intended Jupyter Server port. For the purposes of this tutorial, the port will be set to 9999 . Miniconda installed on the instance. Miniconda is a lightweight version of Anaconda. Once the prerequisites are complete, the following steps must be performed to install and setup Jupyter Notebook Server. It is highly recommended to build an Anaconda Environment using a reproducible Environment File . The steps below belong to the official Jupyter documentation available at https://jupyter-notebook.readthedocs.io/en/stable/public_server.html# . Warning Leaving your Jupyter Notebook Server unsecured may mean that other people on the UAB Campus Network are able to access your notebooks and other files stored on that cloud instance. Install Jupyter Notebook Server using Miniconda . You will need the following packages. conda-forge channel notebook nb_conda_kernels [Optional] jupyter_contrib_nbextensions anaconda channel ipykernel for python users r-irkernel for R users [Optional] pip Because floating IPs are, by default, reachable by anyone on the campus network, you'll need to secure the server using the steps below. Generate a notebook config file using jupyter notebook --generate-config . [ official docs ] Prepare a password using jupyter notebook password . [ official docs ] Set up SSL for an encrypted connection. For now create a self-signed certificate using the following command. [ official docs ] openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem Warning When you connect to your Jupyter Server, your browser will warn you that the connection may be insecure. This is because self-signed certificates are not trusted by your operating system's root certificates. It is possible to fix this with some additional work using notes at the official docs . Generally the security warning can be bypassed without issue in this case . Configure the notebook server by locating lines like the following in ~/.jupyter/jupyter_notebook_config.py and updating them with the right-hand side of each variable assignment (equals sign = ). This file was created as part of the first step of these instructions. [ official docs ] Note If you used jupyter notebook password the hashed password will be located in jupyter_notebook_config.json instead of .py . Note The lines below may not appear together depending on the version of Jupyter installed. The file jupyter_notebook_config.py contains over a thousand lines. You may need to search using a text editor search or find feature. If you are using nano please visit our nano page to learn how to search. c . NotebookApp . certfile = u '/absolute/path/to/your/certificate/mycert.pem' c . NotebookApp . keyfile = u '/absolute/path/to/your/certificate/mykey.key' c . NotebookApp . ip = '*' c . NotebookApp . password = u 'sha1:bcd259ccf...<your hashed password here>' c . NotebookApp . open_browser = False # It is a good idea to set a known, fixed port for server access c . NotebookApp . port = 9999 Start the server with jupyter notebook . Access the server with the browser on your local machine by navigating to https://<floating-ip>:<port> . In this case the port was set to be 9999 , and <floating-ip> comes from the prerequisites for this section. The port must match that used for the security group to allow traffic between your local machine and the cloud instance. You must also be on the UAB Campus VPN. Important Some browsers may default to using http instead of https when given a raw IP address. Make sure to fully type out https://<floating-ip>:<port> .","title":"Installing Jupyter Notebook Server"},{"location":"uab_cloud/remote_access/","text":"Remote Access to Instances \u00b6 All of the access methods described below are built on top of ssh and require completion of the steps in Basic Security Setup to use with cloud.rc . Some of these steps are referenced in that document. Command Line via SSH \u00b6 SSH stands for S ecure SH ell and is a powerful tool for executing terminal commands on remote machines. It is widely used and ubiquitous, and a number of other technologies are built on top of SSH, like sftp and scp for transferring files. It is also the primary mode of command line communication with Research Computing technologies like Cheaha and cloud.rc. Install an SSH Client \u00b6 There are two main steps to working with SSH efficiently. The first is to ensure you have an SSH client installed, which will let your local machine communicate with remote machines. The second is to ensure you have ssh-agent running in each terminal window to automate management of key files. The ssh-agent software comes with most SSH clients, but does not always run automatically. How to start the ssh-agent software automatically varies depending on operating system and shell flavor, which we will describe below. Terminal Multiplexers \u00b6 Terminal multiplexers are software that aggregate multiple SSH client sessions into one location. They often have the added benefit of keeping sessions alive on the remote machine, so short internet outages won't require you to log in again. For Linux, try using tmux : https://github.com/tmux/tmux/wiki/Installing For Windows, try using Windows Terminal: https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701 Install an SSH Client (Linux) \u00b6 Virtually all Linux distributions come with SSH preinstalled and configured appropriately for ease of use, including automatically starting the ssh-agent . Install an SSH Client (MacOS) \u00b6 MacOS comes with an SSH client installed. If you are on version Leopard 10.5.1 or lower, you may want to have the ssh-agent start automatically using the command sudo touch /var/db/useLS at a terminal window. Versions newer than Leopard 10.5.1 start the ssh-agent automatically. Install an SSH Client (Windows) \u00b6 There are several options for installing an SSH client on Windows, described below. It is highly recommended to install Windows Subsystem for Linux (WSL) as it provides a complete Linux environment within Windows. Windows Subsystem For Linux (WSL) \u00b6 Follow the instructions starting here to install Windows Subsystem for Linux. WSL shells do not automatically start or share the ssh-agent . To fix this we recommend installing keychain to automatically manage the ssh-agent . Run the following command depending on your Linux distribution. DEB-based (Debian, Ubuntu): sudo apt install keychain RPM-based (CentOS, Fedora, openSUSE): sudo yum install keychain Then modify the .*rc file for your shell, generally .bashrc or .zshrc , to automate the ssh-agent by adding the following line. - eval `keychain -q --eval --agents ssh` Tip You can access WSL files from within Windows in two ways. In the WSL terminal, enter explorer.exe . to open a File Explorer window in the current directory. In Windows, open a File Explorer window, click in the top navigation bar and enter \\\\wsl$ . Then select your distribution from the file window to access the filesystem of that WSL operating system. OpenSSH for Windows \u00b6 Follow the instructions here to install the OpenSSH client. Only install the OpenSSH server if you need it, otherwise skip that part. Once the OpenSSH client is installed, you'll want to enable the OpenSSH Agent service on your local machine to streamline adding and using keys. Open the Start Menu and search for \"Services\", and open the result shown in the image. Find the \"OpenSSH Authentication Agent\" service in the list. Double click it, or right-click it and select \"Properties\". In the dialog box, under the \"General\" tab, look for \"Startup Type\". Click the drop-down menu and select \"Automatic (Delayed Start)\". Click \"Apply\" at the bottom-right corner. This will cause the ssh-agent service to start when Windows starts. The \"Start\" button under the horizontal line should become enabled. Click it to start the ssh-agent service now. Git Bash terminal (Git for Windows) \u00b6 The fine folks at Git have worked very hard to package everything needed to use Git on Windows into one installer. This includes a Linux command line interface emulator, Bash and SSH. Visit https://git-scm.com to download and install. Follow the installer instructions. It is recommended to use all of the default installation options. Once installed, locate \"Git Bash\" on your machine to open the Bash terminal. It should be searchable in the Start Menu. To automate running ssh-agent add the following block to the file .bash_profile in the ~ directory within Git Bash. Then use source .bash_profile to start the ssh-agent , or open a new terminal. Note If such a file does not exist, please add it using nano .bash_profile to create a new file in the nano text editor. Copy and paste the block below into the text editor window. The character ^ means Ctrl . Use ^x ( Ctrl + X ) to exit, and continue following the prompts to save the file, using Ctrl shortcuts as needed. env = ~/.ssh/agent.env agent_load_env () { test -f \" $env \" && . \" $env \" > | /dev/null ; } agent_start () { ( umask 077 ; ssh-agent > | \" $env \" ) . \" $env \" > | /dev/null ; } agent_load_env # agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running agent_run_state = $( ssh-add -l > | /dev/null 2 > & 1 ; echo $? ) if [ ! \" $SSH_AUTH_SOCK \" ] || [ $agent_run_state = 2 ] ; then agent_start ssh-add elif [ \" $SSH_AUTH_SOCK \" ] && [ $agent_run_state = 1 ] ; then ssh-add fi Tip Git Bash can also be used with Windows Terminal using this stackoverflow answer: https://stackoverflow.com/a/57369284 . Generating Key Pairs \u00b6 The instructions for generating key pairs are identical for all operating systems. GitHub maintains excellent documentation on generating key pairs. The gist of those instructions follows. Open a terminal window. Use the command ssh-keygen -t ed25519 -C \"your_email@example.com\" You will be prompted to choose a location to store the key, including a file name. You will be prompted to enter a passphrase to secure the key. It is highly recommended to secure your key pair with a passphrase to minimize risk. Managing Keys \u00b6 The instructions below are the same for all operating systems with one small exception noted below. Important If at any point you encounter an error like below, please check to be sure your ssh-agent is running based on how you Installed your SSH Client . Could not open a connection to your authentication agent. Starting the SSH Agent for a Single Session \u00b6 If ssh-agent isn't already running and you encounter an error, use the following commands to start the ssh-agent depending on your environment. It is highly recommended to use the most appropriate method described in Install an SSH Client to have ssh-agent start automatically. Linux, MacOS, Git Bash, WSL: eval $(ssh-agent -s) Windows OpenSSH: start-ssh-agent Add a Private Key \u00b6 Move the key file to the .ssh directory under your home directory. Navigate to the .ssh folder in a terminal window. Run ssh-add <private_key_file> Bug For Linux users and WSL on Windows users. If you experience a Warning: Unprotected Private Key File error when using ssh-add , your ssh file and directory permissions may be incorrect. To fix, please use the following commands. sudo chmod 600 ~/.ssh/<private_key_file> sudo chmod 644 ~/.ssh/known_hosts # if you have ever connected to a remote machine sudo chmod 644 ~/.ssh/config # if you have a config file sudo chmod 755 ~/.ssh Tip MacOS allows storing passphrases to the builtin Keychain with a special flag. Use ssh-add -K <path/to/private_key_file> to permanently store the passphrase that goes with the key file. Remove a Private Key \u00b6 Run ssh-add -d <path/to/private_key_file> Push a New Public Key File to a Remote Machine \u00b6 To push a new public key file to a remote machine, please use the ssh-copy-id command. If your ssh-agent is running and has a known-good private key added, then the command below will work as expected and add the <new_public_keyfile>.pub to the remote machine. You must also have the private key counterpart <new_private_keyfile> with the same name as the public key file, without the .pub extension. ssh-copy-id -i ~/.ssh/<new_public_keyfile> <user>@<remote_ip> The value <user> should be replaced with the remote user you will login as. The value <remote_ip> should be replaced with the IP address of the remote machine. To verify, use ssh -i ~/.ssh/<new_private_keyfile>.pub <user>@<remote_ip> . Remove an Invalid Host Fingerprint \u00b6 Danger The following command should only be run when reusing a floating IP for a new instance in a cloud context. Using it arbitrarily for remote machines you do not control can result in a security breach. Be absolutely certain you trust the source of the key change. A \"Remote Host Identification Has Changed\" error can be resolved by using the following command. It looks like the image below. Run ssh-keygen -R <hostname> where <hostname> is the URL or IP address of the remote machine. Setting up a Configuration File \u00b6 SSH configuration files help streamline the process of logging in to remote terminals by storing commonly-used arguments and flags for each host. To create a configuration file, navigate to your .ssh directory. Create a new plain text file called config with no extension. Open the file and add content like the following. Note that indent matters. Variable values in <> will be replaced with appropriate values before saving. Host <host> HostName <remote_ip> User <user> IdentityFile <absolute_path_to_private_key_file> Be sure to give a meaningful name under <host> so you can easily refer back to this config later and for ease of typing when using ssh with this configuration. Only letters, numbers, dashes and underscores are allowed, and it must start with a letter. The value <remote_ip> can be any remote machine relevant to your work. For cloud.rc it should be whatever IP was assigned in Creating a Floating IP . The value <user> should be whatever user name you will log in as. For cloud.rc, ubuntu or centos are typical, depending on instance operating system. The value <path_to_private_key_file> is the absolute path to the private key file, e.g. the path to your .ssh folder followed by the <private_key_file> file name. For cloud.rc this will be whatever private key file was generated in Creating a Key Pair . Save the config file. Start a new terminal and use the command ssh <host> , with no other flags, to test. SSH Client Usage \u00b6 If you've Set up a Configuration File , simply use ssh <host> , using the configuration name, to connect. If you haven't set up a configuration file, use the following. ssh <user>@<remote_ip> -i <private_key_file> Where user is the remote username, remote_ip is the IP address of the remote machine, and <private_key_file> is the private key file used for access the remote machine. See Generating Key Pairs for general instructions on creating a key pair, or Creating a Key Pair for cloud.rc specific instructions. Server Software \u00b6 Remotely accessing server software requires configuration of Security Groups to open ports the server will communicate on. Please see our information on Installing Server Software for details. Data Transfer \u00b6 SCP \u00b6 SCP stands for S ecure C o P y and works like the cp command, but allows transferring files and directories with remote machines. SCP is built on top of SSH and is installed with most SSH Clients. To install SCP, see Install an SSH Client . Warning The OpenSSH developers recommend using SFTP instead of SCP. Future releases of OpenSSH will have SCP use SFTP protocol. The value <user> is the user you will login as on the remote machine <hostname> . Note that if you are using an SSH Configuration File with Host <host> , then replace all of <user>@<hostname> with just <host> , as you would with SSH. scp <source_file> <user>@<hostname>:<destination_file> # single file scp <source_file> <user>@<hostname>: # retains the file name scp -r <source_directory> <user>@<hostname>:<destination_directory> # full directory scp -r <source_directory> <user>@<hostname>: # retains the directory name Examples: # file scp script.py cheaha: scp script.py user@cheaha.rc.uab.edu:~/existing/shared_script.py scp script.py user@<cloud_vm_ip>: # directory scp -r my_scripts/ user@cheaha.rc.uab.edu: scp -r my_scripts/ cheaha:my_shared_scripts/ SFTP \u00b6 SFTP stands for S ecure F ile T ransfer P rotocol and allows transferring files and directories with remote machines. SFTP is built on top of SSH and is installed with most SSH Clients. To install SFTP, see Install an SSH Client . SFTP works differently from SCP , as it has an interactive prompt. When connected to a remote, the prompt sftp> will appears and enable use of SFTP commands. SFTP can also be used in batch mode with the -b <batch_file> argument. The plaintext <batch_file> should contain one SFTP command per line. To connect, use sftp <user>@<hostname> where <user> is the user you will login as on the remote machine <hostname> . If you are using an SSH Configuration File with Host <host> , you may use sftp <host> . You may optionally use sftp <host>:/path/to/dir to start in a specific directory. Some examples of commands are given below. A complete list is available here # general commands sftp> pwd # remote current directory sftp> lpwd # local current directory sftp> ls # contents of remote pwd sftp> lls # contents of local pwd sftp> mkdir my_dir # create my_dir on remote sftp> lmkdir my_dir # create my_dir on local sftp> cd my_dir # change directory on remote sftp> lcd my_dir # change directory on local # copy to remote sftp> put <local_file> <optional_remote_path> sftp> put -r <local_directory> <optional_remote_path> # if optional remote path is not supplied, uses pwd # file sftp> put script.py # copies file to pwd, same name sftp> put script.py shared_script.py # copies file to pwd, renames sftp> put script.py all_scripts/ # copies to pwd existing subdirectory of pwd # directory, must use trailing '/' character! sftp> put -r my_scripts/ # copies directory to pwd sftp> put -r my_scripts/ all_scripts/ # copies to existing subdirectory of pwd # copy from remote, same syntax as put, reversed direction sftp> get <remote_file> <optional_local_path> sftp> get -r <remote_directory> <optional_local_path> # if optional local path is not supplied, uses lpwd RClone \u00b6 Please see our RClone page for more information on using RClone with the SFTP remote option.","title":"Remote Access"},{"location":"uab_cloud/remote_access/#remote-access-to-instances","text":"All of the access methods described below are built on top of ssh and require completion of the steps in Basic Security Setup to use with cloud.rc . Some of these steps are referenced in that document.","title":"Remote Access to Instances"},{"location":"uab_cloud/remote_access/#command-line-via-ssh","text":"SSH stands for S ecure SH ell and is a powerful tool for executing terminal commands on remote machines. It is widely used and ubiquitous, and a number of other technologies are built on top of SSH, like sftp and scp for transferring files. It is also the primary mode of command line communication with Research Computing technologies like Cheaha and cloud.rc.","title":"Command Line via SSH"},{"location":"uab_cloud/remote_access/#install-an-ssh-client","text":"There are two main steps to working with SSH efficiently. The first is to ensure you have an SSH client installed, which will let your local machine communicate with remote machines. The second is to ensure you have ssh-agent running in each terminal window to automate management of key files. The ssh-agent software comes with most SSH clients, but does not always run automatically. How to start the ssh-agent software automatically varies depending on operating system and shell flavor, which we will describe below.","title":"Install an SSH Client"},{"location":"uab_cloud/remote_access/#terminal-multiplexers","text":"Terminal multiplexers are software that aggregate multiple SSH client sessions into one location. They often have the added benefit of keeping sessions alive on the remote machine, so short internet outages won't require you to log in again. For Linux, try using tmux : https://github.com/tmux/tmux/wiki/Installing For Windows, try using Windows Terminal: https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701","title":"Terminal Multiplexers"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-linux","text":"Virtually all Linux distributions come with SSH preinstalled and configured appropriately for ease of use, including automatically starting the ssh-agent .","title":"Install an SSH Client (Linux)"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-macos","text":"MacOS comes with an SSH client installed. If you are on version Leopard 10.5.1 or lower, you may want to have the ssh-agent start automatically using the command sudo touch /var/db/useLS at a terminal window. Versions newer than Leopard 10.5.1 start the ssh-agent automatically.","title":"Install an SSH Client (MacOS)"},{"location":"uab_cloud/remote_access/#install-an-ssh-client-windows","text":"There are several options for installing an SSH client on Windows, described below. It is highly recommended to install Windows Subsystem for Linux (WSL) as it provides a complete Linux environment within Windows.","title":"Install an SSH Client (Windows)"},{"location":"uab_cloud/remote_access/#windows-subsystem-for-linux-wsl","text":"Follow the instructions starting here to install Windows Subsystem for Linux. WSL shells do not automatically start or share the ssh-agent . To fix this we recommend installing keychain to automatically manage the ssh-agent . Run the following command depending on your Linux distribution. DEB-based (Debian, Ubuntu): sudo apt install keychain RPM-based (CentOS, Fedora, openSUSE): sudo yum install keychain Then modify the .*rc file for your shell, generally .bashrc or .zshrc , to automate the ssh-agent by adding the following line. - eval `keychain -q --eval --agents ssh` Tip You can access WSL files from within Windows in two ways. In the WSL terminal, enter explorer.exe . to open a File Explorer window in the current directory. In Windows, open a File Explorer window, click in the top navigation bar and enter \\\\wsl$ . Then select your distribution from the file window to access the filesystem of that WSL operating system.","title":"Windows Subsystem For Linux (WSL)"},{"location":"uab_cloud/remote_access/#openssh-for-windows","text":"Follow the instructions here to install the OpenSSH client. Only install the OpenSSH server if you need it, otherwise skip that part. Once the OpenSSH client is installed, you'll want to enable the OpenSSH Agent service on your local machine to streamline adding and using keys. Open the Start Menu and search for \"Services\", and open the result shown in the image. Find the \"OpenSSH Authentication Agent\" service in the list. Double click it, or right-click it and select \"Properties\". In the dialog box, under the \"General\" tab, look for \"Startup Type\". Click the drop-down menu and select \"Automatic (Delayed Start)\". Click \"Apply\" at the bottom-right corner. This will cause the ssh-agent service to start when Windows starts. The \"Start\" button under the horizontal line should become enabled. Click it to start the ssh-agent service now.","title":"OpenSSH for Windows"},{"location":"uab_cloud/remote_access/#git-bash-terminal-git-for-windows","text":"The fine folks at Git have worked very hard to package everything needed to use Git on Windows into one installer. This includes a Linux command line interface emulator, Bash and SSH. Visit https://git-scm.com to download and install. Follow the installer instructions. It is recommended to use all of the default installation options. Once installed, locate \"Git Bash\" on your machine to open the Bash terminal. It should be searchable in the Start Menu. To automate running ssh-agent add the following block to the file .bash_profile in the ~ directory within Git Bash. Then use source .bash_profile to start the ssh-agent , or open a new terminal. Note If such a file does not exist, please add it using nano .bash_profile to create a new file in the nano text editor. Copy and paste the block below into the text editor window. The character ^ means Ctrl . Use ^x ( Ctrl + X ) to exit, and continue following the prompts to save the file, using Ctrl shortcuts as needed. env = ~/.ssh/agent.env agent_load_env () { test -f \" $env \" && . \" $env \" > | /dev/null ; } agent_start () { ( umask 077 ; ssh-agent > | \" $env \" ) . \" $env \" > | /dev/null ; } agent_load_env # agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running agent_run_state = $( ssh-add -l > | /dev/null 2 > & 1 ; echo $? ) if [ ! \" $SSH_AUTH_SOCK \" ] || [ $agent_run_state = 2 ] ; then agent_start ssh-add elif [ \" $SSH_AUTH_SOCK \" ] && [ $agent_run_state = 1 ] ; then ssh-add fi Tip Git Bash can also be used with Windows Terminal using this stackoverflow answer: https://stackoverflow.com/a/57369284 .","title":"Git Bash terminal (Git for Windows)"},{"location":"uab_cloud/remote_access/#generating-key-pairs","text":"The instructions for generating key pairs are identical for all operating systems. GitHub maintains excellent documentation on generating key pairs. The gist of those instructions follows. Open a terminal window. Use the command ssh-keygen -t ed25519 -C \"your_email@example.com\" You will be prompted to choose a location to store the key, including a file name. You will be prompted to enter a passphrase to secure the key. It is highly recommended to secure your key pair with a passphrase to minimize risk.","title":"Generating Key Pairs"},{"location":"uab_cloud/remote_access/#managing-keys","text":"The instructions below are the same for all operating systems with one small exception noted below. Important If at any point you encounter an error like below, please check to be sure your ssh-agent is running based on how you Installed your SSH Client . Could not open a connection to your authentication agent.","title":"Managing Keys"},{"location":"uab_cloud/remote_access/#starting-the-ssh-agent-for-a-single-session","text":"If ssh-agent isn't already running and you encounter an error, use the following commands to start the ssh-agent depending on your environment. It is highly recommended to use the most appropriate method described in Install an SSH Client to have ssh-agent start automatically. Linux, MacOS, Git Bash, WSL: eval $(ssh-agent -s) Windows OpenSSH: start-ssh-agent","title":"Starting the SSH Agent for a Single Session"},{"location":"uab_cloud/remote_access/#add-a-private-key","text":"Move the key file to the .ssh directory under your home directory. Navigate to the .ssh folder in a terminal window. Run ssh-add <private_key_file> Bug For Linux users and WSL on Windows users. If you experience a Warning: Unprotected Private Key File error when using ssh-add , your ssh file and directory permissions may be incorrect. To fix, please use the following commands. sudo chmod 600 ~/.ssh/<private_key_file> sudo chmod 644 ~/.ssh/known_hosts # if you have ever connected to a remote machine sudo chmod 644 ~/.ssh/config # if you have a config file sudo chmod 755 ~/.ssh Tip MacOS allows storing passphrases to the builtin Keychain with a special flag. Use ssh-add -K <path/to/private_key_file> to permanently store the passphrase that goes with the key file.","title":"Add a Private Key"},{"location":"uab_cloud/remote_access/#remove-a-private-key","text":"Run ssh-add -d <path/to/private_key_file>","title":"Remove a Private Key"},{"location":"uab_cloud/remote_access/#push-a-new-public-key-file-to-a-remote-machine","text":"To push a new public key file to a remote machine, please use the ssh-copy-id command. If your ssh-agent is running and has a known-good private key added, then the command below will work as expected and add the <new_public_keyfile>.pub to the remote machine. You must also have the private key counterpart <new_private_keyfile> with the same name as the public key file, without the .pub extension. ssh-copy-id -i ~/.ssh/<new_public_keyfile> <user>@<remote_ip> The value <user> should be replaced with the remote user you will login as. The value <remote_ip> should be replaced with the IP address of the remote machine. To verify, use ssh -i ~/.ssh/<new_private_keyfile>.pub <user>@<remote_ip> .","title":"Push a New Public Key File to a Remote Machine"},{"location":"uab_cloud/remote_access/#remove-an-invalid-host-fingerprint","text":"Danger The following command should only be run when reusing a floating IP for a new instance in a cloud context. Using it arbitrarily for remote machines you do not control can result in a security breach. Be absolutely certain you trust the source of the key change. A \"Remote Host Identification Has Changed\" error can be resolved by using the following command. It looks like the image below. Run ssh-keygen -R <hostname> where <hostname> is the URL or IP address of the remote machine.","title":"Remove an Invalid Host Fingerprint"},{"location":"uab_cloud/remote_access/#setting-up-a-configuration-file","text":"SSH configuration files help streamline the process of logging in to remote terminals by storing commonly-used arguments and flags for each host. To create a configuration file, navigate to your .ssh directory. Create a new plain text file called config with no extension. Open the file and add content like the following. Note that indent matters. Variable values in <> will be replaced with appropriate values before saving. Host <host> HostName <remote_ip> User <user> IdentityFile <absolute_path_to_private_key_file> Be sure to give a meaningful name under <host> so you can easily refer back to this config later and for ease of typing when using ssh with this configuration. Only letters, numbers, dashes and underscores are allowed, and it must start with a letter. The value <remote_ip> can be any remote machine relevant to your work. For cloud.rc it should be whatever IP was assigned in Creating a Floating IP . The value <user> should be whatever user name you will log in as. For cloud.rc, ubuntu or centos are typical, depending on instance operating system. The value <path_to_private_key_file> is the absolute path to the private key file, e.g. the path to your .ssh folder followed by the <private_key_file> file name. For cloud.rc this will be whatever private key file was generated in Creating a Key Pair . Save the config file. Start a new terminal and use the command ssh <host> , with no other flags, to test.","title":"Setting up a Configuration File"},{"location":"uab_cloud/remote_access/#ssh-client-usage","text":"If you've Set up a Configuration File , simply use ssh <host> , using the configuration name, to connect. If you haven't set up a configuration file, use the following. ssh <user>@<remote_ip> -i <private_key_file> Where user is the remote username, remote_ip is the IP address of the remote machine, and <private_key_file> is the private key file used for access the remote machine. See Generating Key Pairs for general instructions on creating a key pair, or Creating a Key Pair for cloud.rc specific instructions.","title":"SSH Client Usage"},{"location":"uab_cloud/remote_access/#server-software","text":"Remotely accessing server software requires configuration of Security Groups to open ports the server will communicate on. Please see our information on Installing Server Software for details.","title":"Server Software"},{"location":"uab_cloud/remote_access/#data-transfer","text":"","title":"Data Transfer"},{"location":"uab_cloud/remote_access/#scp","text":"SCP stands for S ecure C o P y and works like the cp command, but allows transferring files and directories with remote machines. SCP is built on top of SSH and is installed with most SSH Clients. To install SCP, see Install an SSH Client . Warning The OpenSSH developers recommend using SFTP instead of SCP. Future releases of OpenSSH will have SCP use SFTP protocol. The value <user> is the user you will login as on the remote machine <hostname> . Note that if you are using an SSH Configuration File with Host <host> , then replace all of <user>@<hostname> with just <host> , as you would with SSH. scp <source_file> <user>@<hostname>:<destination_file> # single file scp <source_file> <user>@<hostname>: # retains the file name scp -r <source_directory> <user>@<hostname>:<destination_directory> # full directory scp -r <source_directory> <user>@<hostname>: # retains the directory name Examples: # file scp script.py cheaha: scp script.py user@cheaha.rc.uab.edu:~/existing/shared_script.py scp script.py user@<cloud_vm_ip>: # directory scp -r my_scripts/ user@cheaha.rc.uab.edu: scp -r my_scripts/ cheaha:my_shared_scripts/","title":"SCP"},{"location":"uab_cloud/remote_access/#sftp","text":"SFTP stands for S ecure F ile T ransfer P rotocol and allows transferring files and directories with remote machines. SFTP is built on top of SSH and is installed with most SSH Clients. To install SFTP, see Install an SSH Client . SFTP works differently from SCP , as it has an interactive prompt. When connected to a remote, the prompt sftp> will appears and enable use of SFTP commands. SFTP can also be used in batch mode with the -b <batch_file> argument. The plaintext <batch_file> should contain one SFTP command per line. To connect, use sftp <user>@<hostname> where <user> is the user you will login as on the remote machine <hostname> . If you are using an SSH Configuration File with Host <host> , you may use sftp <host> . You may optionally use sftp <host>:/path/to/dir to start in a specific directory. Some examples of commands are given below. A complete list is available here # general commands sftp> pwd # remote current directory sftp> lpwd # local current directory sftp> ls # contents of remote pwd sftp> lls # contents of local pwd sftp> mkdir my_dir # create my_dir on remote sftp> lmkdir my_dir # create my_dir on local sftp> cd my_dir # change directory on remote sftp> lcd my_dir # change directory on local # copy to remote sftp> put <local_file> <optional_remote_path> sftp> put -r <local_directory> <optional_remote_path> # if optional remote path is not supplied, uses pwd # file sftp> put script.py # copies file to pwd, same name sftp> put script.py shared_script.py # copies file to pwd, renames sftp> put script.py all_scripts/ # copies to pwd existing subdirectory of pwd # directory, must use trailing '/' character! sftp> put -r my_scripts/ # copies directory to pwd sftp> put -r my_scripts/ all_scripts/ # copies to existing subdirectory of pwd # copy from remote, same syntax as put, reversed direction sftp> get <remote_file> <optional_local_path> sftp> get -r <remote_directory> <optional_local_path> # if optional local path is not supplied, uses lpwd","title":"SFTP"},{"location":"uab_cloud/remote_access/#rclone","text":"Please see our RClone page for more information on using RClone with the SFTP remote option.","title":"RClone"},{"location":"uab_cloud/snapshots/","text":"Working with Snapshots \u00b6 Snapshots are instances or volumes frozen at a moment in time, able to be used in the future. Think of snapshots as a photograph of the state of an instance or volume. Anything done to an instance or volume after the snapshot is taken won't affect the snapshot. We can also create a new instance or volume from an existing snapshot, and continue from that point in time. An instance snapshot is referred to as an image. Volume snapshots do not have a special name. Images or Instance Snapshots \u00b6 Creating an Image \u00b6 Images are a helpful way to store the state of an instance for later use. Repeating tedious tasks like Software Installs can be avoided by taking a snapshot at a known-good point during set up of an instance environment, saving time in the future if something goes wrong. Images may also be shared with other users to simplify workflows and onboarding new collaborators. To create an image please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. To take a snapshot of a particular instance, click the drop down menu under the \"Actions\" column in the row of the desired instance. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Images\" page, where your new image will appear in its own row in the table. Note Notice the image has a size of zero bytes, which is expected and does not affect the ability to create instances. Images are a convenience pointer to the underlying volume snapshot, so they have no size themselves. The underlying volume snapshot does have a fixed size. To see the size of the underlying volume snapshot, click \"Volumes\" and then \"Snapshots\" in the left hand navigation menu. Creating an Instance from an Image \u00b6 To create an instance from an image, follow the directions below, assuming you have Created an Image . Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. Click the \"Launch Instance\" button. A dialog box will open. Follow the instructions at Basic Instance Setup until you get to the \"Source\" tab. In the \"Source\" tab, select \"Instance Snapshot\" under the \"Select Boot Source\" drop down menu. The \"Available\" table will change, and should contain your previously created instance snapshots. Press the up arrow in the appropriate row of the \"Available\" table to move that instance snapshot to the \"Allocated\" table. Note On the \"Flavor\" tab, only flavors with large enough disk capacity to hold the snapshot will be allowed. Flavors that are too small will show a yellow triangular caution symbol. Examples are shown below for a 40 GB instance snapshot. Continue following the instructions at Basic Instance Setup to start the instance. Deleting an Image \u00b6 To delete an image, return to the \"Images\" page using the left-hand navigation pane. In the table, find the row with the image you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Image\" to open a confirmation dialog. Click \"Delete Image\" again to delete the image permanently. Important You will not be able to delete the image if it has an associated volume snapshot or volume . They will need to be removed or deleted first. Volume Snapshots \u00b6 Creating a Volume Snapshot \u00b6 Volume snapshots are a helpful way to store the state of a volume for later use. They are used as the backing for Images, or Instance Snapshots , and have the same benefits. Most volume snapshots are created as part of an instance, but to create a volume snapshot directly please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Volumes\" and then \"Volumes\" in the left-hand navigation menu to open the \"Volumes\" page. To take a snapshot of a particular volume, click the drop down menu under the \"Actions\" column in the row of the desired volume. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Volume Snapshots\" page, where your new snapshot will appear in its own row in the table. Deleting a Volume Snapshot \u00b6 To delete a volume snapshot, return to the \"Volume Snapshots\" page using the left-hand navigation pane. In the table, find the row with the volume snapshot you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume Snapshot\" to open a confirmation dialog. Click \"Delete Volume Snapshot\" again to delete the volume snapshot permanently.","title":"Snapshots and Images"},{"location":"uab_cloud/snapshots/#working-with-snapshots","text":"Snapshots are instances or volumes frozen at a moment in time, able to be used in the future. Think of snapshots as a photograph of the state of an instance or volume. Anything done to an instance or volume after the snapshot is taken won't affect the snapshot. We can also create a new instance or volume from an existing snapshot, and continue from that point in time. An instance snapshot is referred to as an image. Volume snapshots do not have a special name.","title":"Working with Snapshots"},{"location":"uab_cloud/snapshots/#images-or-instance-snapshots","text":"","title":"Images or Instance Snapshots"},{"location":"uab_cloud/snapshots/#creating-an-image","text":"Images are a helpful way to store the state of an instance for later use. Repeating tedious tasks like Software Installs can be avoided by taking a snapshot at a known-good point during set up of an instance environment, saving time in the future if something goes wrong. Images may also be shared with other users to simplify workflows and onboarding new collaborators. To create an image please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. To take a snapshot of a particular instance, click the drop down menu under the \"Actions\" column in the row of the desired instance. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Images\" page, where your new image will appear in its own row in the table. Note Notice the image has a size of zero bytes, which is expected and does not affect the ability to create instances. Images are a convenience pointer to the underlying volume snapshot, so they have no size themselves. The underlying volume snapshot does have a fixed size. To see the size of the underlying volume snapshot, click \"Volumes\" and then \"Snapshots\" in the left hand navigation menu.","title":"Creating an Image"},{"location":"uab_cloud/snapshots/#creating-an-instance-from-an-image","text":"To create an instance from an image, follow the directions below, assuming you have Created an Image . Navigate to \"Compute\" and then \"Instances\" in the left-hand navigation menu to open the \"Instances\" page. Click the \"Launch Instance\" button. A dialog box will open. Follow the instructions at Basic Instance Setup until you get to the \"Source\" tab. In the \"Source\" tab, select \"Instance Snapshot\" under the \"Select Boot Source\" drop down menu. The \"Available\" table will change, and should contain your previously created instance snapshots. Press the up arrow in the appropriate row of the \"Available\" table to move that instance snapshot to the \"Allocated\" table. Note On the \"Flavor\" tab, only flavors with large enough disk capacity to hold the snapshot will be allowed. Flavors that are too small will show a yellow triangular caution symbol. Examples are shown below for a 40 GB instance snapshot. Continue following the instructions at Basic Instance Setup to start the instance.","title":"Creating an Instance from an Image"},{"location":"uab_cloud/snapshots/#deleting-an-image","text":"To delete an image, return to the \"Images\" page using the left-hand navigation pane. In the table, find the row with the image you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Image\" to open a confirmation dialog. Click \"Delete Image\" again to delete the image permanently. Important You will not be able to delete the image if it has an associated volume snapshot or volume . They will need to be removed or deleted first.","title":"Deleting an Image"},{"location":"uab_cloud/snapshots/#volume-snapshots","text":"","title":"Volume Snapshots"},{"location":"uab_cloud/snapshots/#creating-a-volume-snapshot","text":"Volume snapshots are a helpful way to store the state of a volume for later use. They are used as the backing for Images, or Instance Snapshots , and have the same benefits. Most volume snapshots are created as part of an instance, but to create a volume snapshot directly please follow the steps below. We assume you are already logged in at cloud.rc Navigate to \"Volumes\" and then \"Volumes\" in the left-hand navigation menu to open the \"Volumes\" page. To take a snapshot of a particular volume, click the drop down menu under the \"Actions\" column in the row of the desired volume. Then click \"Create Snapshot\". A dialog box will open. Fill in the \"Snapshot Name\" with a memorable name suitable for future reference, then click \"Create Snapshot\". See Naming Conventions . You will be taken to the \"Volume Snapshots\" page, where your new snapshot will appear in its own row in the table.","title":"Creating a Volume Snapshot"},{"location":"uab_cloud/snapshots/#deleting-a-volume-snapshot","text":"To delete a volume snapshot, return to the \"Volume Snapshots\" page using the left-hand navigation pane. In the table, find the row with the volume snapshot you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume Snapshot\" to open a confirmation dialog. Click \"Delete Volume Snapshot\" again to delete the volume snapshot permanently.","title":"Deleting a Volume Snapshot"},{"location":"uab_cloud/tutorial/","text":"cloud.rc Tutorial \u00b6 Welcome to the cloud.rc tutorial! Through this tutorial we will show you how to use cloud.rc to create your very own VM for scientific research on our OpenStack cloud service. The tutorial works best if you read and follow the instructions carefully, and follow the steps in order. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over. Prerequisites \u00b6 To access cloud.rc, you must either be on the UAB Campus Network or the UAB Campus VPN. Please visit https://cloud.rc.uab.edu to ensure you are able to access the site. You will also need an account created for you by our team. If you believe you need an account but do not have one, please Contact Us . Tutorial Sections \u00b6 Network - To do anything meaningful with a VM, it will need to be accessible to you and any collaborators. To make it accessible, you will first need to construct a virtual network by which it can communicate with the UAB Campus Network and beyond. This section will show you how to set up a bare-minimum network configuration. Security - Being able to communicate with the internet means we must take security precautions. This is not only good practice, but an IT Policy requirement . Some layers of security are enforced by how we've configured the cloud service. Your responsibility starts with ensuring the VM is only accessible on specific ports and by specific people. This section will show you how to do that. Instances - Now that we've got a secure network, we are ready to create an instance. Instances are the virtual computing devices used to do the scientific processing for your research. Your responsibility starts with selecting or designing the software you will use, understanding how to install and configure it correctly for your application, and how to use and troubleshoot it. We are not able to provide any support for software within virtual machines. Volumes - Optionally, you can set up persistent storage volumes. Advanced Information \u00b6 Once you have completed the tutorial, you might read over some of the following information to advance your understanding and facilitate usage of the tutorial. Remote Access - Different ways to access instances. Installing Software - Good practices and examples of installing common software. Snapshots - Creating reusable custom images based on VMs you've spent time configuring.","title":"cloud.rc Tutorial"},{"location":"uab_cloud/tutorial/#cloudrc-tutorial","text":"Welcome to the cloud.rc tutorial! Through this tutorial we will show you how to use cloud.rc to create your very own VM for scientific research on our OpenStack cloud service. The tutorial works best if you read and follow the instructions carefully, and follow the steps in order. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.","title":"cloud.rc Tutorial"},{"location":"uab_cloud/tutorial/#prerequisites","text":"To access cloud.rc, you must either be on the UAB Campus Network or the UAB Campus VPN. Please visit https://cloud.rc.uab.edu to ensure you are able to access the site. You will also need an account created for you by our team. If you believe you need an account but do not have one, please Contact Us .","title":"Prerequisites"},{"location":"uab_cloud/tutorial/#tutorial-sections","text":"Network - To do anything meaningful with a VM, it will need to be accessible to you and any collaborators. To make it accessible, you will first need to construct a virtual network by which it can communicate with the UAB Campus Network and beyond. This section will show you how to set up a bare-minimum network configuration. Security - Being able to communicate with the internet means we must take security precautions. This is not only good practice, but an IT Policy requirement . Some layers of security are enforced by how we've configured the cloud service. Your responsibility starts with ensuring the VM is only accessible on specific ports and by specific people. This section will show you how to do that. Instances - Now that we've got a secure network, we are ready to create an instance. Instances are the virtual computing devices used to do the scientific processing for your research. Your responsibility starts with selecting or designing the software you will use, understanding how to install and configure it correctly for your application, and how to use and troubleshoot it. We are not able to provide any support for software within virtual machines. Volumes - Optionally, you can set up persistent storage volumes.","title":"Tutorial Sections"},{"location":"uab_cloud/tutorial/#advanced-information","text":"Once you have completed the tutorial, you might read over some of the following information to advance your understanding and facilitate usage of the tutorial. Remote Access - Different ways to access instances. Installing Software - Good practices and examples of installing common software. Snapshots - Creating reusable custom images based on VMs you've spent time configuring.","title":"Advanced Information"},{"location":"uab_cloud/tutorial/instances/","text":"Instance Setup and Tutorial \u00b6 Instances are the basic unit of compute on cloud.rc. Requesting an instance involves a number of steps, and requires that a Network has already been setup, along with certain Security settings and features. It is also possible to attach persistent reusable Volumes to instances. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over. Creating an Instance \u00b6 Creating an instance is possibly a step you'll perform often, depending on your workflow. There are many smaller steps to create an instance, so please take care to check all the fields when you create an instance. These instructions require that you've set up a Network and followed all of the instructions on the linked page. You should have a Network, Subnet, Router and Floating IP. You will also need to setup a Key Pair and an SSH Security Group . Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Instances\". Click \"Launch Instance\" to open a dialog box. Fill out the dialog box completely. There are several tabs that will need to be completed. Details Tab \u00b6 Enter an \"Instance Name\". See Naming Conventions . Enter a \"Description\". Select \"nova\" in the \"Availability Zone\" drop down box. Select \"1\" in the \"Count\" field. Click \"Next >\" to move to the \"Source\" tab. Source Tab \u00b6 Sources determine what operating system or pre-defined image will be used as the starting point for your operating system (OS). Select \"Image\" in the \"Select Boot Source\" drop down box. Select \"Yes\" under \"Create New Volume\". Choose an appropriate \"Volume Size\" in GB . Note that for many single-use instances, 20 GB is more than enough. If you need more because you have persistent data, please create a persistent volume<volume_setup_basic> . Select \"Yes\" or \"No\" under \"Delete Volume on Instance Delete\" \"Yes\" is a good choice if you don't care about reusing the OS. \"No\" is a good choice if the OS volume will be reused. Pick an image from the list under the \"Available\" section. Use the search box to help find the image that best suits your research needs. When you find the best image, click the button with an up arrow next to the image. The image will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Flavor\" tab. Flavor Tab \u00b6 Flavors determine what hardware will be available to your instance, including cpus, memory and gpus. Pick an instance flavor form the list under the \"Available\" section. Use the search box to help find the flavor that best suits your needs. When you find the best flavor, click the button with an up arrow next to the flavor. The flavor will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Networks\" tab. Networks Tab \u00b6 Networks determine how your instance will talk to the internet and other instances. If you are following along with the tutorial, you should already have a Network set up. See Network for more information. Pick a network from the list under the \"Available' section. A Network may already be picked in the \"Allocated\" section. If this is not the correct Network, use the down arrow next to it to remove it from the \"Allocated\" section. If the Network is correct, skip (ii.) through (iv.). Use the search box to help find the Network that best suits your needs. When you find the best Network, click the button with an up arrow next to the Network. The Network will move to the \"Allocated\" section above the \"available\" section. Click \"Next >\" to move to the \"Network Ports\" tab. Network Ports Tab \u00b6 Leave this tab empty. Click \"Next >\" to move to the \"Security Groups\" tab. Security Groups Tab \u00b6 Security Groups allow for fine-grained control over external access to your instance. If you are following along with the tutorial, you should already have an \"ssh\" Security Group set up. For more information see Creating a Security Group for more information. Pick the \"ssh\" Security Group from the \"Available\" section by pressing the up arrow next to it. The \"default\" Security Group should already be in the \"Allocated\" section. Click \"Next >\" to move to the \"Key Pair\" tab. Key Pair Tab \u00b6 Key Pairs allow individual access rights to the instance via SSH. If you are following along with the tutorial, you should already have a key pair set up. For more information see Creating a Key Pair . Pick one or more key pairs from the list under the \"Available\" section. A Key Pair may already be picked in the \"Allocated\" section. If this is not the correct \"Key Pair\", use the down arrow next to it to remove it form the \"Allocated\" section. If the Key Pair is correct, skip (ii.) through (iv.). Use the search box to help find the Key Pair that best suits your needs. When you find the best Key Pair(s), click the button with an up arrow next to the Key Pair(s). The Key Pair(s) will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Configuration\" tab. Configuration Tab \u00b6 Skip this tab. Click \"Next >\" to move to the \"Server Groups\" tab. Server Groups Tab \u00b6 Skip this tab. Click \"Next >\" to move to the \"Scheduler Hints\" tab. Scheduler Hints Tab \u00b6 Skip this tab. Click \"Next >\" to move to the \"Metadata\" tab. Metadata Tab \u00b6 Skip this tab. Launching the Instance \u00b6 Click \"Launch Instance\" to launch the instance. Redirects to the \"Instances\" page. There should be a new entry in the table. The instance will take some time to build and boot. When the Status column entry says \"Active\" please move to the next steps. Associate a Floating IP \u00b6 If you are following along with the tutorial, you should already have a floating IP set up. In the \"Actions\" column entry, click the drop down triangle and select \"Associate Floating IP\". A dialog box will open. Select an IP address in the \"IP Address\" drop down box. Select a port in the \"Port to be associated\" drop down box. Click \"Associate\" to return to the \"Instances\" page and associate the selected IP. Instances Failing to Start \u00b6 There are a number of reasons an instance might fail. We are able to provide direct support for instances which fail to start for reasons outside the instance itself. To help us correct the error, you'll need to have information from the instance page. Below is an example of a failed instance in the \"Instances\" table, helpfully named failed_instance . Note the \"Error\" label under the \"Status\" column. In the \"Instances\" table, click the name of your failed instance. You should see a page like below, with some basic metadata about the instance as well as a \"Fault\" section. We will need to \"ID\" and the reason for the fault. In this case, the instance failed because it could not allocate a GPU, as all GPUs were allocated at the time of its creation. It is not possible to diagnose the specifics without consulting us, so please feel free to contact Support . Instances can fail for other reasons as well, please contact Support with the \"ID\" and \"Fault\" information. For instances which fail due to internal reasons, i.e. while using SSH or an application, we are still able to provide support but it will have to be on a case-by-case basis. Be prepared to walk us through the steps you took to set up the instance and any software, as well as any data processing steps, leading up to the failure. SSH Into the Instance \u00b6 If you are following the tutorial, then at this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You will need to Install an SSH Client Once your machine has an ssh client, use the following command. If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos or whatever is appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP , and the value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . Install an SSH Client to use SSH from your local machine to your cloud instance. Manage Your Private Key Start the SSH Agent to enable your system to remember your private key. Add a Private Key to the ssh agent to remember it for future use. Verify the SSH Client Works . Use the following command to connect ssh ubuntu@<floating ip> -i ~/.ssh/<private_key_file> If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos , or whatever may be appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP . The value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . (optional, but helpful) Set Up a Configuration File to simplify the command used to make a connection. Note Reusing a floating IP for a new instance can result in a \"Remote Host Identification Has Changed\" error, preventing connection. Please see Remove an Invalid Host Fingerprint . Streamlining SSH \u00b6 Refer to Setting up a Configuration File in Cloud Remote Access . Next Steps \u00b6 Now you are ready to Install Software , set up Security Groups for Servers , and optionally Create a Persistent Volume . Deleting an Instance \u00b6 Note Deleting Instances is not part of the tutorial, and is here as a reference. To delete an instance, return to the \"Instances\" page using the left-hand navigation pane. In the table, find the row with the instance you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Instance\" to open a confirmation dialog. Click \"Delete Instance\" again to delete the instance permanently. Warning It is highly recommended to shut off an instance before deleting it. Help my Instance is Stuck or not Working \u00b6 If your instance is stuck or otherwise not working as expected, first try deleting it and starting over. If you are unable to delete it or it gets stuck while deleting, please contact Support and copy the the instance ID as shown below. Where is my Instance ID? \u00b6 To find your instance ID, navigate to the \"Instances\" table. Click the \"Instance Name\" for the instance you are interested in to load an information page for that instance. In the instance information page, navigate to the \"Overview\" tab. Near the top is a field labeled \"ID\". The value to the right of \"ID\" is a Universally Unique ID (UUID) which uniquely names your instance. We need that ID to delete a stuck instance, so please provide it when requesting cloud.rc instance support. Continuing the Tutorial \u00b6 Now that you have set up a Network , Security Policies and an Instance , you are done with the tutorial, congratulations! There is one remaining optional step. If you need a persistent data volume to move between instances, please check our Volumes page.","title":"(3) Instances"},{"location":"uab_cloud/tutorial/instances/#instance-setup-and-tutorial","text":"Instances are the basic unit of compute on cloud.rc. Requesting an instance involves a number of steps, and requires that a Network has already been setup, along with certain Security settings and features. It is also possible to attach persistent reusable Volumes to instances. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.","title":"Instance Setup and Tutorial"},{"location":"uab_cloud/tutorial/instances/#creating-an-instance","text":"Creating an instance is possibly a step you'll perform often, depending on your workflow. There are many smaller steps to create an instance, so please take care to check all the fields when you create an instance. These instructions require that you've set up a Network and followed all of the instructions on the linked page. You should have a Network, Subnet, Router and Floating IP. You will also need to setup a Key Pair and an SSH Security Group . Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Instances\". Click \"Launch Instance\" to open a dialog box. Fill out the dialog box completely. There are several tabs that will need to be completed.","title":"Creating an Instance"},{"location":"uab_cloud/tutorial/instances/#details-tab","text":"Enter an \"Instance Name\". See Naming Conventions . Enter a \"Description\". Select \"nova\" in the \"Availability Zone\" drop down box. Select \"1\" in the \"Count\" field. Click \"Next >\" to move to the \"Source\" tab.","title":"Details Tab"},{"location":"uab_cloud/tutorial/instances/#source-tab","text":"Sources determine what operating system or pre-defined image will be used as the starting point for your operating system (OS). Select \"Image\" in the \"Select Boot Source\" drop down box. Select \"Yes\" under \"Create New Volume\". Choose an appropriate \"Volume Size\" in GB . Note that for many single-use instances, 20 GB is more than enough. If you need more because you have persistent data, please create a persistent volume<volume_setup_basic> . Select \"Yes\" or \"No\" under \"Delete Volume on Instance Delete\" \"Yes\" is a good choice if you don't care about reusing the OS. \"No\" is a good choice if the OS volume will be reused. Pick an image from the list under the \"Available\" section. Use the search box to help find the image that best suits your research needs. When you find the best image, click the button with an up arrow next to the image. The image will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Flavor\" tab.","title":"Source Tab"},{"location":"uab_cloud/tutorial/instances/#flavor-tab","text":"Flavors determine what hardware will be available to your instance, including cpus, memory and gpus. Pick an instance flavor form the list under the \"Available\" section. Use the search box to help find the flavor that best suits your needs. When you find the best flavor, click the button with an up arrow next to the flavor. The flavor will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Networks\" tab.","title":"Flavor Tab"},{"location":"uab_cloud/tutorial/instances/#networks-tab","text":"Networks determine how your instance will talk to the internet and other instances. If you are following along with the tutorial, you should already have a Network set up. See Network for more information. Pick a network from the list under the \"Available' section. A Network may already be picked in the \"Allocated\" section. If this is not the correct Network, use the down arrow next to it to remove it from the \"Allocated\" section. If the Network is correct, skip (ii.) through (iv.). Use the search box to help find the Network that best suits your needs. When you find the best Network, click the button with an up arrow next to the Network. The Network will move to the \"Allocated\" section above the \"available\" section. Click \"Next >\" to move to the \"Network Ports\" tab.","title":"Networks Tab"},{"location":"uab_cloud/tutorial/instances/#network-ports-tab","text":"Leave this tab empty. Click \"Next >\" to move to the \"Security Groups\" tab.","title":"Network Ports Tab"},{"location":"uab_cloud/tutorial/instances/#security-groups-tab","text":"Security Groups allow for fine-grained control over external access to your instance. If you are following along with the tutorial, you should already have an \"ssh\" Security Group set up. For more information see Creating a Security Group for more information. Pick the \"ssh\" Security Group from the \"Available\" section by pressing the up arrow next to it. The \"default\" Security Group should already be in the \"Allocated\" section. Click \"Next >\" to move to the \"Key Pair\" tab.","title":"Security Groups Tab"},{"location":"uab_cloud/tutorial/instances/#key-pair-tab","text":"Key Pairs allow individual access rights to the instance via SSH. If you are following along with the tutorial, you should already have a key pair set up. For more information see Creating a Key Pair . Pick one or more key pairs from the list under the \"Available\" section. A Key Pair may already be picked in the \"Allocated\" section. If this is not the correct \"Key Pair\", use the down arrow next to it to remove it form the \"Allocated\" section. If the Key Pair is correct, skip (ii.) through (iv.). Use the search box to help find the Key Pair that best suits your needs. When you find the best Key Pair(s), click the button with an up arrow next to the Key Pair(s). The Key Pair(s) will move to the \"Allocated\" section above the \"Available\" section. Click \"Next >\" to move to the \"Configuration\" tab.","title":"Key Pair Tab"},{"location":"uab_cloud/tutorial/instances/#configuration-tab","text":"Skip this tab. Click \"Next >\" to move to the \"Server Groups\" tab.","title":"Configuration Tab"},{"location":"uab_cloud/tutorial/instances/#server-groups-tab","text":"Skip this tab. Click \"Next >\" to move to the \"Scheduler Hints\" tab.","title":"Server Groups Tab"},{"location":"uab_cloud/tutorial/instances/#scheduler-hints-tab","text":"Skip this tab. Click \"Next >\" to move to the \"Metadata\" tab.","title":"Scheduler Hints Tab"},{"location":"uab_cloud/tutorial/instances/#metadata-tab","text":"Skip this tab.","title":"Metadata Tab"},{"location":"uab_cloud/tutorial/instances/#launching-the-instance","text":"Click \"Launch Instance\" to launch the instance. Redirects to the \"Instances\" page. There should be a new entry in the table. The instance will take some time to build and boot. When the Status column entry says \"Active\" please move to the next steps.","title":"Launching the Instance"},{"location":"uab_cloud/tutorial/instances/#associate-a-floating-ip","text":"If you are following along with the tutorial, you should already have a floating IP set up. In the \"Actions\" column entry, click the drop down triangle and select \"Associate Floating IP\". A dialog box will open. Select an IP address in the \"IP Address\" drop down box. Select a port in the \"Port to be associated\" drop down box. Click \"Associate\" to return to the \"Instances\" page and associate the selected IP.","title":"Associate a Floating IP"},{"location":"uab_cloud/tutorial/instances/#instances-failing-to-start","text":"There are a number of reasons an instance might fail. We are able to provide direct support for instances which fail to start for reasons outside the instance itself. To help us correct the error, you'll need to have information from the instance page. Below is an example of a failed instance in the \"Instances\" table, helpfully named failed_instance . Note the \"Error\" label under the \"Status\" column. In the \"Instances\" table, click the name of your failed instance. You should see a page like below, with some basic metadata about the instance as well as a \"Fault\" section. We will need to \"ID\" and the reason for the fault. In this case, the instance failed because it could not allocate a GPU, as all GPUs were allocated at the time of its creation. It is not possible to diagnose the specifics without consulting us, so please feel free to contact Support . Instances can fail for other reasons as well, please contact Support with the \"ID\" and \"Fault\" information. For instances which fail due to internal reasons, i.e. while using SSH or an application, we are still able to provide support but it will have to be on a case-by-case basis. Be prepared to walk us through the steps you took to set up the instance and any software, as well as any data processing steps, leading up to the failure.","title":"Instances Failing to Start"},{"location":"uab_cloud/tutorial/instances/#ssh-into-the-instance","text":"If you are following the tutorial, then at this stage you should be able to SSH into your instance from the UAB Campus Network or on the UAB Campus VPN. You will need to Install an SSH Client Once your machine has an ssh client, use the following command. If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos or whatever is appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP , and the value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . Install an SSH Client to use SSH from your local machine to your cloud instance. Manage Your Private Key Start the SSH Agent to enable your system to remember your private key. Add a Private Key to the ssh agent to remember it for future use. Verify the SSH Client Works . Use the following command to connect ssh ubuntu@<floating ip> -i ~/.ssh/<private_key_file> If your image uses an operating system other than Ubuntu, such as CentOS, replace the user ubuntu with centos , or whatever may be appropriate. The value <floating ip> should be whatever IP was assigned in Creating a Floating IP . The value <private_key_file> should be whatever your key pair file was named from Creating a Key Pair . (optional, but helpful) Set Up a Configuration File to simplify the command used to make a connection. Note Reusing a floating IP for a new instance can result in a \"Remote Host Identification Has Changed\" error, preventing connection. Please see Remove an Invalid Host Fingerprint .","title":"SSH Into the Instance"},{"location":"uab_cloud/tutorial/instances/#streamlining-ssh","text":"Refer to Setting up a Configuration File in Cloud Remote Access .","title":"Streamlining SSH"},{"location":"uab_cloud/tutorial/instances/#next-steps","text":"Now you are ready to Install Software , set up Security Groups for Servers , and optionally Create a Persistent Volume .","title":"Next Steps"},{"location":"uab_cloud/tutorial/instances/#deleting-an-instance","text":"Note Deleting Instances is not part of the tutorial, and is here as a reference. To delete an instance, return to the \"Instances\" page using the left-hand navigation pane. In the table, find the row with the instance you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Instance\" to open a confirmation dialog. Click \"Delete Instance\" again to delete the instance permanently. Warning It is highly recommended to shut off an instance before deleting it.","title":"Deleting an Instance"},{"location":"uab_cloud/tutorial/instances/#help-my-instance-is-stuck-or-not-working","text":"If your instance is stuck or otherwise not working as expected, first try deleting it and starting over. If you are unable to delete it or it gets stuck while deleting, please contact Support and copy the the instance ID as shown below.","title":"Help my Instance is Stuck or not Working"},{"location":"uab_cloud/tutorial/instances/#where-is-my-instance-id","text":"To find your instance ID, navigate to the \"Instances\" table. Click the \"Instance Name\" for the instance you are interested in to load an information page for that instance. In the instance information page, navigate to the \"Overview\" tab. Near the top is a field labeled \"ID\". The value to the right of \"ID\" is a Universally Unique ID (UUID) which uniquely names your instance. We need that ID to delete a stuck instance, so please provide it when requesting cloud.rc instance support.","title":"Where is my Instance ID?"},{"location":"uab_cloud/tutorial/instances/#continuing-the-tutorial","text":"Now that you have set up a Network , Security Policies and an Instance , you are done with the tutorial, congratulations! There is one remaining optional step. If you need a persistent data volume to move between instances, please check our Volumes page.","title":"Continuing the Tutorial"},{"location":"uab_cloud/tutorial/networks/","text":"Network Setup and Tutorial \u00b6 Networking setup should be a one-time setup. While Floating IPs fall under the Networking fold-out, they should be allocated and released together with instances to maximize security. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over. Networks \u00b6 Creating a Network \u00b6 Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network entry should already be in the table. Click \"+ Create Network\" to open a dialog box. Fill out the dialog box. Only the \"Network\" tab is important, we will create a subnet as a separate step. Enter a \"Network Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Uncheck \"Create Subnet\". We will do this as a separate step. The other tabs should be removed. Leave the \"Availability Zone Hints\" box empty. Click \"Create\". Redirects to the \"Networks\" page. There should be a new entry in the table with the name given in (4.a) Deleting a Network \u00b6 Note Deleting Networks is not part of the tutorial, and is here as a reference. To delete a network, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the network you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Network\" to open a confirmation dialog. Click \"Delete Network\" again to delete the network permanently. Important You will not be able to delete the network if it has a subnet with any connected routers or ports. They will need to be removed or deleted first. Subnets \u00b6 Creating a Subnet \u00b6 Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network should already be an entry in the table. At least one other entry must be in the table. See Creating a Network . Under the \"Actions\" column, select the drop-down triangle button in the row corresponding to the network you want to add a subnet to. Click \"Create Subnet\" in the drop-down to open a dialog box. Fill out the dialog box. The \"Subnet\" tab. Enter a \"Subnet Name\". See Naming Conventions . Enter 192.168.0.0/24 as the \"Network Address\". The trailing /24 allocates the entire range from 192.168.0.0 through 192.168.0.255 to the subnet. Ensure \"IPv4\" is selected in the \"IP Version\" drop-down box. Leave \"Gateway IP\" empty to use the default value of 192.168.0.0 . Leave \"Disable Gateway\" unchecked. Click the \"Next >>\" button to move to the \"Subnet Details\" tab. Note If you receive an error like Failed to create subnet `192.168.0.0/24`... Invalid input for operation: Gateway is not valid on a subnet. Try changing the gateway IP address to 192.168.0.1 and trying again. The \"Subnet Details\" tab. Leave \"Enable DHCP\" checked. Enter 192.168.0.20,192.168.0.100 in the \"Allocation Pools\" box. The IP addresses in that range will be assigned to instances on this subnet. Leave \"DNS Name Servers\" empty. Leave \"Host Routes\" empty. Click \"Create\". Redirects to the \"Overview\" page for the network the subnet was added to. Click the \"Subnets\" tab next to \"Overview\" to verify the subnet was added to the table for this network. Deleting a Subnet \u00b6 Note Deleting Subnets is not part of the tutorial, and is here as a reference. To delete a subnet, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the associated subnet, and click the name of the network to go to that network's page. Click on the \"Subnets\" tab to go to the subnets table. In the table, find the row with the subnet you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Subnet\" to open a confirmation dialog. Click \"Delete Subnet\" again to delete the subnet permanently. Important You will not be able to delete the subnet if it is associated with any routers or ports. They will need to be removed or deleted first. Routers \u00b6 Creating a Router \u00b6 To follow these directions for creating a router, a Network and Subnet must already exist. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Routers\" in the fold-out menu. Click \"+ Create Router\" to open a dialog box. Fill out the dialog box. Enter a \"Router Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Select \"uab-campus\" in the \"External Network\" drop down box. Leave the \"Availability Zone Hints\" box empty. Click \"Create Router\". Redirects to the \"Routers\" page. There should be a new entry in the table with the name given in (4.a) Now we need to connect the router to our subnet. Click the name of the new entry under the \"Name\" column to open the router \"Overview\" page. Click the \"Interfaces\" tab. Click \"+ Add Interface\" to open a dialog box. Fill out the dialog box. Select an existing network-subnet pair in the \"Subnet\" drop down box. If this is your only router on the selected subnet, leave \"IP Address\" empty to use the subnet gateway. Click \"Submit\" Redirects to the \"Interfaces\" page for the router. There should be a new entry in the table. Deleting a Router \u00b6 Note Deleting Routers is not part of the tutorial, and is here as a reference. To delete a router, return to the \"Routers\" page using the left-hand navigation pane. In the table, find the row with the router you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Router\" to open a confirmation dialog. Click \"Delete Router\" again to delete the router permanently. Floating IPs \u00b6 Creating a Floating IP \u00b6 Floating IPs are required if you want an instance to talk to devices on the internet. These IPs are a shared resource, so they must be allocated when needed and released when no longer needed. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Floating IPs\". Click \"Allocate IP to Project\" to open a dialog box. Fill out the dialog box. Select \"uab-campus\" in the \"Pool\" drop down box. Enter a \"Description\". Leave \"DNS Domain\" empty. Leave \"DNS Name\" empty. Click \"Allocate IP\". Redirects to the \"Floating IPs\" page. There should be a new entry in the table. Releasing a Floating IP \u00b6 Note Releasing Floating IPs is not part of the tutorial, and is here as a reference. To release a floating IP, return to the \"Floating IPs\" page using the left-hand navigation pane. In the table, find the row with the floating IP you wish to release, and click the drop-down arrow under \"Actions\" in that row. Then click \"Release Floating IP\" to open a confirmation dialog. Click \"Release Floating IP\" again to release the floating IP. Continuing the Tutorial \u00b6 Now that you have set up a Network , the next step is to apply Security Policies to be able to communicate with it. To continue the tutorial, please visit Security Policies next.","title":"(1) Networks"},{"location":"uab_cloud/tutorial/networks/#network-setup-and-tutorial","text":"Networking setup should be a one-time setup. While Floating IPs fall under the Networking fold-out, they should be allocated and released together with instances to maximize security. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.","title":"Network Setup and Tutorial"},{"location":"uab_cloud/tutorial/networks/#networks","text":"","title":"Networks"},{"location":"uab_cloud/tutorial/networks/#creating-a-network","text":"Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network entry should already be in the table. Click \"+ Create Network\" to open a dialog box. Fill out the dialog box. Only the \"Network\" tab is important, we will create a subnet as a separate step. Enter a \"Network Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Uncheck \"Create Subnet\". We will do this as a separate step. The other tabs should be removed. Leave the \"Availability Zone Hints\" box empty. Click \"Create\". Redirects to the \"Networks\" page. There should be a new entry in the table with the name given in (4.a)","title":"Creating a Network"},{"location":"uab_cloud/tutorial/networks/#deleting-a-network","text":"Note Deleting Networks is not part of the tutorial, and is here as a reference. To delete a network, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the network you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Network\" to open a confirmation dialog. Click \"Delete Network\" again to delete the network permanently. Important You will not be able to delete the network if it has a subnet with any connected routers or ports. They will need to be removed or deleted first.","title":"Deleting a Network"},{"location":"uab_cloud/tutorial/networks/#subnets","text":"","title":"Subnets"},{"location":"uab_cloud/tutorial/networks/#creating-a-subnet","text":"Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Networks\" in the fold-out menu. The \"Networks\" page will open. The \"uab_campus\" network should already be an entry in the table. At least one other entry must be in the table. See Creating a Network . Under the \"Actions\" column, select the drop-down triangle button in the row corresponding to the network you want to add a subnet to. Click \"Create Subnet\" in the drop-down to open a dialog box. Fill out the dialog box. The \"Subnet\" tab. Enter a \"Subnet Name\". See Naming Conventions . Enter 192.168.0.0/24 as the \"Network Address\". The trailing /24 allocates the entire range from 192.168.0.0 through 192.168.0.255 to the subnet. Ensure \"IPv4\" is selected in the \"IP Version\" drop-down box. Leave \"Gateway IP\" empty to use the default value of 192.168.0.0 . Leave \"Disable Gateway\" unchecked. Click the \"Next >>\" button to move to the \"Subnet Details\" tab. Note If you receive an error like Failed to create subnet `192.168.0.0/24`... Invalid input for operation: Gateway is not valid on a subnet. Try changing the gateway IP address to 192.168.0.1 and trying again. The \"Subnet Details\" tab. Leave \"Enable DHCP\" checked. Enter 192.168.0.20,192.168.0.100 in the \"Allocation Pools\" box. The IP addresses in that range will be assigned to instances on this subnet. Leave \"DNS Name Servers\" empty. Leave \"Host Routes\" empty. Click \"Create\". Redirects to the \"Overview\" page for the network the subnet was added to. Click the \"Subnets\" tab next to \"Overview\" to verify the subnet was added to the table for this network.","title":"Creating a Subnet"},{"location":"uab_cloud/tutorial/networks/#deleting-a-subnet","text":"Note Deleting Subnets is not part of the tutorial, and is here as a reference. To delete a subnet, return to the \"Networks\" page using the left-hand navigation pane. In the table, find the row with the associated subnet, and click the name of the network to go to that network's page. Click on the \"Subnets\" tab to go to the subnets table. In the table, find the row with the subnet you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Subnet\" to open a confirmation dialog. Click \"Delete Subnet\" again to delete the subnet permanently. Important You will not be able to delete the subnet if it is associated with any routers or ports. They will need to be removed or deleted first.","title":"Deleting a Subnet"},{"location":"uab_cloud/tutorial/networks/#routers","text":"","title":"Routers"},{"location":"uab_cloud/tutorial/networks/#creating-a-router","text":"To follow these directions for creating a router, a Network and Subnet must already exist. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Routers\" in the fold-out menu. Click \"+ Create Router\" to open a dialog box. Fill out the dialog box. Enter a \"Router Name\". See Naming Conventions . Leave \"Enable Admin State\" checked. Select \"uab-campus\" in the \"External Network\" drop down box. Leave the \"Availability Zone Hints\" box empty. Click \"Create Router\". Redirects to the \"Routers\" page. There should be a new entry in the table with the name given in (4.a) Now we need to connect the router to our subnet. Click the name of the new entry under the \"Name\" column to open the router \"Overview\" page. Click the \"Interfaces\" tab. Click \"+ Add Interface\" to open a dialog box. Fill out the dialog box. Select an existing network-subnet pair in the \"Subnet\" drop down box. If this is your only router on the selected subnet, leave \"IP Address\" empty to use the subnet gateway. Click \"Submit\" Redirects to the \"Interfaces\" page for the router. There should be a new entry in the table.","title":"Creating a Router"},{"location":"uab_cloud/tutorial/networks/#deleting-a-router","text":"Note Deleting Routers is not part of the tutorial, and is here as a reference. To delete a router, return to the \"Routers\" page using the left-hand navigation pane. In the table, find the row with the router you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Router\" to open a confirmation dialog. Click \"Delete Router\" again to delete the router permanently.","title":"Deleting a Router"},{"location":"uab_cloud/tutorial/networks/#floating-ips","text":"","title":"Floating IPs"},{"location":"uab_cloud/tutorial/networks/#creating-a-floating-ip","text":"Floating IPs are required if you want an instance to talk to devices on the internet. These IPs are a shared resource, so they must be allocated when needed and released when no longer needed. Click \"Network\" in the left-hand navigation pane to open the fold-out menu. Click \"Floating IPs\". Click \"Allocate IP to Project\" to open a dialog box. Fill out the dialog box. Select \"uab-campus\" in the \"Pool\" drop down box. Enter a \"Description\". Leave \"DNS Domain\" empty. Leave \"DNS Name\" empty. Click \"Allocate IP\". Redirects to the \"Floating IPs\" page. There should be a new entry in the table.","title":"Creating a Floating IP"},{"location":"uab_cloud/tutorial/networks/#releasing-a-floating-ip","text":"Note Releasing Floating IPs is not part of the tutorial, and is here as a reference. To release a floating IP, return to the \"Floating IPs\" page using the left-hand navigation pane. In the table, find the row with the floating IP you wish to release, and click the drop-down arrow under \"Actions\" in that row. Then click \"Release Floating IP\" to open a confirmation dialog. Click \"Release Floating IP\" again to release the floating IP.","title":"Releasing a Floating IP"},{"location":"uab_cloud/tutorial/networks/#continuing-the-tutorial","text":"Now that you have set up a Network , the next step is to apply Security Policies to be able to communicate with it. To continue the tutorial, please visit Security Policies next.","title":"Continuing the Tutorial"},{"location":"uab_cloud/tutorial/security/","text":"Security Policy Setup and Tutorial \u00b6 These instructions show you how to prepare to use SSH with your instances. At a minimum, an SSH security group and at least one key pair must be created. Other security groups can and should be added as needed for additional services. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over. Creating a Security Group \u00b6 Security Groups are used to set rules for how external devices can connect to your instances. Here we will create an SSH Security Group using a method that can be applied to other types of connections. Applications you develop may need other ports opened, so you may need to create additional security groups to handle those. Security groups may be reused across multiple instances. Click \"Networks\" in the left-hand navigation pane to open the fold-out menu. Click \"Security Groups\" in the fold out menu. Click \"+ Create Security Group\" to open a dialog box. Fill out the dialog box. Under \"Name\" enter ssh . Leave \"Description\" empty. Click \"Create Security Group\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be an entry for \"Egress IPv4\" and \"Egress IPv6\". Leave these alone. Click \"+ Add Rule\" to open a dialog box. Select \"SSH\" in the \"Rule\" drop down box. This will change the remaining fields. Leave \"Description\" empty. Select \"CIDR\" in the \"Remote\" drop down box. Type 0.0.0.0/0 in the \"CIDR\" box. For the sake of this tutorial, this value is fine. For properly securing virtual machines, see the \"Warning\" below for more information on better practice. Warning Using the value 0.0.0.0/0 for CIDR is short-hand for saying \"All possible IP addresses\". While cloud.rc is protected from external sources by the UAB firewall, using 0.0.0.0/0 does expose your virtual machine to all machines on the UAB internal network. Using the value 0.0.0.0/0 is the same as saying \"I trust the UAB firewall to protect my VM, and I trust UAB faculty, staff and students to not harm my VM\". Better practice is to limit the CIDR scope to only the IP address ranges that are relevant to your goals. As with all of cybersecurity, there is a security/convenience tradeoff to be made, and properly scoping CIDR will take more work than just using 0.0.0.0/0 . CIDR calculators are available on the internet to assist with calculation, just search for CIDR Calculator . Click \"Add\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be a new entry in the table. Important If you plan to Install Server Software , you will need to revisit this section to set up additional security groups for server ports. Deleting a Security Group \u00b6 Note Deleting Security Groups is not part of the tutorial, and is here as a reference. To delete a security group, return to the \"Security Groups\" page using the left-hand navigation pane. In the table, find the row with the security group you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Security Group\" to open a confirmation dialog. Click \"Delete Security Group\" again to delete the security group permanently. Creating a Key pair \u00b6 A Key Pair is required for SSH access to cloud.rc instances for security reasons. To use a Key Pair and SSH, you will need to Install an SSH Client on your local machine. Key Pairs are security devices used to authenticate and connect to a remote machine, like Cheaha or cloud.rc instances, and use Public-key cryptography to encrypt the connection. As the name suggests, there are two parts: a public key which is placed on the remote machine, and a private key which is kept secret on your personal machine. While key pairs can be reused between instances, we highly recommend using a new key pair with each instance to minimize risk if a private key becomes compromised. See Good Practices for more information. There are a number of pitfalls and potential issues that can arise during this process. For information on these pitfalls and for more information on managing key pairs, see Managing Keys . There are two ways to create a key pair: Use the cloud.rc interface to generate a key pair remotely and download the private key file . Use your personal computer to generate a key pair locally and upload the public key file. Good Practices \u00b6 Good practice is to only use one key pair per person and per local machine. So if you have two computers, each will need its own key pair. Using the same key pair for multiple machines means that they all become compromised when that key is compromised. Using different key pairs for each means only one machine becomes compromised. If you have two people, each will need their own key pair. Private keys are secrets and should not be passed between people, because there is no way to control it once it has been shared with even one other person. Copying the key increases the risk of the system being compromised by an attacker. If the key has to be revoked, you revoke access for every user at once. If you must share access, create a key pair for each additional person to increase security and convenience. Using a password protected Key Pair is highly recommended for additional security, as it buys time to revoke a key pair if it is compromised by an attacker. Currently, this is only possible by uploading a custom public key generated on your local machine. Generating a Key Pair on cloud.rc \u00b6 Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Click \"+ Create Key Pair\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop down box. Click \"+ Create Key Pair\" Opens a download file dialog box in your browser to download a file containing the secret private key. The file may have extension .pem or .crt depending on your operating system. Download the private key file. For security reasons this will be your only chance to ever obtain the private key from cloud.rc. If you lose this file you will have to generate a new Key Pair. Redirects to the \"Key Pairs\" page. There should be a new entry in the table. To add the private key on your local machine please see \"Add key\" under Add a Private Key . Generating a Key Pair Locally \u00b6 To generate a key pair, see instructions located at Generating Key Pairs . Click \"Import Public Key\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop-down box. Click \"Browse...\" to upload a public key file from your custom key pair OR copy-paste the content of that key file into the \"Public Key\" box. Danger Do not upload your private key file! The private key file must stay secret to ensure proper security, and it should never leave the computer it was generated on. Using the Key Pair \u00b6 Please see SSH Into the Instance for more information on using the Key Pair. Revoking a Key Pair \u00b6 Note Revoking Key Pairs is not part of the tutorial, and is here as a reference. Revoking a key pair from cloud.rc is simple. First, log on to the interface. Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Find the key pair you wish to revoke and click the \"Delete Key Pair\" button in that row. Optionally, Remove the Private Key from your local machine. This step is not necessary to ensure security, but can help maintain a clean environment. Continuing the Tutorial \u00b6 Now that you've set up a Network and Security Policies , you're ready to create a virtual machine (VM) Instance to work with. To continue the tutorial, please visit Instances next.","title":"(2) Security Policies"},{"location":"uab_cloud/tutorial/security/#security-policy-setup-and-tutorial","text":"These instructions show you how to prepare to use SSH with your instances. At a minimum, an SSH security group and at least one key pair must be created. Other security groups can and should be added as needed for additional services. Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.","title":"Security Policy Setup and Tutorial"},{"location":"uab_cloud/tutorial/security/#creating-a-security-group","text":"Security Groups are used to set rules for how external devices can connect to your instances. Here we will create an SSH Security Group using a method that can be applied to other types of connections. Applications you develop may need other ports opened, so you may need to create additional security groups to handle those. Security groups may be reused across multiple instances. Click \"Networks\" in the left-hand navigation pane to open the fold-out menu. Click \"Security Groups\" in the fold out menu. Click \"+ Create Security Group\" to open a dialog box. Fill out the dialog box. Under \"Name\" enter ssh . Leave \"Description\" empty. Click \"Create Security Group\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be an entry for \"Egress IPv4\" and \"Egress IPv6\". Leave these alone. Click \"+ Add Rule\" to open a dialog box. Select \"SSH\" in the \"Rule\" drop down box. This will change the remaining fields. Leave \"Description\" empty. Select \"CIDR\" in the \"Remote\" drop down box. Type 0.0.0.0/0 in the \"CIDR\" box. For the sake of this tutorial, this value is fine. For properly securing virtual machines, see the \"Warning\" below for more information on better practice. Warning Using the value 0.0.0.0/0 for CIDR is short-hand for saying \"All possible IP addresses\". While cloud.rc is protected from external sources by the UAB firewall, using 0.0.0.0/0 does expose your virtual machine to all machines on the UAB internal network. Using the value 0.0.0.0/0 is the same as saying \"I trust the UAB firewall to protect my VM, and I trust UAB faculty, staff and students to not harm my VM\". Better practice is to limit the CIDR scope to only the IP address ranges that are relevant to your goals. As with all of cybersecurity, there is a security/convenience tradeoff to be made, and properly scoping CIDR will take more work than just using 0.0.0.0/0 . CIDR calculators are available on the internet to assist with calculation, just search for CIDR Calculator . Click \"Add\". Redirects to the \"Manage Security Group Rules: ssh\" page. There should be a new entry in the table. Important If you plan to Install Server Software , you will need to revisit this section to set up additional security groups for server ports.","title":"Creating a Security Group"},{"location":"uab_cloud/tutorial/security/#deleting-a-security-group","text":"Note Deleting Security Groups is not part of the tutorial, and is here as a reference. To delete a security group, return to the \"Security Groups\" page using the left-hand navigation pane. In the table, find the row with the security group you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Security Group\" to open a confirmation dialog. Click \"Delete Security Group\" again to delete the security group permanently.","title":"Deleting a Security Group"},{"location":"uab_cloud/tutorial/security/#creating-a-key-pair","text":"A Key Pair is required for SSH access to cloud.rc instances for security reasons. To use a Key Pair and SSH, you will need to Install an SSH Client on your local machine. Key Pairs are security devices used to authenticate and connect to a remote machine, like Cheaha or cloud.rc instances, and use Public-key cryptography to encrypt the connection. As the name suggests, there are two parts: a public key which is placed on the remote machine, and a private key which is kept secret on your personal machine. While key pairs can be reused between instances, we highly recommend using a new key pair with each instance to minimize risk if a private key becomes compromised. See Good Practices for more information. There are a number of pitfalls and potential issues that can arise during this process. For information on these pitfalls and for more information on managing key pairs, see Managing Keys . There are two ways to create a key pair: Use the cloud.rc interface to generate a key pair remotely and download the private key file . Use your personal computer to generate a key pair locally and upload the public key file.","title":"Creating a Key pair"},{"location":"uab_cloud/tutorial/security/#good-practices","text":"Good practice is to only use one key pair per person and per local machine. So if you have two computers, each will need its own key pair. Using the same key pair for multiple machines means that they all become compromised when that key is compromised. Using different key pairs for each means only one machine becomes compromised. If you have two people, each will need their own key pair. Private keys are secrets and should not be passed between people, because there is no way to control it once it has been shared with even one other person. Copying the key increases the risk of the system being compromised by an attacker. If the key has to be revoked, you revoke access for every user at once. If you must share access, create a key pair for each additional person to increase security and convenience. Using a password protected Key Pair is highly recommended for additional security, as it buys time to revoke a key pair if it is compromised by an attacker. Currently, this is only possible by uploading a custom public key generated on your local machine.","title":"Good Practices"},{"location":"uab_cloud/tutorial/security/#generating-a-key-pair-on-cloudrc","text":"Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Click \"+ Create Key Pair\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop down box. Click \"+ Create Key Pair\" Opens a download file dialog box in your browser to download a file containing the secret private key. The file may have extension .pem or .crt depending on your operating system. Download the private key file. For security reasons this will be your only chance to ever obtain the private key from cloud.rc. If you lose this file you will have to generate a new Key Pair. Redirects to the \"Key Pairs\" page. There should be a new entry in the table. To add the private key on your local machine please see \"Add key\" under Add a Private Key .","title":"Generating a Key Pair on cloud.rc"},{"location":"uab_cloud/tutorial/security/#generating-a-key-pair-locally","text":"To generate a key pair, see instructions located at Generating Key Pairs . Click \"Import Public Key\" to open a dialog box. Fill out the dialog box. Enter a \"Key Pair Name\". See Naming Conventions . Select \"SSH Key\" in the \"Key Type\" drop-down box. Click \"Browse...\" to upload a public key file from your custom key pair OR copy-paste the content of that key file into the \"Public Key\" box. Danger Do not upload your private key file! The private key file must stay secret to ensure proper security, and it should never leave the computer it was generated on.","title":"Generating a Key Pair Locally"},{"location":"uab_cloud/tutorial/security/#using-the-key-pair","text":"Please see SSH Into the Instance for more information on using the Key Pair.","title":"Using the Key Pair"},{"location":"uab_cloud/tutorial/security/#revoking-a-key-pair","text":"Note Revoking Key Pairs is not part of the tutorial, and is here as a reference. Revoking a key pair from cloud.rc is simple. First, log on to the interface. Click \"Compute\" in the left-hand navigation pane to open the fold-out menu. Click \"Key Pairs\". Find the key pair you wish to revoke and click the \"Delete Key Pair\" button in that row. Optionally, Remove the Private Key from your local machine. This step is not necessary to ensure security, but can help maintain a clean environment.","title":"Revoking a Key Pair"},{"location":"uab_cloud/tutorial/security/#continuing-the-tutorial","text":"Now that you've set up a Network and Security Policies , you're ready to create a virtual machine (VM) Instance to work with. To continue the tutorial, please visit Instances next.","title":"Continuing the Tutorial"},{"location":"uab_cloud/tutorial/volumes/","text":"Volume Setup and Tutorial \u00b6 These instructions are intended for researchers who want to setup a persistent volume for use across instances. To follow these instructions you'll need to have already setup an Instance . Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over. Creating a Volume \u00b6 Click the \"Volumes\" fold-out in the left-hand navigation pane - the fold-out should open. Click \"Volumes\" within the fold-out to open the \"Volumes\" table page. Click \"+ Create Volume\" to open a dialog box. Fill out the dialog box. Enter a \"Volume Name\". See Naming Conventions . Enter a \"Description\". Select \"No source, empty volume\" in the \"Volume Source\" drop-down box to create an empty volume. Select \"__DEFAULT__\" in the \"Type\" drop down box. Select a size in GB appropriate for your needs. Select \"nova\" in the \"Availability Zone\" drop down box. Select \"No group\" in the \"Group\" drop down box. Click \"Create Volume\" Returns to the \"Volumes\" table page. There will be a new entry in the \"Volumes\" table. Attaching a Volume to a Running Instance \u00b6 To attach a volume you must have already created at least one using the cloud.rc interface. More information can be found in [link] Open the instances table by clicking \"Compute\" in the left-hand navigation pane and clicking \"Instances\". In the \"Actions\" column entry, click the drop down triangle button and select \"Attach Volume\". A dialog box will open. Select a volume in the \"Volume ID\" drop down box. Click \"Attach Volume\". Now the volume should be attached to the instance. From here you may format the volume and mount it. Formatting a Volume \u00b6 To format a volume, you must have created a volume and attached it to an instance capable of formatting it correctly. These instructions assume a Linux operating system. Click \"Compute\" in the left-hand navigation pane, then open the \"Instances\" menu. Click the name of any instance you wish to use to format the volume. Then click \"Overview\". Scroll down to \"Volumes Attached\" and make note of the <mount> part of <volume-name> on <mount> for your attached volume as it will be used in later steps. SSH into the instance from your local machine or from Cheaha. Verify the volume is attached by using sudo fdisk -l | egrep \"<mount>\"\" Format the volume using sudo fdisk \"<mount>\" You will be in the fdisk utility. Enter n to create a new partition. Enter p to make it the primary partition. Enter numeral 1 to make it the first partition. Press enter to accept the default first sector. Press enter to accept the default last sector. Enter t to change partition type. Enter numerals 83 to change to Linux partition type. Enter p to display the partition setup. Note that the partition will be labeled <mount>1 . This literally whatever <mount> was from earlier followed by the numeral 1 . Further steps will refer to this as <pmount> Enter w to execute the setup prepared in the previous substeps. Verify the volume is not mounted using sudo mount | egrep \"<mount>\" . If there is no output, then move to the next step. If there is some output then use sudo umount -l \"<mount>\" to unmount the volume and verify again. Create the filesystem using sudo mkfs.ext4 \"<pmount>\" . Ensure that the output looks like the following: ubuntu@my-instance:~$ sudo mkfs.ext4 /dev/vdb1 mke2fs 1 .45.5 ( 07 -Jan-2020 ) Discarding device blocks: done Creating filesystem with 26214144 4k blocks and 6553600 inodes Filesystem UUID: 335704a9-2435-440a-aeea-8ae29438ac64 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 , 1605632 , 654208 , 4096000 , 7962624 , 11239424 , 20480000 , 23887872 Allocating group tables: done Writing inode tables: done Creating journal ( 131072 blocks ) : done Writing superblocks and filesystem accounting information: done The volume is now formatted and ready for mounting within an attached instance OS. You will need to make note of <pmount> for when you are ready to mount the volume to an instance. Mounting a Volume in an Instance \u00b6 Mounting a volume needs to be done once per instance it will be attached to. It is assumed you've already created and formatted a volume and attached it to some instance. You'll need the <pmount> label from when you formatted the volume. SSH into the instance from your local machine or from Cheaha. Obtain the uuid of the volume using sudo blkid | egrep \"<pmount>\" . This will be referred to as <uuid> in future steps. Create a directory to mount the volume as. A good choice is sudo mkdir /mnt/<volume-name> where <volume-name> is something meaningful for you or your project. This directory will be referred to as <directory> in future steps. Mount the volume to the directory using sudo mount -U <uuid> <directory> . Verify the volume is mounted using df -h | egrep <pmount> Edit the fstab file to make mounting persistent across instance reboots. Edit the file using sudo nano /etc/fstab . Add the following line to the file: /dev/disk/by-uuid/<uuid> <directory> auto defaults,nofail 0 3 Verify fstab was modified correctly by soft rebooting the instance and verifying the mount again using df -h | egrep \"<pmount>\" . Set access control using the following commands: sudo apt install acl # or yum install, etc., if not already installed sudo setfacl -R -m u:<username>:rwx <directory> Verify the access controls were modified correctly by creating a test file and then listing files in <directory> to ensure the file was created. The following commands will achieve this: cd <directory> touch testfile ls The volume is now mounted to your instance and ready for use and re-use across sessions and reboots. Deleting a Volume \u00b6 Note Deleting a Volume is not part of the tutorial, and is here as a reference. To delete a volume, return to the \"Volumes\" page using the left-hand navigation pane. In the table, find the row with the volume you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume\" to open a confirmation dialog. Click \"Delete Volume\" again to delete the volume permanently. Important It will not be possible to delete a volume if it has an associated volume snapshot . The snapshot will need to be deleted first.","title":"(4) Volumes (optional)"},{"location":"uab_cloud/tutorial/volumes/#volume-setup-and-tutorial","text":"These instructions are intended for researchers who want to setup a persistent volume for use across instances. To follow these instructions you'll need to have already setup an Instance . Important If you are viewing this page as part of the cloud.rc tutorial, please follow the steps in order from top to bottom. Ignore any sections on deleting or releasing resources unless you need to correct a mistake. Note Virtual machines are disposable! If you get stuck at any point, or things don't seem like they're working as expected, etc., feel free to delete the instance and start over.","title":"Volume Setup and Tutorial"},{"location":"uab_cloud/tutorial/volumes/#creating-a-volume","text":"Click the \"Volumes\" fold-out in the left-hand navigation pane - the fold-out should open. Click \"Volumes\" within the fold-out to open the \"Volumes\" table page. Click \"+ Create Volume\" to open a dialog box. Fill out the dialog box. Enter a \"Volume Name\". See Naming Conventions . Enter a \"Description\". Select \"No source, empty volume\" in the \"Volume Source\" drop-down box to create an empty volume. Select \"__DEFAULT__\" in the \"Type\" drop down box. Select a size in GB appropriate for your needs. Select \"nova\" in the \"Availability Zone\" drop down box. Select \"No group\" in the \"Group\" drop down box. Click \"Create Volume\" Returns to the \"Volumes\" table page. There will be a new entry in the \"Volumes\" table.","title":"Creating a Volume"},{"location":"uab_cloud/tutorial/volumes/#attaching-a-volume-to-a-running-instance","text":"To attach a volume you must have already created at least one using the cloud.rc interface. More information can be found in [link] Open the instances table by clicking \"Compute\" in the left-hand navigation pane and clicking \"Instances\". In the \"Actions\" column entry, click the drop down triangle button and select \"Attach Volume\". A dialog box will open. Select a volume in the \"Volume ID\" drop down box. Click \"Attach Volume\". Now the volume should be attached to the instance. From here you may format the volume and mount it.","title":"Attaching a Volume to a Running Instance"},{"location":"uab_cloud/tutorial/volumes/#formatting-a-volume","text":"To format a volume, you must have created a volume and attached it to an instance capable of formatting it correctly. These instructions assume a Linux operating system. Click \"Compute\" in the left-hand navigation pane, then open the \"Instances\" menu. Click the name of any instance you wish to use to format the volume. Then click \"Overview\". Scroll down to \"Volumes Attached\" and make note of the <mount> part of <volume-name> on <mount> for your attached volume as it will be used in later steps. SSH into the instance from your local machine or from Cheaha. Verify the volume is attached by using sudo fdisk -l | egrep \"<mount>\"\" Format the volume using sudo fdisk \"<mount>\" You will be in the fdisk utility. Enter n to create a new partition. Enter p to make it the primary partition. Enter numeral 1 to make it the first partition. Press enter to accept the default first sector. Press enter to accept the default last sector. Enter t to change partition type. Enter numerals 83 to change to Linux partition type. Enter p to display the partition setup. Note that the partition will be labeled <mount>1 . This literally whatever <mount> was from earlier followed by the numeral 1 . Further steps will refer to this as <pmount> Enter w to execute the setup prepared in the previous substeps. Verify the volume is not mounted using sudo mount | egrep \"<mount>\" . If there is no output, then move to the next step. If there is some output then use sudo umount -l \"<mount>\" to unmount the volume and verify again. Create the filesystem using sudo mkfs.ext4 \"<pmount>\" . Ensure that the output looks like the following: ubuntu@my-instance:~$ sudo mkfs.ext4 /dev/vdb1 mke2fs 1 .45.5 ( 07 -Jan-2020 ) Discarding device blocks: done Creating filesystem with 26214144 4k blocks and 6553600 inodes Filesystem UUID: 335704a9-2435-440a-aeea-8ae29438ac64 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 , 1605632 , 654208 , 4096000 , 7962624 , 11239424 , 20480000 , 23887872 Allocating group tables: done Writing inode tables: done Creating journal ( 131072 blocks ) : done Writing superblocks and filesystem accounting information: done The volume is now formatted and ready for mounting within an attached instance OS. You will need to make note of <pmount> for when you are ready to mount the volume to an instance.","title":"Formatting a Volume"},{"location":"uab_cloud/tutorial/volumes/#mounting-a-volume-in-an-instance","text":"Mounting a volume needs to be done once per instance it will be attached to. It is assumed you've already created and formatted a volume and attached it to some instance. You'll need the <pmount> label from when you formatted the volume. SSH into the instance from your local machine or from Cheaha. Obtain the uuid of the volume using sudo blkid | egrep \"<pmount>\" . This will be referred to as <uuid> in future steps. Create a directory to mount the volume as. A good choice is sudo mkdir /mnt/<volume-name> where <volume-name> is something meaningful for you or your project. This directory will be referred to as <directory> in future steps. Mount the volume to the directory using sudo mount -U <uuid> <directory> . Verify the volume is mounted using df -h | egrep <pmount> Edit the fstab file to make mounting persistent across instance reboots. Edit the file using sudo nano /etc/fstab . Add the following line to the file: /dev/disk/by-uuid/<uuid> <directory> auto defaults,nofail 0 3 Verify fstab was modified correctly by soft rebooting the instance and verifying the mount again using df -h | egrep \"<pmount>\" . Set access control using the following commands: sudo apt install acl # or yum install, etc., if not already installed sudo setfacl -R -m u:<username>:rwx <directory> Verify the access controls were modified correctly by creating a test file and then listing files in <directory> to ensure the file was created. The following commands will achieve this: cd <directory> touch testfile ls The volume is now mounted to your instance and ready for use and re-use across sessions and reboots.","title":"Mounting a Volume in an Instance"},{"location":"uab_cloud/tutorial/volumes/#deleting-a-volume","text":"Note Deleting a Volume is not part of the tutorial, and is here as a reference. To delete a volume, return to the \"Volumes\" page using the left-hand navigation pane. In the table, find the row with the volume you wish to delete, and click the drop-down arrow under \"Actions\" in that row. Then click \"Delete Volume\" to open a confirmation dialog. Click \"Delete Volume\" again to delete the volume permanently. Important It will not be possible to delete a volume if it has an associated volume snapshot . The snapshot will need to be deleted first.","title":"Deleting a Volume"},{"location":"workflow_solutions/getting_containers/","text":"Software Containers \u00b6 Containers help to manage software installations and all their dependencies in a single large image. These containers are a self-contained operating system along with any software the container creator added. Containers avoid software conflicts due to versioning as well as OS incompatibility and can be run on most, if not all, operating systems. The most common container engine is called Docker. Docker is an open-source platform for building, deploying, running, updating, and managing containers and has distributions for Linux, Windows, and Mac. Singularity is another common container engine specialized for use on HPC systems such as Cheaha where Docker cannot be used. Fantastic Containers and Where to Find Them \u00b6 Docker containers are available in https://hub.docker.com/ . This docker hub repository allows to share containers and use pre-existing docker images. It is often a good idea to search the Github repo for an application or pipeline to see if a container has already been provided by the authors. Containers on Cheaha \u00b6 Using containers on Cheaha bypasses the need to message support to install necessary software. Containers can be downloaded by any user into their personal space and used immediately without admin permission. as mentioned above, you will need to use Singularity containers on Cheaha. You can find all of the Singularity modules using the following command: module spider Singularity It's highly recommended to only use Singularity versions 3+. Pull Singularity Images \u00b6 Singularity can pull images from a variety of sources, including Dockerhub, and convert them to the proper format automatically. In order to download an image, use the pull subcommand followed by the output image name and the URI of the image. The general form of the command for pulling from Dockerhub is as follows: singularity pull <output.sif> docker://<account>/<image> [ :<version_tag> ] For example, if we wanted to pull the lolcow container : singularity pull lolcow.sif docker://godlovedc/lolcow We now have the lolcow.sif image we can run or share with other researchers. It's important to remember that containers are just independent files that can be moved, copied, or deleted the same as any other file. Running Singularity Images \u00b6 There are 3 ways to run Singularity images, all with their unique purposes and are as follows: singularity run : run a container using a default command set by the author. Generally, this will be used when a container encompasses a full pipeline controlled by a single command. The general form for this command is singularity run <image.sif> [options] where [options] are defined by the default command. You can use singularity run <image.sif> --help to see what those options are. singularity exec : run any command available in the container. This provides more flexibility than run and would be useful in the cases where a container has more modular components as opposed to a single control script. The general form for this would be singularity exec <image.sif> <command> [options] . You can add the --help option to see what a given command does and its inputs. singularity shell : allow interactive use of the container through the terminal. This changes your active environment to that in the container. You can traverse the container's directory tree and search for various files and commands as if it was a virtual machine. This is very useful for interactive development as well as investigation of a container's contents. The general form of the command is singularity shell <image.sif> . It's important to note that both run and exec enter the container as part of their execution and then exit back to the original shell environment afterwards whereas shell keeps you in the container until you either close the terminal or use the exit command. Important singularity shell is not executable via shell scripts. Any singularity commands in a batch script should be run or exec instead. Singularity Paths \u00b6 By default, Singularity containers have limited access to the general filesystem. Containers get default access to the /home directory as well as the directory the container was run from. If you run the container from $HOME but try to access files in $USER_DATA , you will see an error. In order to give a container access to other directories, use the -B or --bind option when invoking the container. For instance, if I wanted to use run on a container that had an input option called -i and give the container access to a subfolder called my_data in a project space called UABRC , the singularity command would look like: singularity run --bind /data/project/UABRC/my_data image.sif -i /data/project/UABRC/my_data You can also alias the bind path to a shorter name and use it in the command. In that case, the bind option would look like --bind </directory_path>:</alias> . For example, if I was running a container and was giving the my_data directory as an input, I could alias it to /input_data and use it in the command like so: singularity run --bind /data/project/UABRC/my_data:/input_data image.sif -i /input_data These bind paths can be used in both exec and shell subcommands as well. Note Bind paths cannot grant access to folders and files your account does not have access to. For instance, you cannot use a container to access data in another user's account unless that user has explicitly given you the correct permissions via chmod or ACLs. Using Containers on UAB RC Cloud (cloud.rc.uab.edu) \u00b6 To access docker containers, install Docker in your system. To install docker desktop on your computer, follow this link: Docker Desktop Page . Docker Installation on UAB RC Cloud \u00b6 Following are the installation instructions to install Docker on UAB RC Cloud with Ubuntu operating system. Tested the installation on Ubuntu 20.04. Setting up UAB RC Cloud account can be found in UAB RC Cloud . sudo apt-get update sudo apt install docker.io Using a Docker Container from DockerHub \u00b6 We can start pulling a container named alpine from the Docker hub. alpine is a general-purpose Linux distribution. Look for the container alpine in the docker hub, copy the pull command, and paste it into your terminal. sudo docker pull alpine Once the image is pulled, you can verify if the image exists using the below command. Note that if you do not specify the tag/version of the container, the recent version is built, and the tag is listed as latest . sudo docker images If you prefer to pull a particular version of the alpine container, you need to mention the tag details in your pull command. You can see the available tags/versions of alpine from the Docker hub. To pull particular version of alpine container, use the below syntax. sudo docker pull container_name:tag Here the container_name is alpine , and the tag is 3.14 . sudo docker pull alpine:3.14 The existing image looks like, Create Your Own Docker Container \u00b6 You can create your own Docker container, build it, and upload/share them in the Docker hub or UAB GitLab container registry. Let us take a synthetic python code and formulate the packages/dependencies required to build your software container. Below is a python script that requires packages, namely, numpy, scipy, and matplotlib. Next, the steps to create a Dockerfile is illustrated. Let us name this script python_test.py . import numpy as np import matplotlib import pylab import matplotlib.pylab as plt import scipy.integrate as integrate a = np.array ([ 0 , 10 , 20 , 30 , 40 ]) print ( a ) b = np.arange ( -5, 5 , 0 .5 ) print ( b ) t = np.arange ( 0 ,20.5,0.5 ) print ( t ) result = integrate.quad ( np.sin, 0 , np.pi ) print ( result ) plt.plot ([ 1 , 2 , 3 , 4 ] , [ 1 , 4 , 9 , 16 ]) plt.show () plt.savefig ( 'testing.png' ) Create a Dockerfile that has Miniconda Installed \u00b6 We require numpy, scipy, and matplotlib libraries to execute the above Python script. Following are the steps to create a specification file and build a container image. Create an empty directory miniconda . mkdir miniconda Create a Dockerfile within the miniconda directory with the following contents. The file name Dockerfile is case-sensitive. # You may start with a base image # Always use a specific tag like \"4.10.3\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda3:4.12.0 # Use RUN to execute commands inside the miniconda image RUN conda install -y numpy \">=1.16.5, <1.23.0\" # RUN multiple commands together # Last two lines are cleaning out the local repository and removing the state # information for installed package RUN apt-get update \\ && conda install -y scipy = 1 .7.3 \\ && conda install -y matplotlib = 3 .5.1 \\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is the specification file. It provides Docker with the software information, and versions, it needs to build our new container. See the Docker Container documentation for more information https://docs.docker.com/engine/reference/builder/ . In the Dockerfile, we start with an existing container continuumio/miniconda3:4.12.0 . This container is obtained from Dockerhub; here, continuumio is the producer, and the repo name is continuumio/miniconda3 . You may specify the required version from the Tag list. Here the tag/version is 4.12.0 . Also its a very good practice to specify the version of packages for numpy, scipy, and matplotlib for better reproducibility. Containers and Reproducibiliy Always include version numbers for Anaconda, package managers, software you are installing, and the dependencies for those software. Containers are not by nature scientifically reproducible, but if you include versions for as much software in the container as possible, they can be reproducible years later. To build your container, change the directory to miniconda and use the below syntax to build the Dockerfile . Here we use . to say \"current directory.\" This will only work if you are in the directory with the Dockerfile . sudo docker build -t repository_name:tag . Here the repository_name is py3-miniconda and the tag is 2022-08 . cd miniconda sudo docker build -t py3-miniconda:2022-08 . Note The . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside. Also, you may rename the repository_name and tag as you prefer. sudo docker images Running the Built Miniconda Docker Container Interactively \u00b6 To run docker interactively and execute commands inside the container, use the below syntax. Here run executes the command in a new container, and -it starts an interactive shell inside the container. After executing this command, the command prompt will change and move into the bash shell. sudo docker run -it repository_name:tag /bin/bash To execute your container py3-miniconda interactively, run this command with the tag `2022-08'. sudo docker run -it py3-miniconda:2022-08 /bin/bash cd /opt/conda/bin/ The python executables to execute our synthetic python script are within the directory structure /opt/conda/bin . Mounting Data Onto a Container \u00b6 Before we mount data onto a container, remember you initially created the python script python_test.py when creating your own container. Move python_test.py within miniconda directory. Now you have your miniconda/python_test.py outside the container. To access the files outside the container you should mount the file path along with the docker run command. To mount a host directory into your docker container, use the -v flag. sudo docker run -v /host/directory/:/container/directory -other-options So the command for our example will be, sudo docker run -v /home/ubuntu/:/home -it py3-miniconda:2022-08 /bin/sh Here we are mounting the $HOME directory /home/ubuntu from a host into containers' $HOME directory. Note that you may mount a particular directory according to your preference. The following shows the list of files in containers' $HOME directory with and without mounting. Before mounting, there are no files found within the $HOME directory. After mounting using -v flag, files show up within the $HOME directory. The highlighted miniconda is our working directory with python script. We can now execute the script, python_test.py using this command. python python_test.py More lessons on Docker can be found in this link: Introduction to Docker and Docker Documentation . Sharing Containers Using UAB GitLab Container Registry \u00b6 If you prefer to share your container with a particular team/group, then the UAB GitLab container registry is the best and most secure option. The following steps help you to create a container registry in UAB GitLab: Create a UAB Gitlab account following the guidelines from the UAB GitLab page . Create a new_project on UAB GitLab and click Package and Registries , and then go to Container Registry . Initially, the container registry looks empty because there are no container images in the registry. Note Copy these CLI commands for future reference. It contains commands (1) to login to your project UAB GitLab container registry (2) Add an image to the registry using the push/build command. We will use the push command as we already have the existing container in our system. Login to UAB GitLab Registry using your registry_name:ID . sudo docker login gitlab.rc.uab.edu:4567 Note The registry_name and ID shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing. Note: For securing concerns, use an access token to log in. Create an access token in UAB GitLab to push/pull the docker container in the container registry (Secure token and guidelines to follow are shown next). sudo docker login gitlab.rc.uab.edu:4567 -u username \u2013p access_token Creating an Access Token: From the UAB GitLab page, you can create an access token instead of using a password to log in to the UAB GitLab registry. Goto Edit profile -> Click Access Tokens . Then enter: Token name. :Suggestion: \"container\"_\"repository-name\" Expiry date. Suggestion: Default is 30 days. You can set your expiry date 3 months from the date you are making it. Under select scopes, check read and write registry (to push images to the registry) -> Then click create personal access token . Once you create the token, copy the new personal access token since it\u2019s a one-time step and hard to retrieve after a refresh. Use the personal access token for login. Warning Running docker login leads to a warning message that your password is stored unencrypted in /root/.docker/config.json (or) $HOME/.docker/config.json . To ignore this warning, follow the instructions in this Github page or the Docker credentials store page . Push Alpine Container from your System to UAB GitLab Container Registry \u00b6 List the docker images on your local computer using the docker images command. An alpine image exists already on this computer. Your container will likely have a different name. sudo docker images Tag alpine to push into UAB GitLab registry. We need to have the UAB GitLab registry name to push. It will show the default command on the container registry page. Copy these commands for future reference. The tag is test here. sudo docker tag alpine:latest gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test You can see the tag test associated with the alpine image. sudo docker images The below first command is the syntax to push the Docker image to the UAB GitLab container registry from your computer. The second command is an example of pushing a Docker image to the UAB GitLab container registry from your computer. Note The registry_name,ID, and gitlab_group_name shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing. sudo docker push gitlab_registry_name:ID/gitlab_group_name/project_name:tag sudo docker push gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test In your GitLab's page container registry, refresh to view the alpine container is pushed to the registry. Now lets pull the alpine container from GitLab's page container registry to your system. Before that, remove the previous image from the system, which already has a test tag, to avoid discrepancies. sudo docker rmi -f image_id In your GitLab's page container registry, copy the pull command from the test container registry, and use it to pull the docker container to your system. You can see the image is reflected in the image list. sudo docker pull gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test","title":"Software Containers"},{"location":"workflow_solutions/getting_containers/#software-containers","text":"Containers help to manage software installations and all their dependencies in a single large image. These containers are a self-contained operating system along with any software the container creator added. Containers avoid software conflicts due to versioning as well as OS incompatibility and can be run on most, if not all, operating systems. The most common container engine is called Docker. Docker is an open-source platform for building, deploying, running, updating, and managing containers and has distributions for Linux, Windows, and Mac. Singularity is another common container engine specialized for use on HPC systems such as Cheaha where Docker cannot be used.","title":"Software Containers"},{"location":"workflow_solutions/getting_containers/#fantastic-containers-and-where-to-find-them","text":"Docker containers are available in https://hub.docker.com/ . This docker hub repository allows to share containers and use pre-existing docker images. It is often a good idea to search the Github repo for an application or pipeline to see if a container has already been provided by the authors.","title":"Fantastic Containers and Where to Find Them"},{"location":"workflow_solutions/getting_containers/#containers-on-cheaha","text":"Using containers on Cheaha bypasses the need to message support to install necessary software. Containers can be downloaded by any user into their personal space and used immediately without admin permission. as mentioned above, you will need to use Singularity containers on Cheaha. You can find all of the Singularity modules using the following command: module spider Singularity It's highly recommended to only use Singularity versions 3+.","title":"Containers on Cheaha"},{"location":"workflow_solutions/getting_containers/#pull-singularity-images","text":"Singularity can pull images from a variety of sources, including Dockerhub, and convert them to the proper format automatically. In order to download an image, use the pull subcommand followed by the output image name and the URI of the image. The general form of the command for pulling from Dockerhub is as follows: singularity pull <output.sif> docker://<account>/<image> [ :<version_tag> ] For example, if we wanted to pull the lolcow container : singularity pull lolcow.sif docker://godlovedc/lolcow We now have the lolcow.sif image we can run or share with other researchers. It's important to remember that containers are just independent files that can be moved, copied, or deleted the same as any other file.","title":"Pull Singularity Images"},{"location":"workflow_solutions/getting_containers/#running-singularity-images","text":"There are 3 ways to run Singularity images, all with their unique purposes and are as follows: singularity run : run a container using a default command set by the author. Generally, this will be used when a container encompasses a full pipeline controlled by a single command. The general form for this command is singularity run <image.sif> [options] where [options] are defined by the default command. You can use singularity run <image.sif> --help to see what those options are. singularity exec : run any command available in the container. This provides more flexibility than run and would be useful in the cases where a container has more modular components as opposed to a single control script. The general form for this would be singularity exec <image.sif> <command> [options] . You can add the --help option to see what a given command does and its inputs. singularity shell : allow interactive use of the container through the terminal. This changes your active environment to that in the container. You can traverse the container's directory tree and search for various files and commands as if it was a virtual machine. This is very useful for interactive development as well as investigation of a container's contents. The general form of the command is singularity shell <image.sif> . It's important to note that both run and exec enter the container as part of their execution and then exit back to the original shell environment afterwards whereas shell keeps you in the container until you either close the terminal or use the exit command. Important singularity shell is not executable via shell scripts. Any singularity commands in a batch script should be run or exec instead.","title":"Running Singularity Images"},{"location":"workflow_solutions/getting_containers/#singularity-paths","text":"By default, Singularity containers have limited access to the general filesystem. Containers get default access to the /home directory as well as the directory the container was run from. If you run the container from $HOME but try to access files in $USER_DATA , you will see an error. In order to give a container access to other directories, use the -B or --bind option when invoking the container. For instance, if I wanted to use run on a container that had an input option called -i and give the container access to a subfolder called my_data in a project space called UABRC , the singularity command would look like: singularity run --bind /data/project/UABRC/my_data image.sif -i /data/project/UABRC/my_data You can also alias the bind path to a shorter name and use it in the command. In that case, the bind option would look like --bind </directory_path>:</alias> . For example, if I was running a container and was giving the my_data directory as an input, I could alias it to /input_data and use it in the command like so: singularity run --bind /data/project/UABRC/my_data:/input_data image.sif -i /input_data These bind paths can be used in both exec and shell subcommands as well. Note Bind paths cannot grant access to folders and files your account does not have access to. For instance, you cannot use a container to access data in another user's account unless that user has explicitly given you the correct permissions via chmod or ACLs.","title":"Singularity Paths"},{"location":"workflow_solutions/getting_containers/#using-containers-on-uab-rc-cloud-cloudrcuabedu","text":"To access docker containers, install Docker in your system. To install docker desktop on your computer, follow this link: Docker Desktop Page .","title":"Using Containers on UAB RC Cloud (cloud.rc.uab.edu)"},{"location":"workflow_solutions/getting_containers/#docker-installation-on-uab-rc-cloud","text":"Following are the installation instructions to install Docker on UAB RC Cloud with Ubuntu operating system. Tested the installation on Ubuntu 20.04. Setting up UAB RC Cloud account can be found in UAB RC Cloud . sudo apt-get update sudo apt install docker.io","title":"Docker Installation on UAB RC Cloud"},{"location":"workflow_solutions/getting_containers/#using-a-docker-container-from-dockerhub","text":"We can start pulling a container named alpine from the Docker hub. alpine is a general-purpose Linux distribution. Look for the container alpine in the docker hub, copy the pull command, and paste it into your terminal. sudo docker pull alpine Once the image is pulled, you can verify if the image exists using the below command. Note that if you do not specify the tag/version of the container, the recent version is built, and the tag is listed as latest . sudo docker images If you prefer to pull a particular version of the alpine container, you need to mention the tag details in your pull command. You can see the available tags/versions of alpine from the Docker hub. To pull particular version of alpine container, use the below syntax. sudo docker pull container_name:tag Here the container_name is alpine , and the tag is 3.14 . sudo docker pull alpine:3.14 The existing image looks like,","title":"Using a Docker Container from DockerHub"},{"location":"workflow_solutions/getting_containers/#create-your-own-docker-container","text":"You can create your own Docker container, build it, and upload/share them in the Docker hub or UAB GitLab container registry. Let us take a synthetic python code and formulate the packages/dependencies required to build your software container. Below is a python script that requires packages, namely, numpy, scipy, and matplotlib. Next, the steps to create a Dockerfile is illustrated. Let us name this script python_test.py . import numpy as np import matplotlib import pylab import matplotlib.pylab as plt import scipy.integrate as integrate a = np.array ([ 0 , 10 , 20 , 30 , 40 ]) print ( a ) b = np.arange ( -5, 5 , 0 .5 ) print ( b ) t = np.arange ( 0 ,20.5,0.5 ) print ( t ) result = integrate.quad ( np.sin, 0 , np.pi ) print ( result ) plt.plot ([ 1 , 2 , 3 , 4 ] , [ 1 , 4 , 9 , 16 ]) plt.show () plt.savefig ( 'testing.png' )","title":"Create Your Own Docker Container"},{"location":"workflow_solutions/getting_containers/#create-a-dockerfile-that-has-miniconda-installed","text":"We require numpy, scipy, and matplotlib libraries to execute the above Python script. Following are the steps to create a specification file and build a container image. Create an empty directory miniconda . mkdir miniconda Create a Dockerfile within the miniconda directory with the following contents. The file name Dockerfile is case-sensitive. # You may start with a base image # Always use a specific tag like \"4.10.3\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda3:4.12.0 # Use RUN to execute commands inside the miniconda image RUN conda install -y numpy \">=1.16.5, <1.23.0\" # RUN multiple commands together # Last two lines are cleaning out the local repository and removing the state # information for installed package RUN apt-get update \\ && conda install -y scipy = 1 .7.3 \\ && conda install -y matplotlib = 3 .5.1 \\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is the specification file. It provides Docker with the software information, and versions, it needs to build our new container. See the Docker Container documentation for more information https://docs.docker.com/engine/reference/builder/ . In the Dockerfile, we start with an existing container continuumio/miniconda3:4.12.0 . This container is obtained from Dockerhub; here, continuumio is the producer, and the repo name is continuumio/miniconda3 . You may specify the required version from the Tag list. Here the tag/version is 4.12.0 . Also its a very good practice to specify the version of packages for numpy, scipy, and matplotlib for better reproducibility. Containers and Reproducibiliy Always include version numbers for Anaconda, package managers, software you are installing, and the dependencies for those software. Containers are not by nature scientifically reproducible, but if you include versions for as much software in the container as possible, they can be reproducible years later. To build your container, change the directory to miniconda and use the below syntax to build the Dockerfile . Here we use . to say \"current directory.\" This will only work if you are in the directory with the Dockerfile . sudo docker build -t repository_name:tag . Here the repository_name is py3-miniconda and the tag is 2022-08 . cd miniconda sudo docker build -t py3-miniconda:2022-08 . Note The . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside. Also, you may rename the repository_name and tag as you prefer. sudo docker images","title":"Create a Dockerfile that has Miniconda Installed"},{"location":"workflow_solutions/getting_containers/#running-the-built-miniconda-docker-container-interactively","text":"To run docker interactively and execute commands inside the container, use the below syntax. Here run executes the command in a new container, and -it starts an interactive shell inside the container. After executing this command, the command prompt will change and move into the bash shell. sudo docker run -it repository_name:tag /bin/bash To execute your container py3-miniconda interactively, run this command with the tag `2022-08'. sudo docker run -it py3-miniconda:2022-08 /bin/bash cd /opt/conda/bin/ The python executables to execute our synthetic python script are within the directory structure /opt/conda/bin .","title":"Running the Built Miniconda Docker Container Interactively"},{"location":"workflow_solutions/getting_containers/#mounting-data-onto-a-container","text":"Before we mount data onto a container, remember you initially created the python script python_test.py when creating your own container. Move python_test.py within miniconda directory. Now you have your miniconda/python_test.py outside the container. To access the files outside the container you should mount the file path along with the docker run command. To mount a host directory into your docker container, use the -v flag. sudo docker run -v /host/directory/:/container/directory -other-options So the command for our example will be, sudo docker run -v /home/ubuntu/:/home -it py3-miniconda:2022-08 /bin/sh Here we are mounting the $HOME directory /home/ubuntu from a host into containers' $HOME directory. Note that you may mount a particular directory according to your preference. The following shows the list of files in containers' $HOME directory with and without mounting. Before mounting, there are no files found within the $HOME directory. After mounting using -v flag, files show up within the $HOME directory. The highlighted miniconda is our working directory with python script. We can now execute the script, python_test.py using this command. python python_test.py More lessons on Docker can be found in this link: Introduction to Docker and Docker Documentation .","title":"Mounting Data Onto a Container"},{"location":"workflow_solutions/getting_containers/#sharing-containers-using-uab-gitlab-container-registry","text":"If you prefer to share your container with a particular team/group, then the UAB GitLab container registry is the best and most secure option. The following steps help you to create a container registry in UAB GitLab: Create a UAB Gitlab account following the guidelines from the UAB GitLab page . Create a new_project on UAB GitLab and click Package and Registries , and then go to Container Registry . Initially, the container registry looks empty because there are no container images in the registry. Note Copy these CLI commands for future reference. It contains commands (1) to login to your project UAB GitLab container registry (2) Add an image to the registry using the push/build command. We will use the push command as we already have the existing container in our system. Login to UAB GitLab Registry using your registry_name:ID . sudo docker login gitlab.rc.uab.edu:4567 Note The registry_name and ID shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing. Note: For securing concerns, use an access token to log in. Create an access token in UAB GitLab to push/pull the docker container in the container registry (Secure token and guidelines to follow are shown next). sudo docker login gitlab.rc.uab.edu:4567 -u username \u2013p access_token Creating an Access Token: From the UAB GitLab page, you can create an access token instead of using a password to log in to the UAB GitLab registry. Goto Edit profile -> Click Access Tokens . Then enter: Token name. :Suggestion: \"container\"_\"repository-name\" Expiry date. Suggestion: Default is 30 days. You can set your expiry date 3 months from the date you are making it. Under select scopes, check read and write registry (to push images to the registry) -> Then click create personal access token . Once you create the token, copy the new personal access token since it\u2019s a one-time step and hard to retrieve after a refresh. Use the personal access token for login. Warning Running docker login leads to a warning message that your password is stored unencrypted in /root/.docker/config.json (or) $HOME/.docker/config.json . To ignore this warning, follow the instructions in this Github page or the Docker credentials store page .","title":"Sharing Containers Using UAB GitLab Container Registry"},{"location":"workflow_solutions/getting_containers/#push-alpine-container-from-your-system-to-uab-gitlab-container-registry","text":"List the docker images on your local computer using the docker images command. An alpine image exists already on this computer. Your container will likely have a different name. sudo docker images Tag alpine to push into UAB GitLab registry. We need to have the UAB GitLab registry name to push. It will show the default command on the container registry page. Copy these commands for future reference. The tag is test here. sudo docker tag alpine:latest gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test You can see the tag test associated with the alpine image. sudo docker images The below first command is the syntax to push the Docker image to the UAB GitLab container registry from your computer. The second command is an example of pushing a Docker image to the UAB GitLab container registry from your computer. Note The registry_name,ID, and gitlab_group_name shown in this examples are for understanding and not meant for testing. Please use your GitLab registry name and ID for testing. sudo docker push gitlab_registry_name:ID/gitlab_group_name/project_name:tag sudo docker push gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test In your GitLab's page container registry, refresh to view the alpine container is pushed to the registry. Now lets pull the alpine container from GitLab's page container registry to your system. Before that, remove the previous image from the system, which already has a test tag, to avoid discrepancies. sudo docker rmi -f image_id In your GitLab's page container registry, copy the pull command from the test container registry, and use it to pull the docker container to your system. You can see the image is reflected in the image list. sudo docker pull gitlab.rc.uab.edu:4567/rc-data-science/build-and-push-container/alpinegitlab:test","title":"Push Alpine Container from your System to UAB GitLab Container Registry"},{"location":"workflow_solutions/git/","text":"Git \u00b6 Introductory Guides \u00b6 Above all else, git is a software and document collaboration tool. It may be used to record and manage changes to projects. Git allows users to commit changes to branches in a repository. Those changes and branches are recorded and may be shared with other users who have clones of the project. Git can be thought of as a combination of a timeline and a tree. Git is a timeline because it records change history on a timeline. It is a tree because changes can be meaningfully grouped into branches, each of which is an independent set of related commits. The branches can fork from or merge into each other as changes are recorded along the timeline. An example of part of the state of the repository for this documentation can be seen below (visualized using VSCode and https://github.com/mhutchie/vscode-git-graph ). Each row is a single commit, or set of changes. The vertical threads in the \"Graph\" column are branches which can be seen to fork and merge with one another. Each branch tip is labeled with the branch name under the \"Description\" column, along with a message describing the commit. The \"Date\" column shows when a commit was made, \"Author\" by whom, and \"Commit\" a partial hash that uniquely identifies the commit. Educational Resources \u00b6 The internet has many guides for using git. Rather than devise our own complete lesson plan for using git, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the git lesson, see https://swcarpentry.github.io/git-novice/ . Denser and more complete documentation is available at https://git-scm.com/doc . Reference \u00b6 Glossary \u00b6 A repository is the largest unit of information that git keeps track of. A useful model for a repository is a single coding or documentation project. A repository is composed of a collection of files, a working tree and index, and all of the change history associated with those things. Change history includes branches and commits. Repositories are decentralized, so that two people can work independently on parts of the same project and then merge their changes later. A local repository is a repository that is housed on the same machine you are working on. A remote repository or remote is a repository that is housed on a machine other than the one you are working on. Remotes are often housed on internet repository services like https://github.com and https://gitlab.org . UAB also maintains a private Gitlab instance at https://gitlab.rc.uab.edu . The working tree is the structure used to model repository contents and history. The index contains changes since the most recent commit. The staging area , a subset of the index, contains changes ready to be committed. A branch is a single thread of commits made to the working tree. Branches are independent of each other until they are merged. A repository can have one or many branches. Branches can be set up to track remote branches, facilitating pushing and pulling. A commit is a collection of changes that have been made to the working tree. Commits have associated messages. Repositories and branches are structured collections of commits. A repository may have many commits, distributed among branches. A branch tip is the most recent commit on a branch. The HEAD is the most recent commit on the currently selected branch. Cloning means creating a complete, but independent, copy of a repository and its history. Fetching means retrieving changes in from a remote repository. Merging is the process of incorporating changes from a source branch into a target branch. Pulling is fetching followed by merging. Pushing means communicating changes out to a remote repository. How should I configure git? \u00b6 Good practice for configuring git includes adding your name and email globally on your local machine so that you received proper attribution when making changes to repositories. To add your name and email enter the following two lines, replacing <name> and <email> as appropriate. git config --global user.name <name> git config --global user.email <email> More information is available at the official git webbook . Full documentation . How do I obtain git repositories? \u00b6 Obtaining repositories is how projects start, or how you might start working on someone else's repository. Substantially more information is available from the official git webbook Start a Local Repository (Init) \u00b6 Starting a local repository is also known as \"initializing\" a repository. It can be done with an empty folder, or a folder with existing code. Navigate to the folder and use the following command to initialize a repository. git init Forking \u00b6 Forking a repository is not a git concept, but a concept of remote repository hosting services like GitHub and GitLab. Forking is cloning a repository from another users account to your own account on GitHub or GitLab. GitHub fork documentation . GitLab fork documentation . Cloning \u00b6 Cloning a repository means making an independent copy of a remote repository on your local machine. To clone a repository you will need the remote URL where the repository is available. Navigate to the directory where you would like the repository located. Use the following to create a new subdirectory containing the clone. git clone <remote-url> Note Clones may be made from GitHub and GitLab using either https or ssh . To use ssh you will need to set up SSH Keys. Note Best practice for the remote URL is to ensure it ends with .git . While the URL for this documentation's repository is https://github.com/uabrc/uabrc.github.io you should instead use https://github.com/uabrc/uabrc.github.io.git when cloning. Using the .git suffix is optional for GitHub, but required for GitLab. Full documentation . How do I interact with remotes? \u00b6 A key part of git's usefulness is facilitating collaboration on code and documentation projects. Interacting with independent, remote copies of a repository is central to the purpose of git. Substantially more information is available at the official git webbook . Managing remotes \u00b6 Check what remotes are available using git remote -v Add a new remote with git remote add <remote-name> <remote-url> Change the url of an existing remote with git remote set-url <remote-name> <new-remote-url> Remove a remote with git remote remove <remote-name> Full documentation . Fetching and pulling \u00b6 Fetching changes from remote repositories without incorporating them: Check and download changes for all remotes using git fetch --all . Check and download changes for a remote using git fetch <remote-name> . Pull (fetch and merge) changes from a branch using git pull <remote-name> <remote-branch-name>. These changes are merged to the current HEAD , so be sure to checkout the correct branch first. Full documentation of git fetch . Full documentation of git pull . Pushing \u00b6 Push changes to a remote branch on a remote repository using git push <remote-name> <remote-branch-name> . Full documentation . How do I record changes? \u00b6 The key part of git's function is recording changes. Substantially more information is available at the official git webbook . Checking status of changes \u00b6 Use git status . It will show your current branch, staged files and unstaged files. Full documentation . Staging and committing changes \u00b6 Staging files is done with the git add command, committing files with git commit . git add new_file.txt git commit -m \"added new text file\" Full documentation of git add . Full documentation of git commit . Stop tracking files \u00b6 Use git rm <file> if the file has no changes. Despite the name this will not delete the file, only stop git from tracking any future changes. Specifically, it will stage a removal of the file from git tracking, which you will need to commit to stop tracking that file. If the file has changes: Use git rm --cached <file> to keep the changes Use git rm -f <file> to remove the changes Full documentation . Ignore files \u00b6 Place a file called .gitignore at the root directory of your repository. A modified form of Glob Syntax may be used, one pattern per line, to indicate files that should be ignored by git. Any files or directories matching a line in the .gitignore file will not be tracked, unless the file has already been committed to the working tree. * : a wildcard meaning any number of any characters except for a slash ** : a wildcard for matching directories ! : negates a pattern, causing inclusion instead of exclusion The last pattern in the file takes precedence # ignores all files with extension .txt *.txt # includes file despite previous rule !included.txt # ignores all directories called test **/test/ # ignores all directories called build in site site/**/build # ignores all files in ouptut, but not in subdirectories output/* # recursively ignores all files in output, including subdirectories output/** Full documentation . How do I manage branches? \u00b6 Listing branches \u00b6 To list local branches use git branch . To list remote branches use git branch -r . Checking out existing branches \u00b6 To checkout an existing local branch use git checkout <branch-name> . To checkout and track a remote branch use git checkout --track <remote-name>/<branch-name> . Creating new branches \u00b6 To create a new local branch use git branch <branch-name> . To create and checkout a new local branch use git checkout -b <branch-name> . Note Branches are always created from the HEAD. Deleting branches \u00b6 To delete a local branch use git branch --delete <branch-name> . Warning Deleting remote branches can be destructive if active pull requests depend on them. Be sure you are deleting the correct remote branch. To delete a remote branch use git push <remote-name> --delete <branch-name> . Merging branches \u00b6 To merge branch A into branch B , first select branch B using git checkout B , then merge A into B using git merge A . Some merges may cause conflicts. Conflict resolution is part art, part science, and beyond the scope of this document. Several strategies exist to minimize conflict frequency and scope. The general gist requires making smaller but more frequent commits of complete units. More information on merging is available at the official git webbook . How do I reset changes? \u00b6 Danger git reset can be highly destructive if used improperly. If you are unsure then please do not use git reset --hard . Use git reset <mode> <commit> . There are several modes: --hard is destructive changes since <commit> in the working tree and index are discarded all untracked files \"in the way\" of those changes are deleted --mixed (default) is non-destructive changes since <commit> in the index are tracked but not staged --soft is non-destructive changes since <commit> in the index are staged If <commit> is left empty then the most recent commit on the working branch is used. If you wish to specify a commit, then provide enough characters at the start of the commit hash (7 characters is usually enough for unique identification). Danger Using git reset --hard is destructive. All changes since the last commit are discarded and are most likely lost. Use git reset --mixed (default) or git reset --soft instead. More information is available at the official git webbook . Full documentation . How do I tag a commit? \u00b6 Use git tag . Tags are useful for tracking software versions or milestones in the history. There are two types, lightweight and annotated. Here we only discuss lightweight tags. Tag HEAD: git tag <tag-label> Tag a specific commit by hash: git tag <tag-label> <commit-hash> Push tag to remote: git push <remote-name> <tag-label> Delete tag: git tag -d <tag-label> Delete tag from remote: git push origin --delete <tag-label> More information is available at the official git webbook . Full documentation . Help! Something went wrong \u00b6 Stop. Breathe. Don't panic. It's likely that your changes are still cached by git. Don't make any additional changes or run any git commits as this may cause cached files to be deleted. If you have some experience with git, then chances are good you can recover your lost changes. If you do not have much experience with git or do not feel confident recovering, then please contact Support . If you wish to proceed without assistance, then please visit the following two pages depending on your situation. I need to undo a change that I've made \u00b6 Warning Do not use git reset --hard if you are not sure of what you are doing! Please visit: https://wwarriner.github.io/gitfix/ , read each card carefully, and answer the questions until you arrive at a solution. Do not use git reset --hard unless you are sure of what that command will do. More information is available at the official git webbook . I've lost changes I made \u00b6 The changes may be lost permanently, or they may be partially recoverable. Git has a garbage collector for old versions of files that is emptied periodically as new commands are run. If you have not run any commands since losing changes, the old versions should still be available. To recover the changes: In a terminal, navigate to the repository directory. Use the command git fsck --lost-found . Navigate to the subdirectory .git/lost-found/other . Highly recommended : make a backup of the .git/lost-found/other folder before proceeding. The recovery will only be partial. File content should all be recoverable. However, directory structure and file names will be lost. This solution will require considerable manual or scripted work to identify files and restore directory structure and file names.","title":"Using Git"},{"location":"workflow_solutions/git/#git","text":"","title":"Git"},{"location":"workflow_solutions/git/#introductory-guides","text":"Above all else, git is a software and document collaboration tool. It may be used to record and manage changes to projects. Git allows users to commit changes to branches in a repository. Those changes and branches are recorded and may be shared with other users who have clones of the project. Git can be thought of as a combination of a timeline and a tree. Git is a timeline because it records change history on a timeline. It is a tree because changes can be meaningfully grouped into branches, each of which is an independent set of related commits. The branches can fork from or merge into each other as changes are recorded along the timeline. An example of part of the state of the repository for this documentation can be seen below (visualized using VSCode and https://github.com/mhutchie/vscode-git-graph ). Each row is a single commit, or set of changes. The vertical threads in the \"Graph\" column are branches which can be seen to fork and merge with one another. Each branch tip is labeled with the branch name under the \"Description\" column, along with a message describing the commit. The \"Date\" column shows when a commit was made, \"Author\" by whom, and \"Commit\" a partial hash that uniquely identifies the commit.","title":"Introductory Guides"},{"location":"workflow_solutions/git/#educational-resources","text":"The internet has many guides for using git. Rather than devise our own complete lesson plan for using git, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the git lesson, see https://swcarpentry.github.io/git-novice/ . Denser and more complete documentation is available at https://git-scm.com/doc .","title":"Educational Resources"},{"location":"workflow_solutions/git/#reference","text":"","title":"Reference"},{"location":"workflow_solutions/git/#glossary","text":"A repository is the largest unit of information that git keeps track of. A useful model for a repository is a single coding or documentation project. A repository is composed of a collection of files, a working tree and index, and all of the change history associated with those things. Change history includes branches and commits. Repositories are decentralized, so that two people can work independently on parts of the same project and then merge their changes later. A local repository is a repository that is housed on the same machine you are working on. A remote repository or remote is a repository that is housed on a machine other than the one you are working on. Remotes are often housed on internet repository services like https://github.com and https://gitlab.org . UAB also maintains a private Gitlab instance at https://gitlab.rc.uab.edu . The working tree is the structure used to model repository contents and history. The index contains changes since the most recent commit. The staging area , a subset of the index, contains changes ready to be committed. A branch is a single thread of commits made to the working tree. Branches are independent of each other until they are merged. A repository can have one or many branches. Branches can be set up to track remote branches, facilitating pushing and pulling. A commit is a collection of changes that have been made to the working tree. Commits have associated messages. Repositories and branches are structured collections of commits. A repository may have many commits, distributed among branches. A branch tip is the most recent commit on a branch. The HEAD is the most recent commit on the currently selected branch. Cloning means creating a complete, but independent, copy of a repository and its history. Fetching means retrieving changes in from a remote repository. Merging is the process of incorporating changes from a source branch into a target branch. Pulling is fetching followed by merging. Pushing means communicating changes out to a remote repository.","title":"Glossary"},{"location":"workflow_solutions/git/#how-should-i-configure-git","text":"Good practice for configuring git includes adding your name and email globally on your local machine so that you received proper attribution when making changes to repositories. To add your name and email enter the following two lines, replacing <name> and <email> as appropriate. git config --global user.name <name> git config --global user.email <email> More information is available at the official git webbook . Full documentation .","title":"How should I configure git?"},{"location":"workflow_solutions/git/#how-do-i-obtain-git-repositories","text":"Obtaining repositories is how projects start, or how you might start working on someone else's repository. Substantially more information is available from the official git webbook","title":"How do I obtain git repositories?"},{"location":"workflow_solutions/git/#start-a-local-repository-init","text":"Starting a local repository is also known as \"initializing\" a repository. It can be done with an empty folder, or a folder with existing code. Navigate to the folder and use the following command to initialize a repository. git init","title":"Start a Local Repository (Init)"},{"location":"workflow_solutions/git/#forking","text":"Forking a repository is not a git concept, but a concept of remote repository hosting services like GitHub and GitLab. Forking is cloning a repository from another users account to your own account on GitHub or GitLab. GitHub fork documentation . GitLab fork documentation .","title":"Forking"},{"location":"workflow_solutions/git/#cloning","text":"Cloning a repository means making an independent copy of a remote repository on your local machine. To clone a repository you will need the remote URL where the repository is available. Navigate to the directory where you would like the repository located. Use the following to create a new subdirectory containing the clone. git clone <remote-url> Note Clones may be made from GitHub and GitLab using either https or ssh . To use ssh you will need to set up SSH Keys. Note Best practice for the remote URL is to ensure it ends with .git . While the URL for this documentation's repository is https://github.com/uabrc/uabrc.github.io you should instead use https://github.com/uabrc/uabrc.github.io.git when cloning. Using the .git suffix is optional for GitHub, but required for GitLab. Full documentation .","title":"Cloning"},{"location":"workflow_solutions/git/#how-do-i-interact-with-remotes","text":"A key part of git's usefulness is facilitating collaboration on code and documentation projects. Interacting with independent, remote copies of a repository is central to the purpose of git. Substantially more information is available at the official git webbook .","title":"How do I interact with remotes?"},{"location":"workflow_solutions/git/#managing-remotes","text":"Check what remotes are available using git remote -v Add a new remote with git remote add <remote-name> <remote-url> Change the url of an existing remote with git remote set-url <remote-name> <new-remote-url> Remove a remote with git remote remove <remote-name> Full documentation .","title":"Managing remotes"},{"location":"workflow_solutions/git/#fetching-and-pulling","text":"Fetching changes from remote repositories without incorporating them: Check and download changes for all remotes using git fetch --all . Check and download changes for a remote using git fetch <remote-name> . Pull (fetch and merge) changes from a branch using git pull <remote-name> <remote-branch-name>. These changes are merged to the current HEAD , so be sure to checkout the correct branch first. Full documentation of git fetch . Full documentation of git pull .","title":"Fetching and pulling"},{"location":"workflow_solutions/git/#pushing","text":"Push changes to a remote branch on a remote repository using git push <remote-name> <remote-branch-name> . Full documentation .","title":"Pushing"},{"location":"workflow_solutions/git/#how-do-i-record-changes","text":"The key part of git's function is recording changes. Substantially more information is available at the official git webbook .","title":"How do I record changes?"},{"location":"workflow_solutions/git/#checking-status-of-changes","text":"Use git status . It will show your current branch, staged files and unstaged files. Full documentation .","title":"Checking status of changes"},{"location":"workflow_solutions/git/#staging-and-committing-changes","text":"Staging files is done with the git add command, committing files with git commit . git add new_file.txt git commit -m \"added new text file\" Full documentation of git add . Full documentation of git commit .","title":"Staging and committing changes"},{"location":"workflow_solutions/git/#stop-tracking-files","text":"Use git rm <file> if the file has no changes. Despite the name this will not delete the file, only stop git from tracking any future changes. Specifically, it will stage a removal of the file from git tracking, which you will need to commit to stop tracking that file. If the file has changes: Use git rm --cached <file> to keep the changes Use git rm -f <file> to remove the changes Full documentation .","title":"Stop tracking files"},{"location":"workflow_solutions/git/#ignore-files","text":"Place a file called .gitignore at the root directory of your repository. A modified form of Glob Syntax may be used, one pattern per line, to indicate files that should be ignored by git. Any files or directories matching a line in the .gitignore file will not be tracked, unless the file has already been committed to the working tree. * : a wildcard meaning any number of any characters except for a slash ** : a wildcard for matching directories ! : negates a pattern, causing inclusion instead of exclusion The last pattern in the file takes precedence # ignores all files with extension .txt *.txt # includes file despite previous rule !included.txt # ignores all directories called test **/test/ # ignores all directories called build in site site/**/build # ignores all files in ouptut, but not in subdirectories output/* # recursively ignores all files in output, including subdirectories output/** Full documentation .","title":"Ignore files"},{"location":"workflow_solutions/git/#how-do-i-manage-branches","text":"","title":"How do I manage branches?"},{"location":"workflow_solutions/git/#listing-branches","text":"To list local branches use git branch . To list remote branches use git branch -r .","title":"Listing branches"},{"location":"workflow_solutions/git/#checking-out-existing-branches","text":"To checkout an existing local branch use git checkout <branch-name> . To checkout and track a remote branch use git checkout --track <remote-name>/<branch-name> .","title":"Checking out existing branches"},{"location":"workflow_solutions/git/#creating-new-branches","text":"To create a new local branch use git branch <branch-name> . To create and checkout a new local branch use git checkout -b <branch-name> . Note Branches are always created from the HEAD.","title":"Creating new branches"},{"location":"workflow_solutions/git/#deleting-branches","text":"To delete a local branch use git branch --delete <branch-name> . Warning Deleting remote branches can be destructive if active pull requests depend on them. Be sure you are deleting the correct remote branch. To delete a remote branch use git push <remote-name> --delete <branch-name> .","title":"Deleting branches"},{"location":"workflow_solutions/git/#merging-branches","text":"To merge branch A into branch B , first select branch B using git checkout B , then merge A into B using git merge A . Some merges may cause conflicts. Conflict resolution is part art, part science, and beyond the scope of this document. Several strategies exist to minimize conflict frequency and scope. The general gist requires making smaller but more frequent commits of complete units. More information on merging is available at the official git webbook .","title":"Merging branches"},{"location":"workflow_solutions/git/#how-do-i-reset-changes","text":"Danger git reset can be highly destructive if used improperly. If you are unsure then please do not use git reset --hard . Use git reset <mode> <commit> . There are several modes: --hard is destructive changes since <commit> in the working tree and index are discarded all untracked files \"in the way\" of those changes are deleted --mixed (default) is non-destructive changes since <commit> in the index are tracked but not staged --soft is non-destructive changes since <commit> in the index are staged If <commit> is left empty then the most recent commit on the working branch is used. If you wish to specify a commit, then provide enough characters at the start of the commit hash (7 characters is usually enough for unique identification). Danger Using git reset --hard is destructive. All changes since the last commit are discarded and are most likely lost. Use git reset --mixed (default) or git reset --soft instead. More information is available at the official git webbook . Full documentation .","title":"How do I reset changes?"},{"location":"workflow_solutions/git/#how-do-i-tag-a-commit","text":"Use git tag . Tags are useful for tracking software versions or milestones in the history. There are two types, lightweight and annotated. Here we only discuss lightweight tags. Tag HEAD: git tag <tag-label> Tag a specific commit by hash: git tag <tag-label> <commit-hash> Push tag to remote: git push <remote-name> <tag-label> Delete tag: git tag -d <tag-label> Delete tag from remote: git push origin --delete <tag-label> More information is available at the official git webbook . Full documentation .","title":"How do I tag a commit?"},{"location":"workflow_solutions/git/#help-something-went-wrong","text":"Stop. Breathe. Don't panic. It's likely that your changes are still cached by git. Don't make any additional changes or run any git commits as this may cause cached files to be deleted. If you have some experience with git, then chances are good you can recover your lost changes. If you do not have much experience with git or do not feel confident recovering, then please contact Support . If you wish to proceed without assistance, then please visit the following two pages depending on your situation.","title":"Help! Something went wrong"},{"location":"workflow_solutions/git/#i-need-to-undo-a-change-that-ive-made","text":"Warning Do not use git reset --hard if you are not sure of what you are doing! Please visit: https://wwarriner.github.io/gitfix/ , read each card carefully, and answer the questions until you arrive at a solution. Do not use git reset --hard unless you are sure of what that command will do. More information is available at the official git webbook .","title":"I need to undo a change that I've made"},{"location":"workflow_solutions/git/#ive-lost-changes-i-made","text":"The changes may be lost permanently, or they may be partially recoverable. Git has a garbage collector for old versions of files that is emptied periodically as new commands are run. If you have not run any commands since losing changes, the old versions should still be available. To recover the changes: In a terminal, navigate to the repository directory. Use the command git fsck --lost-found . Navigate to the subdirectory .git/lost-found/other . Highly recommended : make a backup of the .git/lost-found/other folder before proceeding. The recovery will only be partial. File content should all be recoverable. However, directory structure and file names will be lost. This solution will require considerable manual or scripted work to identify files and restore directory structure and file names.","title":"I've lost changes I made"},{"location":"workflow_solutions/git_collaboration/","text":"How to Collaborate with Git \u00b6 Git is a powerful tool for version control of software and other plain-text information. However, Git alone is not ideal for enabling and facilitating collaboration between many users working on the same research software project. Be sure to check the Important Note on Terms if you aren't familiar with Git. If you are here because you need to know how to get software from either instance, please see Obtaining Software below. If you are here because you need a place to collaborate with others on a software project, please see Collaborating below. Important Note on Terms \u00b6 On most pages in this documentation, we use \"local\" to refer to your laptop or desktop computer, and \"remote\" to refer to Cheaha or a Cloud.rc virtual machine. When dealing with Git and repository hosting services like GitHub and GitLab we use remote to refer to repositories that are on a repository hosting site like GitHub or GitLab, and local to refer to repositories that are not on a repository hosting site. To summarize: Git, GitHub, GitLab context local - the repository on a computer where you work on your code (laptop, desktop, Cheaha) remote - a remote server or service that stores code (github, gitlab, etc) Cheaha and Cloud.rc context local - the machine (laptop, desktop) you use to access Cheaha or the Cloud.rc VM remote - Cheaha or the Cloud.rc VM For Obtaining Software \u00b6 Cloning from GitHub \u00b6 To do anything with GitHub, you will first need to navigate to their website https://github.com and create an account . To clone a repository, be sure you have the repository URL. Then, using git at a terminal, clone the repository using whatever settings are appropriate. GitHub repository pages look something like the page for this documentation, shown below. You may also use the \"Code\" button on the page to see instructions for cloning the repository. More in-depth instructions, including for SSH cloning, are provided at the official documentation . Cloning from GitLab \u00b6 To do anything with our GitLab instance, you will first need to create an account. Please see our GitLab Account Management page . To clone a repository, be sure you have the repository URL. Then, using git at a terminal, clone the repository using whatever settings are appropriate. Be sure to append .git to the end of the repository or the clone will note be successful. For example, if the URL is https://gitlab.rc.uab.edu/user/repository then you will clone https://gitlab.rc.uab.edu/user/repository.git . GitLab repository pages look like the example shown below. You may also use the \"Clone\" button on the page to see instructions for cloning the repository. More in-depth instructions, including for SSH cloning, are provided at the official documentation . For Collaborating \u00b6 GitHub and GitLab can both be used for software project management, and have helpful tools to facilitate group collaboration within projects and across multiple projects. Both services use organizations to manage projects across a team of people: GitHub docs page , GitLab docs page . Within a GitHub organization, people and repositories can be arranged into teams . GitLab allows arrangement of people and repositories with projects . An important feature, used extensively for this documentation's GitHub repository, is the issue tracker. Both GitHub and GitLab have per-repository issue trackers. Collaborators can create and manage issues, label them, and resolve them. How do I Choose Between GitHub and GitLab? \u00b6 Want to collaborate publicly and outside UAB? Consider using GitHub. Want your project private or internal to UAB? Consider using our GitLab instance. It is possible to collaborate publicly using GitLab, but there may be additional challenges. While external collaborators can see a public GitLab repository on our instance, they can't make any changes or create issues without a XIAS Account . It is possible to collaborate privately using GitHub with no additional hurdles, but if your project contains sensitive or protected information of any kinds, it should not be posted to GitHub, even in private repositories. Please consult with us via Support before Good Practice for Organizing a Lab Space \u00b6 Below is a bulleted list of good practices for organizing a lab space. Each bullet is followed by links to relevant GitHub and GitLab documentation pages, as appropriate. Have an organization for your lab space. GitHub GitLab The organization should have its PI or PIs as owners, i.e., the owner role. GitHub GitLab Other trusted individuals can be made administrators as needed, to delegate important and sensitive tasks that require elevated permissions. For each software project, create a repository within your organization. GitHub , GitLab By default, organization members will have access at their assigned role level. These can be changed by managing roles and using teams effectively, if needed. For smaller labs this is often not necessary. The created repository is the central one for the organization and should not be changed directly. For every individual, including owners and admins, work should be performed on a personal fork of the repository and then merged by submitting pull/merge requests. Forks are copies of repositories made as a snapshot at the moment they are created. From that point on they are independent repositories with some features to facilitate collaborative workflows. GitHub GitLab Pull/merge requests allow individuals to contribute to a central repository. They allow reviewers to check the changes to ensure code quality, and to provide reviews or request changes. They are the primary means of controlling how code changes over time, and who is allowed to make those changes. GitHub GitLab See the Fork-Pull/Merge Request Workflow Section for more details on this valuable method of change management. The Fork-Pull/Merge Request Workflow \u00b6 The Fork-Pull/Merge Request workflow is a central concept to effective collaboration on individual repositories. It allows code owners and admins to effectively control how code changes, while giving accountability and credit to code maintainers and programmers. Every person working on a project has an effective means of working independently while being able to pull their changes together in a central location. It also neatly ties into issue tracking, which is discussed in the Issue Tracking Section . The workflow assumes a central repository already exists within an organization on either GitHub or GitLab. The workflow is written from the point of view of a new programmer who wants to work on the repository. The programmer must have a local machine where they will do their work and it must have Git installed. One-time setup Fork an individual repository (downstream) from the central organization repository (upstream). GitHub GitLab Clone the downstream fork to the local machine where the programming will happen. Workflow Decide on a set of changes to make. Good practice is only working on one conceptual unit at a time. One feature, one bug fix, or one documentation page. Prefer fixing bugs before adding features. Synchronize your downstream fork with the upstream fork to minimize risk of merge conflicts. GitHub GitLab Pull the downstream fork main branch to your local clone main branch. Create a working branch for intended changes. Give it a short, descriptive name like feature-add-button or fix-broken-link . Checkout the working branch . Make changes to the code on the local machine using your preferred editor. Make small units of change at a time, try not to commit too much, but make sure your changes don't break the code. There is an art to this that comes with practice, but don't be afraid of trying. Commit those changes to the working branch. Keep making changes and committing until the set of changes is complete. When all needed changes have been made, push the working branch to your fork. Create a pull/merge request from the downstream working branch to the upstream main branch. GitHub GitLab Wait for reviews, make needed changes, and hopefully merging of your request. Sometimes merging will be blocked because of a merge conflict. One programmer may make changes to code being worked on by another, and the two changes come into conflict. If this occurs, below are some steps that may help resolve the issue. In some cases, conflict resolution is straightforward, but in other cases thought will be necessary to disentangle what code should be kept, what should be discarded, and what should be modified. The downstream programmer should try synchronizing their fork, pulling it to their local main branch, and [merging] the main branch into their working branch. The conflict may still occur on their local machine, but they will be able to more easily see and test the effects of various conflict resolution attempts. Use a three-way diff program or editor which will let you see both sets of conflicting code, and facilitate making changes and selections. VSCode has a built-in three-way merge editor. Be sure everyone is using the same formatting rules in their editors. Sometimes spurious conflicts can occur as a result of inconsistent formatting. To minimize risk of conflict, don't have more than one programmer work on the same section of code if possible. Effective Issue Tracking \u00b6 Effective use of issue tracking can greatly reduce cognitive load and simplify code management. It gives a central location where users and maintainers can report bugs, make feature requests, and ask for clarifications on usage and documentation. These issues are tracked over time, can be labeled and organized, and closed and reopened. GitHub GitLab The typical issue lifecycle, at a high level, is something like below. Create an issue. GitHub GitLab Ask for clarifications and discuss as needed. Use the Fork-Pull/Merge Request Workflow to resolve the issue. In the Pull Request description, put the text Fixes #... where ... should be replaced by the issue's number. When the request is merged, the issue will automatically be linked to the request and closed. Common Scenarios \u00b6 Uploading an Existing Code Folder \u00b6 The process for this has a few intricate steps that may be unfamiliar even to regular users of git, and has a few pitfalls. Use git init in the top-level code folder on the local machine, if it is not already a git repository. If it already is a repository, be sure the primary branch is called main . Use git branch -m <oldname> main . Create a repository on the remote server GitHub , GitLab Use git remote add origin <url> to add the remote URL to the local repository with the name origin . Verify the URL is correct with git remote -v . Fix it with git remote set-url origin <url> if needed. Checkout the main branch without git checkout main . Use git pull origin main --allow-unrelated-histories to combine the main branches of the remote and local repository, within your local repository. Use git push origin main to push the combined histories to the remote repository. Be sure to verify the repository looks good at the GitHub/GitLab repository page (depending on which you used). Note --allow-unrelated-histories is necessary because Git considers the remote repository to be a completely distinct entity from the local repository. Their histories are unrelated. HTTPS vs SSH Access \u00b6 For most beginners using Git, GitHub and GitLab, HTTPS (hypertext transfer protocol secure) is probably a sufficient method for accessing in early stages. It is the default mode of accessing GitHub and GitLab when using Git at the command line. HTTPS is less secure than SSH (secure shell). We recommend learning to use SSH as soon as possible to minimize security risks. Below are links to GitHub and GitLab documentation for using SSH. GitHub SSH documentation GitLab SSH documentation","title":"Software Collaboration with GitHub and GitLab"},{"location":"workflow_solutions/git_collaboration/#how-to-collaborate-with-git","text":"Git is a powerful tool for version control of software and other plain-text information. However, Git alone is not ideal for enabling and facilitating collaboration between many users working on the same research software project. Be sure to check the Important Note on Terms if you aren't familiar with Git. If you are here because you need to know how to get software from either instance, please see Obtaining Software below. If you are here because you need a place to collaborate with others on a software project, please see Collaborating below.","title":"How to Collaborate with Git"},{"location":"workflow_solutions/git_collaboration/#important-note-on-terms","text":"On most pages in this documentation, we use \"local\" to refer to your laptop or desktop computer, and \"remote\" to refer to Cheaha or a Cloud.rc virtual machine. When dealing with Git and repository hosting services like GitHub and GitLab we use remote to refer to repositories that are on a repository hosting site like GitHub or GitLab, and local to refer to repositories that are not on a repository hosting site. To summarize: Git, GitHub, GitLab context local - the repository on a computer where you work on your code (laptop, desktop, Cheaha) remote - a remote server or service that stores code (github, gitlab, etc) Cheaha and Cloud.rc context local - the machine (laptop, desktop) you use to access Cheaha or the Cloud.rc VM remote - Cheaha or the Cloud.rc VM","title":"Important Note on Terms"},{"location":"workflow_solutions/git_collaboration/#for-obtaining-software","text":"","title":"For Obtaining Software"},{"location":"workflow_solutions/git_collaboration/#cloning-from-github","text":"To do anything with GitHub, you will first need to navigate to their website https://github.com and create an account . To clone a repository, be sure you have the repository URL. Then, using git at a terminal, clone the repository using whatever settings are appropriate. GitHub repository pages look something like the page for this documentation, shown below. You may also use the \"Code\" button on the page to see instructions for cloning the repository. More in-depth instructions, including for SSH cloning, are provided at the official documentation .","title":"Cloning from GitHub"},{"location":"workflow_solutions/git_collaboration/#cloning-from-gitlab","text":"To do anything with our GitLab instance, you will first need to create an account. Please see our GitLab Account Management page . To clone a repository, be sure you have the repository URL. Then, using git at a terminal, clone the repository using whatever settings are appropriate. Be sure to append .git to the end of the repository or the clone will note be successful. For example, if the URL is https://gitlab.rc.uab.edu/user/repository then you will clone https://gitlab.rc.uab.edu/user/repository.git . GitLab repository pages look like the example shown below. You may also use the \"Clone\" button on the page to see instructions for cloning the repository. More in-depth instructions, including for SSH cloning, are provided at the official documentation .","title":"Cloning from GitLab"},{"location":"workflow_solutions/git_collaboration/#for-collaborating","text":"GitHub and GitLab can both be used for software project management, and have helpful tools to facilitate group collaboration within projects and across multiple projects. Both services use organizations to manage projects across a team of people: GitHub docs page , GitLab docs page . Within a GitHub organization, people and repositories can be arranged into teams . GitLab allows arrangement of people and repositories with projects . An important feature, used extensively for this documentation's GitHub repository, is the issue tracker. Both GitHub and GitLab have per-repository issue trackers. Collaborators can create and manage issues, label them, and resolve them.","title":"For Collaborating"},{"location":"workflow_solutions/git_collaboration/#how-do-i-choose-between-github-and-gitlab","text":"Want to collaborate publicly and outside UAB? Consider using GitHub. Want your project private or internal to UAB? Consider using our GitLab instance. It is possible to collaborate publicly using GitLab, but there may be additional challenges. While external collaborators can see a public GitLab repository on our instance, they can't make any changes or create issues without a XIAS Account . It is possible to collaborate privately using GitHub with no additional hurdles, but if your project contains sensitive or protected information of any kinds, it should not be posted to GitHub, even in private repositories. Please consult with us via Support before","title":"How do I Choose Between GitHub and GitLab?"},{"location":"workflow_solutions/git_collaboration/#good-practice-for-organizing-a-lab-space","text":"Below is a bulleted list of good practices for organizing a lab space. Each bullet is followed by links to relevant GitHub and GitLab documentation pages, as appropriate. Have an organization for your lab space. GitHub GitLab The organization should have its PI or PIs as owners, i.e., the owner role. GitHub GitLab Other trusted individuals can be made administrators as needed, to delegate important and sensitive tasks that require elevated permissions. For each software project, create a repository within your organization. GitHub , GitLab By default, organization members will have access at their assigned role level. These can be changed by managing roles and using teams effectively, if needed. For smaller labs this is often not necessary. The created repository is the central one for the organization and should not be changed directly. For every individual, including owners and admins, work should be performed on a personal fork of the repository and then merged by submitting pull/merge requests. Forks are copies of repositories made as a snapshot at the moment they are created. From that point on they are independent repositories with some features to facilitate collaborative workflows. GitHub GitLab Pull/merge requests allow individuals to contribute to a central repository. They allow reviewers to check the changes to ensure code quality, and to provide reviews or request changes. They are the primary means of controlling how code changes over time, and who is allowed to make those changes. GitHub GitLab See the Fork-Pull/Merge Request Workflow Section for more details on this valuable method of change management.","title":"Good Practice for Organizing a Lab Space"},{"location":"workflow_solutions/git_collaboration/#the-fork-pullmerge-request-workflow","text":"The Fork-Pull/Merge Request workflow is a central concept to effective collaboration on individual repositories. It allows code owners and admins to effectively control how code changes, while giving accountability and credit to code maintainers and programmers. Every person working on a project has an effective means of working independently while being able to pull their changes together in a central location. It also neatly ties into issue tracking, which is discussed in the Issue Tracking Section . The workflow assumes a central repository already exists within an organization on either GitHub or GitLab. The workflow is written from the point of view of a new programmer who wants to work on the repository. The programmer must have a local machine where they will do their work and it must have Git installed. One-time setup Fork an individual repository (downstream) from the central organization repository (upstream). GitHub GitLab Clone the downstream fork to the local machine where the programming will happen. Workflow Decide on a set of changes to make. Good practice is only working on one conceptual unit at a time. One feature, one bug fix, or one documentation page. Prefer fixing bugs before adding features. Synchronize your downstream fork with the upstream fork to minimize risk of merge conflicts. GitHub GitLab Pull the downstream fork main branch to your local clone main branch. Create a working branch for intended changes. Give it a short, descriptive name like feature-add-button or fix-broken-link . Checkout the working branch . Make changes to the code on the local machine using your preferred editor. Make small units of change at a time, try not to commit too much, but make sure your changes don't break the code. There is an art to this that comes with practice, but don't be afraid of trying. Commit those changes to the working branch. Keep making changes and committing until the set of changes is complete. When all needed changes have been made, push the working branch to your fork. Create a pull/merge request from the downstream working branch to the upstream main branch. GitHub GitLab Wait for reviews, make needed changes, and hopefully merging of your request. Sometimes merging will be blocked because of a merge conflict. One programmer may make changes to code being worked on by another, and the two changes come into conflict. If this occurs, below are some steps that may help resolve the issue. In some cases, conflict resolution is straightforward, but in other cases thought will be necessary to disentangle what code should be kept, what should be discarded, and what should be modified. The downstream programmer should try synchronizing their fork, pulling it to their local main branch, and [merging] the main branch into their working branch. The conflict may still occur on their local machine, but they will be able to more easily see and test the effects of various conflict resolution attempts. Use a three-way diff program or editor which will let you see both sets of conflicting code, and facilitate making changes and selections. VSCode has a built-in three-way merge editor. Be sure everyone is using the same formatting rules in their editors. Sometimes spurious conflicts can occur as a result of inconsistent formatting. To minimize risk of conflict, don't have more than one programmer work on the same section of code if possible.","title":"The Fork-Pull/Merge Request Workflow"},{"location":"workflow_solutions/git_collaboration/#effective-issue-tracking","text":"Effective use of issue tracking can greatly reduce cognitive load and simplify code management. It gives a central location where users and maintainers can report bugs, make feature requests, and ask for clarifications on usage and documentation. These issues are tracked over time, can be labeled and organized, and closed and reopened. GitHub GitLab The typical issue lifecycle, at a high level, is something like below. Create an issue. GitHub GitLab Ask for clarifications and discuss as needed. Use the Fork-Pull/Merge Request Workflow to resolve the issue. In the Pull Request description, put the text Fixes #... where ... should be replaced by the issue's number. When the request is merged, the issue will automatically be linked to the request and closed.","title":"Effective Issue Tracking"},{"location":"workflow_solutions/git_collaboration/#common-scenarios","text":"","title":"Common Scenarios"},{"location":"workflow_solutions/git_collaboration/#uploading-an-existing-code-folder","text":"The process for this has a few intricate steps that may be unfamiliar even to regular users of git, and has a few pitfalls. Use git init in the top-level code folder on the local machine, if it is not already a git repository. If it already is a repository, be sure the primary branch is called main . Use git branch -m <oldname> main . Create a repository on the remote server GitHub , GitLab Use git remote add origin <url> to add the remote URL to the local repository with the name origin . Verify the URL is correct with git remote -v . Fix it with git remote set-url origin <url> if needed. Checkout the main branch without git checkout main . Use git pull origin main --allow-unrelated-histories to combine the main branches of the remote and local repository, within your local repository. Use git push origin main to push the combined histories to the remote repository. Be sure to verify the repository looks good at the GitHub/GitLab repository page (depending on which you used). Note --allow-unrelated-histories is necessary because Git considers the remote repository to be a completely distinct entity from the local repository. Their histories are unrelated.","title":"Uploading an Existing Code Folder"},{"location":"workflow_solutions/git_collaboration/#https-vs-ssh-access","text":"For most beginners using Git, GitHub and GitLab, HTTPS (hypertext transfer protocol secure) is probably a sufficient method for accessing in early stages. It is the default mode of accessing GitHub and GitLab when using Git at the command line. HTTPS is less secure than SSH (secure shell). We recommend learning to use SSH as soon as possible to minimize security risks. Below are links to GitHub and GitLab documentation for using SSH. GitHub SSH documentation GitLab SSH documentation","title":"HTTPS vs SSH Access"},{"location":"workflow_solutions/r_environments/","text":"R Projects and Environments \u00b6 When working on multiple projects, it's likely that different sets of external analysis packages and their dependencies will be needed for each project. Managing these different projects is simple in something like Anaconda by creating a different virtual environment for each project, but this functionality is not fully built into RStudio by default. Instead, we suggest to take advantage of R Projects and the renv package to keep environments separate for each project you start. Important If you are planning to use the renv package for environment management, you should install it before creating a project. R Projects \u00b6 RStudio and Hadley Wickham have written extensively on using Projects to organize different research projects, so please read through that documentation. These docs will give a condensed overview of how to set up Projects. Creating a Project \u00b6 To begin, projects start at a parent or root directory within which most, if not all, files associated with that project should be stored. To create a project, find the Project dropdown at the top-right of RStudio, above the Environment window. Click New Project... This will open up a screen to select whether you want to create a new folder for your project, use an existing folder, or clone an existing Git repository. The following instructions assume you choose to create a new directory. Next you will need to choose your project type. There are a number of different preset projects for R packages, Shiny applications, and different implementations of Quarto. The top option will open a generic project, but any of these options can be converted to any other type of project. Finally, you will choose your project name, the location for the project directory, as well as choose whether you want to initialize a git repo for the project. In addition, you can choose to use renv for package dependency management. Please read more about renv here . It's highly suggested to use renv for future environment reproducibility. Afterwards, RStudio will reset, change the working directory to the project root, and create a .RProj file that controls the project settings. The project dropdown will have the newly created project name now. Project Settings \u00b6 At this point, you can start writting scripts as normal, but it would be useful to change some of the settings for the projects to help with some RStudio performance and set up Git and renv. General Settings \u00b6 Click the project dropdown again and select Project Options at the bottom. A window will appear with general settings. We advise to change the general settings to the following: This will create a clean environment each time you open the project and does not save environment variables when exiting RStudio. In our experience, trying to automatically load variables from a previous session can cause RStudio to open very slowly or crash depending on the amount of data being used. All variables can be recreated by running your scripts or by purposefully saving and loading selected variables from other data files such as csv or RData. Git Settings \u00b6 Another useful part of Projects is Git integration. Normally, you would need to manage git using the command line even though your development is in RStudio, but with Projects you can link a remote git repository to your Project. It then provides a graphical interface for creating branches, committing changes, and generally managing a remote code repository. Note To read more about how to get started with Git, please read our git documentation To begin, you should create an empty repository either at Github or UAB's Gitlab where your project will be stored. this will open a new page with instructions on linking this remote repository with your local project. Keep these instructions open for later. A picture of the important piece can be seen below. Then open the Git/SVN tab in the Project Settings. It will have no option set: Click the Version Control dropdown menu and select Git. RStudio will ask you if you want to initialize a new git repository. Click Yes. Restart RStudio if it asks you to. Now a new Git tab will be available in the upper right pane You will then need to add a link to the remote repository as the origin. Click the More dropdown in the Files tab in the bottom right pane and select Open New Terminal Here Copy the instructions for pushing an existing repository from the Git repo into your terminal. You can see an example of these instructions under step 1, and commands specific to your repository were given after you created your repository. Run these commands to link the local Project to the remote repository. Afterwards, the Origin field in the Git/SVN options will have changed to your remote repository address. Project Paths \u00b6 Projects are designed to be portable and so use relative paths the vast majority of the time. This means any folder or file should be referenced using the top level of the project as the root directory. For example, if I wanted to load the subj01.csv file from the data directory in the top level of the project, I would do that using read.csv('data/subj01.csv') . Absolute paths will cause scripts to break if the project is moved to a different location within the filesystem or onto another computer entirely. As well, opening a project will automatically set the working directory to be at the top level of the project. It's inadvisable to the change the working directory location while a project is open. renv \u00b6 Most, if not all, projects will use some combination of the thousands of packages available in R such as the tidyverse . The renv package helps manage all of your project's package dependencies as well as provide a way to easily share package environments with other researchers. This functionality is very similar to conda and virtualenv environments for Python users. More information about renv can be found on their site . It's suggested to keep a record of the packages your project uses with either renv or another tool for general reproducibility. In order to use renv , you should install it either through the package install tool in RStudio or using the install.packages command before creating an environment. If you already have an existing project but want to use renv ,you can install it and manually intialize the environment useing renv::init() . renv manages package versions and dependencies separately for each project. All downloaded packages are, by default, stored in $HOME/.cache/R/renv/cache/ and symlinks are created to these folders within a created renv folder in the project directory. If multiple projects use different versions of the same package, each individual version is installed and kept separate from the other versions. If a package has already been installed for one project, any future projects that use that same package version will automatically add a symlink to the existing package install saving on package installation and compilation time. Installing Packages \u00b6 You can make a change to your package environment at any point using a variety of installation methods. renv keeps track of packages installed from the following locations: CRAN using install.packages or the RStudio package installation interface Bioconductor using BiocManager::install() Github using devtools::install_github() or remotes::install_github() Gitlab using devtools::install_gitlab() or remotes::install_gitlab() Bitbucket Saving an Environment \u00b6 Once your environment is set to your satisfaction, you can save the state of the environment in an renv.lock file saved in the top level of your project directory using the renv::snapshot() command. This will save information such as the repository the packages was installed from (CRAN, github, etc.), the version, and the requirements. It will also store which version of R the project was using. Loading an Environment \u00b6 Once an environment has been saved to a lockfile, it can be loaded again using the renv::restore() command. This is useful in a number of situations. For instance, if you have installed a package that you are not happy with and do not want to keep, you can revert to the previous environment save state to remove that package and all of its dependencies. Another situation this is useful is when you want to rebuild the environment on another computer or when you are sharing your environment with another researcher.","title":"R Projects and Environments"},{"location":"workflow_solutions/r_environments/#r-projects-and-environments","text":"When working on multiple projects, it's likely that different sets of external analysis packages and their dependencies will be needed for each project. Managing these different projects is simple in something like Anaconda by creating a different virtual environment for each project, but this functionality is not fully built into RStudio by default. Instead, we suggest to take advantage of R Projects and the renv package to keep environments separate for each project you start. Important If you are planning to use the renv package for environment management, you should install it before creating a project.","title":"R Projects and Environments"},{"location":"workflow_solutions/r_environments/#r-projects","text":"RStudio and Hadley Wickham have written extensively on using Projects to organize different research projects, so please read through that documentation. These docs will give a condensed overview of how to set up Projects.","title":"R Projects"},{"location":"workflow_solutions/r_environments/#creating-a-project","text":"To begin, projects start at a parent or root directory within which most, if not all, files associated with that project should be stored. To create a project, find the Project dropdown at the top-right of RStudio, above the Environment window. Click New Project... This will open up a screen to select whether you want to create a new folder for your project, use an existing folder, or clone an existing Git repository. The following instructions assume you choose to create a new directory. Next you will need to choose your project type. There are a number of different preset projects for R packages, Shiny applications, and different implementations of Quarto. The top option will open a generic project, but any of these options can be converted to any other type of project. Finally, you will choose your project name, the location for the project directory, as well as choose whether you want to initialize a git repo for the project. In addition, you can choose to use renv for package dependency management. Please read more about renv here . It's highly suggested to use renv for future environment reproducibility. Afterwards, RStudio will reset, change the working directory to the project root, and create a .RProj file that controls the project settings. The project dropdown will have the newly created project name now.","title":"Creating a Project"},{"location":"workflow_solutions/r_environments/#project-settings","text":"At this point, you can start writting scripts as normal, but it would be useful to change some of the settings for the projects to help with some RStudio performance and set up Git and renv.","title":"Project Settings"},{"location":"workflow_solutions/r_environments/#general-settings","text":"Click the project dropdown again and select Project Options at the bottom. A window will appear with general settings. We advise to change the general settings to the following: This will create a clean environment each time you open the project and does not save environment variables when exiting RStudio. In our experience, trying to automatically load variables from a previous session can cause RStudio to open very slowly or crash depending on the amount of data being used. All variables can be recreated by running your scripts or by purposefully saving and loading selected variables from other data files such as csv or RData.","title":"General Settings"},{"location":"workflow_solutions/r_environments/#git-settings","text":"Another useful part of Projects is Git integration. Normally, you would need to manage git using the command line even though your development is in RStudio, but with Projects you can link a remote git repository to your Project. It then provides a graphical interface for creating branches, committing changes, and generally managing a remote code repository. Note To read more about how to get started with Git, please read our git documentation To begin, you should create an empty repository either at Github or UAB's Gitlab where your project will be stored. this will open a new page with instructions on linking this remote repository with your local project. Keep these instructions open for later. A picture of the important piece can be seen below. Then open the Git/SVN tab in the Project Settings. It will have no option set: Click the Version Control dropdown menu and select Git. RStudio will ask you if you want to initialize a new git repository. Click Yes. Restart RStudio if it asks you to. Now a new Git tab will be available in the upper right pane You will then need to add a link to the remote repository as the origin. Click the More dropdown in the Files tab in the bottom right pane and select Open New Terminal Here Copy the instructions for pushing an existing repository from the Git repo into your terminal. You can see an example of these instructions under step 1, and commands specific to your repository were given after you created your repository. Run these commands to link the local Project to the remote repository. Afterwards, the Origin field in the Git/SVN options will have changed to your remote repository address.","title":"Git Settings"},{"location":"workflow_solutions/r_environments/#project-paths","text":"Projects are designed to be portable and so use relative paths the vast majority of the time. This means any folder or file should be referenced using the top level of the project as the root directory. For example, if I wanted to load the subj01.csv file from the data directory in the top level of the project, I would do that using read.csv('data/subj01.csv') . Absolute paths will cause scripts to break if the project is moved to a different location within the filesystem or onto another computer entirely. As well, opening a project will automatically set the working directory to be at the top level of the project. It's inadvisable to the change the working directory location while a project is open.","title":"Project Paths"},{"location":"workflow_solutions/r_environments/#renv","text":"Most, if not all, projects will use some combination of the thousands of packages available in R such as the tidyverse . The renv package helps manage all of your project's package dependencies as well as provide a way to easily share package environments with other researchers. This functionality is very similar to conda and virtualenv environments for Python users. More information about renv can be found on their site . It's suggested to keep a record of the packages your project uses with either renv or another tool for general reproducibility. In order to use renv , you should install it either through the package install tool in RStudio or using the install.packages command before creating an environment. If you already have an existing project but want to use renv ,you can install it and manually intialize the environment useing renv::init() . renv manages package versions and dependencies separately for each project. All downloaded packages are, by default, stored in $HOME/.cache/R/renv/cache/ and symlinks are created to these folders within a created renv folder in the project directory. If multiple projects use different versions of the same package, each individual version is installed and kept separate from the other versions. If a package has already been installed for one project, any future projects that use that same package version will automatically add a symlink to the existing package install saving on package installation and compilation time.","title":"renv"},{"location":"workflow_solutions/r_environments/#installing-packages","text":"You can make a change to your package environment at any point using a variety of installation methods. renv keeps track of packages installed from the following locations: CRAN using install.packages or the RStudio package installation interface Bioconductor using BiocManager::install() Github using devtools::install_github() or remotes::install_github() Gitlab using devtools::install_gitlab() or remotes::install_gitlab() Bitbucket","title":"Installing Packages"},{"location":"workflow_solutions/r_environments/#saving-an-environment","text":"Once your environment is set to your satisfaction, you can save the state of the environment in an renv.lock file saved in the top level of your project directory using the renv::snapshot() command. This will save information such as the repository the packages was installed from (CRAN, github, etc.), the version, and the requirements. It will also store which version of R the project was using.","title":"Saving an Environment"},{"location":"workflow_solutions/r_environments/#loading-an-environment","text":"Once an environment has been saved to a lockfile, it can be loaded again using the renv::restore() command. This is useful in a number of situations. For instance, if you have installed a package that you are not happy with and do not want to keep, you can revert to the previous environment save state to remove that package and all of its dependencies. Another situation this is useful is when you want to rebuild the environment on another computer or when you are sharing your environment with another researcher.","title":"Loading an Environment"},{"location":"workflow_solutions/shell/","text":"Shell Reference \u00b6 Introductory Guides \u00b6 The shell is a powerful tool, and with great power comes great responsibility. The following warnings are not intended to frighten, but to give a sense of respect for the power of shell commands. Most commands are perfectly safe, and often when they do something unexpected it can be fixed with some work. We will do our best to warn you of commands with greater potential for destruction, but no documentation is perfect. We are not responsible for accidental deletions or overwrites caused inadvertently, or otherwise, by any commands run by researchers. Be warned that directories, files and file contents that are deleted or overwritten cannot be restored by us under any circumstances. Researchers are responsible for maintaining backups of their files. If in doubt about a command please contact Contact Us for guidance. Educational Resources \u00b6 The internet has thousands of guides for using the shell. Rather than devise our own complete lesson plan for using the shell, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the shell lesson, see https://swcarpentry.github.io/shell-novice/ . At the shell prompt, you can also use the command curl cheat.sh/<command> to get a simple-to-understand explanation of what the command does and how to use it (see curl ). Below is an example for the pwd command . Reference \u00b6 Command Concepts \u00b6 Commands are entered at the prompt. The prompt can take many forms, typically something like one of the following. Common features are: (1) a prompt character, often the dollar sign $ ; (2) a caret to indicate where characters will be inserted when you type, typically a blinking underscore _ or rectangle; (3) color to enhance meaning of various components. Bash on Cheaha... Git Bash on Windows Desktop... Oh My Zsh on Debian... Commands take the form command [optional] <required> . The word command should be replaced with the literal name of the command, such as pwd , ls and cd , among many others. The text [optional] is for flags and inputs that are not required to run the command or that have default values. These flags can be useful for modifying the behavior or output of the command. The text [required] is for flags and inputs that are required to run the command. These must be supplied by the user or the command will not function or produce an error. Flags start with the character - as with the -l flag in ls -l (see ls ). Flags that do not require input can be combined as ls -al . Flags that require input may not be combined as with the flags -n and -m 2 in grep -n -m 2 pattern textfile.txt (see grep ). All inputs are separated by the space character Space . If you wish to or must use a space character in an input, that input must be surrounded by quotation marks. Note that single quotes and double quotes have different behavior. Single quotes '' interpret all characters between them literally. Double quotes \"\" interprets special characters . In most cases, especially with variable contents, double quotes \"\" are preferred. All commands are run in a process. By default, commands run at the shell prompt are run in the shell process, and wait for execution to stop before returning control to you. It is possible to regain control earlier in a number of ways. Warning Copying commands from rich-text sources, such as .pdf , Microsoft Office and webpages, can result in copying special or invisible unicode characters. These characters can cause commands to behave unexpectedly and can be difficult to diagnose. Instead, please try pasting your command into a plain-text editor, like notepad, before copying to the shell prompt. How do I regain control of the prompt while a command is running? \u00b6 Running commands may be terminated using Ctrl + C . Pressing it once will request a graceful termination of the running command. Pressing it more than one will attempt to immediately kill the program. Open a new shell terminal and use that instead. Start the command as command [optional] <required> & . Note the trailing ampersand character & , which causes the command to be run asynchronously in the background. How do I terminate a process running asynchronously? \u00b6 Danger The commands listed here can cause loss of work by termination of incorrect processes if not used carefully. To kill a process running in another shell terminal or running in the background, use either kill or pkill together with an appropriate signal flag. The flag -15 sends SIGTERM which will allow the program to terminate itself gracefully. The flag -9 sends SIGKILL and will immediately terminate the process, in case -15 is not working. kill <signal> <pid> if you know the process id <pid> of the process. Use ps -u <username> to see your running processes. On Cheaha you can use ps -u $USER as a shortcut. pkill <signal> <name-pattern> if you know the name of the process. Warning Using pkill requires carefully thinking of an appropriate name pattern. An incorrect name pattern can cause unwanted termination of processes that may be important to you. Process termination cannot be stopped or undone. Special (Escaped) Characters \u00b6 Backslash \\ is used to write literal versions of certain special characters. Backslash is also called the escape character, and the special characters are also called escape sequences. Character escape sequences are useful in situations where you need a representation of a character, instead of the result of pressing the corresponding key on your keyboard. For example, if you want to store a newline character in a string then you can't just press Enter . If you did, you would immediately execute the command before you finished the string. Instead you can type \\n , which is the escape sequence for a newline character. \\t is interpreted as a tab character. The plain-text equivalent of pressing Tab . \\n is interpreted as a newline character. The plain-text equivalent of pressing Enter . \\\\ is interpreted as a single backslash. At the shell prompt, double quotes \"\" interpret newline characters, while single quotes '' do not. Below is an example file containing escaped characters (shown in nano ), and its interpreted output (shown with echo ). Piping and Command Chains \u00b6 Commands may be composed into chains using pipes with the pipe character | . For example, ls -l | wc -l counts the number of lines returned by ls -l (see ls and wc ). Warning This construct does not accurately count the number of items in a directory, and is only for demonstration purposes. Do not use this exact command chain in practice. Redirects \u00b6 Command inputs and outputs may be redirected with the characters < for input and > for output. Output redirects using > overwrite the contents of existing files and are destructive. Using >> in place of > appends contents to a file, rather than overwriting the contents. Inputs come from STDIN or 0 . Typical output is written to STDOUT or 1 and errors are written to STDERR or 2 . ls -l 1> dirlist stores the directory listing to the file dirlist . In this case using > is the same as 1> . ls -l doesnotexist 1> error 2>&1 stores the error message to the file error . The text 2>&1 means write STDERR to STDOUT . wc -l 0< lines reads the contents of file lines and counts the number of lines. Note that for wc this is not needed, but may be needed for other commands. In this case using < is the same as 0< . For more information on the commands used in the examples, see ls , wc and cat . Danger Output redirects using > are destructive. The contents of the target file are immediately overwritten when the command is executed. It is not possible to recover the contents of the file under any circumstances. Researchers are responsible for maintaining backups of their files. Path Concepts \u00b6 The working directory is the directory you are currently in and may be identified using the command pwd . Dot . is a shortcut for the working directory. This is only used in some contexts. Double dot .. is a shortcut for the immediate parent directory of whatever comes before it. Twiddle ~ is a shortcut for your home directory. Forwardslash / is the path to the root directory of the filesystem, which has no parent. Files and directory names starting with . are hidden. Paths are formed of text-based directory names separated by / Absolute paths start at the root directory, e.g. /home/user/documents/ . Relative paths start at the working directory, e.g. bin . Below are examples of constructed paths tested with ls . Glob Syntax \u00b6 Glob is a shorthand syntax for dealing with many files and directories matching simple patterns. Question mark ? matches a single character. c?t matches both cat and cut . Star * matches any string. c* matches cat , cut , and clatter . *.png matches all png files. Double star ** matches any number of directories with any names. **/*.png matches all png files within any subdirectory of the working directory. This is not commonly used, but extremely useful for some applications. Below are examples of glob usage tested with ls . Environment Concepts \u00b6 Environment variables may be assigned by using var=value where var is the variable name and value is its value. Below is an example tested with echo . Environment variable values may be expanded by using \"${var}\" where var is the variable name. On Cheaha... \"${HOME}\" expands to the path to your home directory. \"${USER_DATA}\" expands to the path to your data/user/<username> directory. \"${USER}\" expands to your user name. Always use double quotes around variables. DO use \"${HOME}\" , do NOT use ${HOME} . Double quotes ensure that space characters in expanded values are handled appropriately. Below, note the error occurring without double quotes due to the spaces in the directory name. Double quoting the variable fixes the error. The commands used to test are ls , cd and pwd . Expanding a variable that isn't defined returns an empty string and does not produce an error, but may cause unexpected behavior. Environment variables may be expanded in paths and command arguments to save effort and time. \"${USER_DATA}/project/inputs\" command ${VARIABLE} Warning Modifying, changing or overwriting existing environment variables while in a shell session can result in unexpected behavior. The environment can be reset to its default starting state by exiting the shell session and starting a new session. Script Concepts \u00b6 Scripts are a way to bundle many commands together and execute them in sequence. Scripts should start with the intended interpreter using a hash-bang like #!/bin/bash . Most commonly bash is the intended interpreter on our systems. Other shell interpreters may be installed and used, but are not necessary. To execute the script given by the hash-bang or she-bang (pronounced shih-bang) #! , use ./script.sh in the folder containing the script. Executable permissions must be set to use a script this way, with chmod u+x <script-path> . To execute the script using a specific interpreter use bash <script-path> , or replace bash with your preferred interpreter. Beware that not all interpreters behave the same way. Executable permissions do not need to be set to use a script this way. Space-separated arguments may be passed to a script when executed in the same way as any other command. Script Arguments \u00b6 Arguments or parameters are passed to a command or script as a space separated list. Arguments may be referred to using numeric variables. The following list contains examples of variable references to arguments. \"${0}\" is the execution path. If you use ./script.sh then \"${0}\" will be ./script.sh . \"${1}\" , \"${2}\" , etc., are the first, second, etc., space separated variables. Calling ./script.sh hello world will have $1 = hello and $2 = world . Many arguments may be passed this way. ${@} is all arguments except \"${0}\" . Important!! Note that double quotes \" are not used! Double quoting would bind all the arguments together. If you need to pass a group of arguments to another script, be cautious about using quotes. ${@: 2} is all arguments starting with \"${2}\" . ${@: 2:2} is the second and third argument. ${@: -2} is the last two arguments. ${@: -2:1} is the second to last argument only. Below is an example script file, hash-bang not shown, demonstrating how each argument variable works, and its interpreted output. The text editor nano is used to display the file and chmod is used to modify file permissions. Tip Using the shell requires some defensive techniques. Never use the space character Space in variables, directory names, file names, etc. Instead, only use letters, numbers and the underscore character _ . In bash it is also allowed to use the hyphen character - , but this may not be portable to other shell interpreters. Names should only start with one or more letters or numbers. Do protect yourself from others who might use the space character Space by always double quoting your variables like \"${var}\" instead of ${var} or $var . Commands for Solutions to Common Problems \u00b6 Below is a reference guide to various commands through the lens of problems to be solved. Note When you see words surrounded by angle brackets like <name> , you should not take that as a literal part of the command. In the case of <name> you would replace it with whatever name is appropriate. Important If you are using Cheaha and working with more than a few files or directories, or the files are large, please run your shell commands in a Job Context . Danger It is safest to assume that any command run at the shell cannot be undone. Be especially aware of the rm command, which is destructive. We do not maintain backups of any files, so once those files are removed or deleted they cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Show working directory ( pwd ) \u00b6 Use pwd , which stands for present working directory. List files and directories ( ls ) \u00b6 Below are common uses of ls , short for \"list\", used to display directory contents and examine details of files and directories. It may be used to check file permissions when using chmod . Visible files only, list: ls . Multiple entries per line. Visible files only, table: ls -l . One entry per line and shows permissions , size in bytes on disk, and timestamp. Visible and hidden files, list ls -a . Same as ls , but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Visible and hidden files, table ls -al . Same as ls -l but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Examine disk usage ( du ) \u00b6 Use du to examine disk usage of files and directories. By default all values are given in bytes, use the flag -h to give values in K , M , G and T for kilobytes, megabytes, gigabytes and terabytes, respectively. Use the flag -s to summarize space used by directories. Below is an example of du -sh . Note that only directories with read permissions can be examined by du . When culling files to conserve storage space, it helps to find the largest files and directories. To find the ten largest, use the command du -sh .[^.]* * | sort -hr | head -n10 in the top-level directory of your data. To better understand this command chain, see also sort , head and piping . Change working directory ( cd ) \u00b6 To change to a different directory, use cd <directory-path> . The variable <path> may be relative, like my/path . This will move to the subdirectory my/path within the working directory. Relative directory paths may contain multiple .. shortcuts to indicate parent directories. Or <path> may be absolute, like /my/path which will move to the directory /my/path , starting at the root of the filesystem. Recall the root directory is just / . To move up to the parent of the working directory, use cd .. . Below are examples of cd usage, tested with ls . Copy files and directories ( cp ) \u00b6 Below are use cases with associated commands, and examples tested using ls . Single file, change name: cp -a <source-file-path> <destination-file-path> . Single file, keep name: cp -a <source-file-path> <destination-directory-path> . Directory: cp -a <source-directory-path> <destination-directory-path> . Glob: cp -a <source-path-glob> <destination-directory-path> . Move files and directories mv \u00b6 Below are use cases with associated commands, and examples tested using ls . Single file, change name: mv <source-file-path> <destination-file-path> . Single file, keep name: mv <source-file-path> <destination-directory-path> . Directory: mv <source-directory-path> <destination-directory-path> . Glob: mv <source-path-glob> <destination-directory-path> . Delete files and directories ( rm , rmdir ) \u00b6 Danger The rm command is destructive and cannot be undone. We do not maintain backups of any files, so files that are removed or deleted cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Below are use cases with associated commands, and examples tested using ls . Single file: rm <file-path> . Empty Directory: rmdir <directory-path> . Directory with Contents: rm -r <directory-path> . Glob: rm <file-path-glob> . Warning Careless use of the directory and glob forms of rm can lead to unwanted data loss. Be sure to double check your commands before executing. Download files from internet sources (curl) \u00b6 Use curl to download files and webpages from internet sources. By default curl writes to STDOUT . If you wish to save the output to a file, use the -o <file-path> command. Note that curl does not transform, encode or decode the data in any way, and it is saved exactly as received from the supplied url. By far the most common usage is to download a file. To do so use curl -o <file-path> <url> , where <file-path> is the desired local path to save and <url> is the web address of the source data. Create directories ( mkdir ) \u00b6 Use mkdir <directory-name> . Example below is tested using ls . Create files ( touch ) \u00b6 Use touch <file-path> . Example below is tested using ls . Edit plain-text files ( nano ) \u00b6 Use nano <file-path> . If the file exists, it will be opened. If the file does not exist, it will be stored in memory until saved. If the contents are not saved they will be discarded and cannot be recovered. You may also use just nano , without a file path, to create a new empty file. The character ^ is the Ctrl key, so ^y would require pressing Ctrl + Y . To save use Ctrl + X to open the exit prompt. Press Y for yes to bring up the save prompt. If you opened nano using an existing file, or passed in a file path, then the File Name to Write prompt will be autofilled with the given file name. If not, you will need to enter it here. When you have the file name entered as desired, press Enter to save. Below is a demonstration of the shell with the new file listed using ls . Example below is tested using ls . Searching for text in nano \u00b6 Use the key combination Ctrl + W to search for text. Follow the prompts as they appear to navigate. Count lines, words and characters ( wc ) \u00b6 Lines: wc -l <file-path> . Words: wc -w <file-path> . Characters: wc -m <file-path> . All: wc <file-path> . Below are example outputs of a file named newfile containing the text hello world followed by a blank line. Note the file is redirected to wc from STDIN . Display file contents ( cat , less ) \u00b6 Use cat <file-path> to display the contents of the file at <file-path> . If the contents are too long to be displayed on one screen, you can pipe the output to less to add scrolling functionality by using cat <file-path> | less . Use the keyboard key Q to stop using less . Below are two images showing what less looks like. Note the last line of each image, which indicates you are using the less environment. The first image is an example where there is more text below the visible text. The second image is an example at the bottom of the text. Examine start and end of file ( head , tail ) \u00b6 To display only the first two lines use head -n 2 <file-path> . Use tail instead of head for the last two lines. Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. Sort file contents ( sort ) \u00b6 Alphabetical: sort <file-path> . Numeric: sort -n <file-path> . Ignore case: sort -i <file-path> . Lines in the input file are returned in sorted order. The results are displayed in stdout. To write the sorted result to a file use sort <file-path> > <new-file-path> . Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. Test a command ( echo ) \u00b6 Use echo \"<command>\" to see the expanded command without executing it. For example echo \"cp -a $USER_DATA /mydir $HOME \" # prints cp -a /data/user/<username>/mydir /home/<username> Use the -e flag if you need to interpret escaped characters such as \\t for tab or \\n for new line. The command being examined in the example below is cp Search for text ( grep ) \u00b6 Use grep \"<pattern>\" \"<file-path>\" to search for <pattern> in the file at <file-path> . Use the -n flag to display line numbers with results. Below is an example of grep -n on a file. The number at the start of the result line is the line number of the pattern match. The matched portion is shown in red while other text on the same line is shown in white. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. We are looking for the literal text \"echo\" within the file. Close the session ( exit ) \u00b6 Use exit . Clear the shell display ( clear ) \u00b6 Use clear . Where is a command located? ( which ) \u00b6 Use which <command-name> . The command being searched for is ls . What does a command do? ( whatis , man ) \u00b6 For builtin command and aliases there are two distinct options for learning more. The command being examined is ls . Use whatis <command-name> to get a brief summary of the command. Use man <command-name> to get the full help file in a less environment. Use q on your keyboard to exit less . Remotely access shell on other machines ( ssh ) \u00b6 See our SSH Section for more detailed information. Remotely access or transfer files between machines ( sftp ) \u00b6 See our Remote Access - Data Transfer Section for more detailed information. Submit and manage jobs on Cheaha \u00b6 See our Slurm Section for more detailed information. Manage permissions of files and directores ( chmod ) \u00b6 Use chmod with the least permissions needed to accomplish a task. Permission management is an important part of managing access and control of files and directories. Danger Please carefully consider security when working in shared spaces like Cheaha. Setting private directories or files as readable by other users can inadvertently expose sensitive or protected information and may violate IT policy, FERPA, HIPAA or some combination. There are legitimate use cases for truly shared spaces. Please Contact Us if you need to share information with other users or collaborators and aren't sure of how to do so securely. What Permissions Do \u00b6 Setting the permissions of a file affect the contents of only that file. A read-only file can still be deleted by users with write permissions in its parent directory. Read permissions allow viewing and copying contents of a file. Write permissions allow changing the contents of a file, including deleting all of the contents. Execute permissions allow using the file as an executable. Helpful for scripts and compiled programs. Scripts and interpreted language files, like Python, must also have read permission set. Setting the permissions of directories affects what can be done with contained files and directories. Read permission allows the use of ls within the directory. cp may be used to copy files from the directory to somewhere else. Write permission allows creation of files and directories, as well as the use of touch , mv and rm on files and directories, within the directory. cp may be used to copy files into the directory. Executable permission allows setting the directory as working directory and the use of cd into the directory. Permissions are not inherited from their parent directory. How to Check Permissions \u00b6 Use ls -ald <path> to see the permissions on the file or directory at <path> . The -d flag lists directories instead of their contents. Patterns for Setting Permissions \u00b6 Two separate patterns can be used to set or change permissions on files and directories. Either may be used, but they cannot be combined in a single use of chmod . In the example images below, the command ls is used to check permissions and the command cat is used to display the contents of the script . Symbols The letter and symbol pattern is in the form a=r . There are three parts. A collection of letters denoting who, e.g. a in this case. Multiple letters may be used. u the owner of the file or directory g the owner's group members o users outside the owner's group a all users (same as ugo ). A symbol indicating how to change the permissions = set permissions - remove permissions + add permissions A collection of letters denoting which permissions to change. Multiple letters may be used. r read w write (change the contents) x execute To add executable permission for only the owner chmod u+x <file-path> . Useful for custom scripts and compiled executables you will use directly from the command line. To set read-only permission for everyone use chmod a=r <file-path> . Note Using = to set permissions will both add and remove permissions. Using a=r will take away existing write and execute permissions. Below is an example of chmod used symbolically to set user execute permissions on a script. Note the error before permissions are set. Numerals Bit mask patterns are in the form 755 . Each digit is the sum of three binary bits. The bits are 4 read 2 write 1 execute The left digit is the owner's permissions. Middle digit is the owner's group. Right digit is users outside the owner's group. Setting chmod 755 means the following: For the owner, set 4 read, 2 write and 1 execute. 4+2+1=7 . For the owner's group, set 4 read, 1 execute. 4+1=5 . For other users, set 4 read, 1 execute. 4+1=5 . Setting 755 is a common pattern for system-wide scripts. Because the 4 read and 1 executable bits are set for all users, it can be called from anywhere by any other script, and not just the owner. However, the 2 write bit is set only for the owner, so other users cannot modify the contents of the script. Below is an example of chmod used numerically to add execution permissions to all users. Note the error before permissions are set. Examples \u00b6 chmod u+x script.sh adds execute permission for you, to a script. chmod 755 script.sh makes a script readable and executable by all, but only writeable by you. This is a common permission for non-sensitive files and directories. chmod 740 sensitive_directory make a directory readable by you and your group, and writeable and executable by only you. Other users cannot delete files in this folder. chmod ug=r notes.txt followed by chmod o-rwx notes.txt makes a file read-only for you and your group and removes all permissions for other users. Manage group ownership ( chgrp ) \u00b6 To change group ownership of files and directories use chgrp . The command chgrp may only be used on a file or directory if you own it, or if you are a member of its current group owner and a member of its new group owner. For single files use chgrp <new-group> <file> . To change a directory and all of its contents recursively use chgrp -hR <new-group> <file> . The -h flag will avoid walking through the targets of symbolic links. Warning When using chgrp -R , the default behavior is to walk through the contents of symbolic links. If this is not desired, use -hR . Manage researcher access to files and directories ( getfacl , setfacl ) \u00b6 Construction Under construction.","title":"Using the Shell"},{"location":"workflow_solutions/shell/#shell-reference","text":"","title":"Shell Reference"},{"location":"workflow_solutions/shell/#introductory-guides","text":"The shell is a powerful tool, and with great power comes great responsibility. The following warnings are not intended to frighten, but to give a sense of respect for the power of shell commands. Most commands are perfectly safe, and often when they do something unexpected it can be fixed with some work. We will do our best to warn you of commands with greater potential for destruction, but no documentation is perfect. We are not responsible for accidental deletions or overwrites caused inadvertently, or otherwise, by any commands run by researchers. Be warned that directories, files and file contents that are deleted or overwritten cannot be restored by us under any circumstances. Researchers are responsible for maintaining backups of their files. If in doubt about a command please contact Contact Us for guidance.","title":"Introductory Guides"},{"location":"workflow_solutions/shell/#educational-resources","text":"The internet has thousands of guides for using the shell. Rather than devise our own complete lesson plan for using the shell, we recommend finding and using one of the high-quality lessons available on the internet. The Software Carpentries group offers a number of high-quality online lesson plans for introductory computing and data science tools. While these lesson plans are intended to be delivered by an instructor in a classroom setting, they can be still be useful to follow solo. For the shell lesson, see https://swcarpentry.github.io/shell-novice/ . At the shell prompt, you can also use the command curl cheat.sh/<command> to get a simple-to-understand explanation of what the command does and how to use it (see curl ). Below is an example for the pwd command .","title":"Educational Resources"},{"location":"workflow_solutions/shell/#reference","text":"","title":"Reference"},{"location":"workflow_solutions/shell/#command-concepts","text":"Commands are entered at the prompt. The prompt can take many forms, typically something like one of the following. Common features are: (1) a prompt character, often the dollar sign $ ; (2) a caret to indicate where characters will be inserted when you type, typically a blinking underscore _ or rectangle; (3) color to enhance meaning of various components. Bash on Cheaha... Git Bash on Windows Desktop... Oh My Zsh on Debian... Commands take the form command [optional] <required> . The word command should be replaced with the literal name of the command, such as pwd , ls and cd , among many others. The text [optional] is for flags and inputs that are not required to run the command or that have default values. These flags can be useful for modifying the behavior or output of the command. The text [required] is for flags and inputs that are required to run the command. These must be supplied by the user or the command will not function or produce an error. Flags start with the character - as with the -l flag in ls -l (see ls ). Flags that do not require input can be combined as ls -al . Flags that require input may not be combined as with the flags -n and -m 2 in grep -n -m 2 pattern textfile.txt (see grep ). All inputs are separated by the space character Space . If you wish to or must use a space character in an input, that input must be surrounded by quotation marks. Note that single quotes and double quotes have different behavior. Single quotes '' interpret all characters between them literally. Double quotes \"\" interprets special characters . In most cases, especially with variable contents, double quotes \"\" are preferred. All commands are run in a process. By default, commands run at the shell prompt are run in the shell process, and wait for execution to stop before returning control to you. It is possible to regain control earlier in a number of ways. Warning Copying commands from rich-text sources, such as .pdf , Microsoft Office and webpages, can result in copying special or invisible unicode characters. These characters can cause commands to behave unexpectedly and can be difficult to diagnose. Instead, please try pasting your command into a plain-text editor, like notepad, before copying to the shell prompt.","title":"Command Concepts"},{"location":"workflow_solutions/shell/#how-do-i-regain-control-of-the-prompt-while-a-command-is-running","text":"Running commands may be terminated using Ctrl + C . Pressing it once will request a graceful termination of the running command. Pressing it more than one will attempt to immediately kill the program. Open a new shell terminal and use that instead. Start the command as command [optional] <required> & . Note the trailing ampersand character & , which causes the command to be run asynchronously in the background.","title":"How do I regain control of the prompt while a command is running?"},{"location":"workflow_solutions/shell/#how-do-i-terminate-a-process-running-asynchronously","text":"Danger The commands listed here can cause loss of work by termination of incorrect processes if not used carefully. To kill a process running in another shell terminal or running in the background, use either kill or pkill together with an appropriate signal flag. The flag -15 sends SIGTERM which will allow the program to terminate itself gracefully. The flag -9 sends SIGKILL and will immediately terminate the process, in case -15 is not working. kill <signal> <pid> if you know the process id <pid> of the process. Use ps -u <username> to see your running processes. On Cheaha you can use ps -u $USER as a shortcut. pkill <signal> <name-pattern> if you know the name of the process. Warning Using pkill requires carefully thinking of an appropriate name pattern. An incorrect name pattern can cause unwanted termination of processes that may be important to you. Process termination cannot be stopped or undone.","title":"How do I terminate a process running asynchronously?"},{"location":"workflow_solutions/shell/#special-escaped-characters","text":"Backslash \\ is used to write literal versions of certain special characters. Backslash is also called the escape character, and the special characters are also called escape sequences. Character escape sequences are useful in situations where you need a representation of a character, instead of the result of pressing the corresponding key on your keyboard. For example, if you want to store a newline character in a string then you can't just press Enter . If you did, you would immediately execute the command before you finished the string. Instead you can type \\n , which is the escape sequence for a newline character. \\t is interpreted as a tab character. The plain-text equivalent of pressing Tab . \\n is interpreted as a newline character. The plain-text equivalent of pressing Enter . \\\\ is interpreted as a single backslash. At the shell prompt, double quotes \"\" interpret newline characters, while single quotes '' do not. Below is an example file containing escaped characters (shown in nano ), and its interpreted output (shown with echo ).","title":"Special (Escaped) Characters"},{"location":"workflow_solutions/shell/#piping-and-command-chains","text":"Commands may be composed into chains using pipes with the pipe character | . For example, ls -l | wc -l counts the number of lines returned by ls -l (see ls and wc ). Warning This construct does not accurately count the number of items in a directory, and is only for demonstration purposes. Do not use this exact command chain in practice.","title":"Piping and Command Chains"},{"location":"workflow_solutions/shell/#redirects","text":"Command inputs and outputs may be redirected with the characters < for input and > for output. Output redirects using > overwrite the contents of existing files and are destructive. Using >> in place of > appends contents to a file, rather than overwriting the contents. Inputs come from STDIN or 0 . Typical output is written to STDOUT or 1 and errors are written to STDERR or 2 . ls -l 1> dirlist stores the directory listing to the file dirlist . In this case using > is the same as 1> . ls -l doesnotexist 1> error 2>&1 stores the error message to the file error . The text 2>&1 means write STDERR to STDOUT . wc -l 0< lines reads the contents of file lines and counts the number of lines. Note that for wc this is not needed, but may be needed for other commands. In this case using < is the same as 0< . For more information on the commands used in the examples, see ls , wc and cat . Danger Output redirects using > are destructive. The contents of the target file are immediately overwritten when the command is executed. It is not possible to recover the contents of the file under any circumstances. Researchers are responsible for maintaining backups of their files.","title":"Redirects"},{"location":"workflow_solutions/shell/#path-concepts","text":"The working directory is the directory you are currently in and may be identified using the command pwd . Dot . is a shortcut for the working directory. This is only used in some contexts. Double dot .. is a shortcut for the immediate parent directory of whatever comes before it. Twiddle ~ is a shortcut for your home directory. Forwardslash / is the path to the root directory of the filesystem, which has no parent. Files and directory names starting with . are hidden. Paths are formed of text-based directory names separated by / Absolute paths start at the root directory, e.g. /home/user/documents/ . Relative paths start at the working directory, e.g. bin . Below are examples of constructed paths tested with ls .","title":"Path Concepts"},{"location":"workflow_solutions/shell/#glob-syntax","text":"Glob is a shorthand syntax for dealing with many files and directories matching simple patterns. Question mark ? matches a single character. c?t matches both cat and cut . Star * matches any string. c* matches cat , cut , and clatter . *.png matches all png files. Double star ** matches any number of directories with any names. **/*.png matches all png files within any subdirectory of the working directory. This is not commonly used, but extremely useful for some applications. Below are examples of glob usage tested with ls .","title":"Glob Syntax"},{"location":"workflow_solutions/shell/#environment-concepts","text":"Environment variables may be assigned by using var=value where var is the variable name and value is its value. Below is an example tested with echo . Environment variable values may be expanded by using \"${var}\" where var is the variable name. On Cheaha... \"${HOME}\" expands to the path to your home directory. \"${USER_DATA}\" expands to the path to your data/user/<username> directory. \"${USER}\" expands to your user name. Always use double quotes around variables. DO use \"${HOME}\" , do NOT use ${HOME} . Double quotes ensure that space characters in expanded values are handled appropriately. Below, note the error occurring without double quotes due to the spaces in the directory name. Double quoting the variable fixes the error. The commands used to test are ls , cd and pwd . Expanding a variable that isn't defined returns an empty string and does not produce an error, but may cause unexpected behavior. Environment variables may be expanded in paths and command arguments to save effort and time. \"${USER_DATA}/project/inputs\" command ${VARIABLE} Warning Modifying, changing or overwriting existing environment variables while in a shell session can result in unexpected behavior. The environment can be reset to its default starting state by exiting the shell session and starting a new session.","title":"Environment Concepts"},{"location":"workflow_solutions/shell/#script-concepts","text":"Scripts are a way to bundle many commands together and execute them in sequence. Scripts should start with the intended interpreter using a hash-bang like #!/bin/bash . Most commonly bash is the intended interpreter on our systems. Other shell interpreters may be installed and used, but are not necessary. To execute the script given by the hash-bang or she-bang (pronounced shih-bang) #! , use ./script.sh in the folder containing the script. Executable permissions must be set to use a script this way, with chmod u+x <script-path> . To execute the script using a specific interpreter use bash <script-path> , or replace bash with your preferred interpreter. Beware that not all interpreters behave the same way. Executable permissions do not need to be set to use a script this way. Space-separated arguments may be passed to a script when executed in the same way as any other command.","title":"Script Concepts"},{"location":"workflow_solutions/shell/#script-arguments","text":"Arguments or parameters are passed to a command or script as a space separated list. Arguments may be referred to using numeric variables. The following list contains examples of variable references to arguments. \"${0}\" is the execution path. If you use ./script.sh then \"${0}\" will be ./script.sh . \"${1}\" , \"${2}\" , etc., are the first, second, etc., space separated variables. Calling ./script.sh hello world will have $1 = hello and $2 = world . Many arguments may be passed this way. ${@} is all arguments except \"${0}\" . Important!! Note that double quotes \" are not used! Double quoting would bind all the arguments together. If you need to pass a group of arguments to another script, be cautious about using quotes. ${@: 2} is all arguments starting with \"${2}\" . ${@: 2:2} is the second and third argument. ${@: -2} is the last two arguments. ${@: -2:1} is the second to last argument only. Below is an example script file, hash-bang not shown, demonstrating how each argument variable works, and its interpreted output. The text editor nano is used to display the file and chmod is used to modify file permissions. Tip Using the shell requires some defensive techniques. Never use the space character Space in variables, directory names, file names, etc. Instead, only use letters, numbers and the underscore character _ . In bash it is also allowed to use the hyphen character - , but this may not be portable to other shell interpreters. Names should only start with one or more letters or numbers. Do protect yourself from others who might use the space character Space by always double quoting your variables like \"${var}\" instead of ${var} or $var .","title":"Script Arguments"},{"location":"workflow_solutions/shell/#commands-for-solutions-to-common-problems","text":"Below is a reference guide to various commands through the lens of problems to be solved. Note When you see words surrounded by angle brackets like <name> , you should not take that as a literal part of the command. In the case of <name> you would replace it with whatever name is appropriate. Important If you are using Cheaha and working with more than a few files or directories, or the files are large, please run your shell commands in a Job Context . Danger It is safest to assume that any command run at the shell cannot be undone. Be especially aware of the rm command, which is destructive. We do not maintain backups of any files, so once those files are removed or deleted they cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files.","title":"Commands for Solutions to Common Problems"},{"location":"workflow_solutions/shell/#show-working-directory-pwd","text":"Use pwd , which stands for present working directory.","title":"Show working directory (pwd)"},{"location":"workflow_solutions/shell/#list-files-and-directories-ls","text":"Below are common uses of ls , short for \"list\", used to display directory contents and examine details of files and directories. It may be used to check file permissions when using chmod . Visible files only, list: ls . Multiple entries per line. Visible files only, table: ls -l . One entry per line and shows permissions , size in bytes on disk, and timestamp. Visible and hidden files, list ls -a . Same as ls , but has hidden files and directories and . and .. . Example below is truncated to conserve page space. Visible and hidden files, table ls -al . Same as ls -l but has hidden files and directories and . and .. . Example below is truncated to conserve page space.","title":"List files and directories (ls)"},{"location":"workflow_solutions/shell/#examine-disk-usage-du","text":"Use du to examine disk usage of files and directories. By default all values are given in bytes, use the flag -h to give values in K , M , G and T for kilobytes, megabytes, gigabytes and terabytes, respectively. Use the flag -s to summarize space used by directories. Below is an example of du -sh . Note that only directories with read permissions can be examined by du . When culling files to conserve storage space, it helps to find the largest files and directories. To find the ten largest, use the command du -sh .[^.]* * | sort -hr | head -n10 in the top-level directory of your data. To better understand this command chain, see also sort , head and piping .","title":"Examine disk usage (du)"},{"location":"workflow_solutions/shell/#change-working-directory-cd","text":"To change to a different directory, use cd <directory-path> . The variable <path> may be relative, like my/path . This will move to the subdirectory my/path within the working directory. Relative directory paths may contain multiple .. shortcuts to indicate parent directories. Or <path> may be absolute, like /my/path which will move to the directory /my/path , starting at the root of the filesystem. Recall the root directory is just / . To move up to the parent of the working directory, use cd .. . Below are examples of cd usage, tested with ls .","title":"Change working directory (cd)"},{"location":"workflow_solutions/shell/#copy-files-and-directories-cp","text":"Below are use cases with associated commands, and examples tested using ls . Single file, change name: cp -a <source-file-path> <destination-file-path> . Single file, keep name: cp -a <source-file-path> <destination-directory-path> . Directory: cp -a <source-directory-path> <destination-directory-path> . Glob: cp -a <source-path-glob> <destination-directory-path> .","title":"Copy files and directories (cp)"},{"location":"workflow_solutions/shell/#move-files-and-directories-mv","text":"Below are use cases with associated commands, and examples tested using ls . Single file, change name: mv <source-file-path> <destination-file-path> . Single file, keep name: mv <source-file-path> <destination-directory-path> . Directory: mv <source-directory-path> <destination-directory-path> . Glob: mv <source-path-glob> <destination-directory-path> .","title":"Move files and directories mv"},{"location":"workflow_solutions/shell/#delete-files-and-directories-rm-rmdir","text":"Danger The rm command is destructive and cannot be undone. We do not maintain backups of any files, so files that are removed or deleted cannot be recovered by us under any circumstances. Researchers are responsible for maintaining backups of their files. Below are use cases with associated commands, and examples tested using ls . Single file: rm <file-path> . Empty Directory: rmdir <directory-path> . Directory with Contents: rm -r <directory-path> . Glob: rm <file-path-glob> . Warning Careless use of the directory and glob forms of rm can lead to unwanted data loss. Be sure to double check your commands before executing.","title":"Delete files and directories (rm, rmdir)"},{"location":"workflow_solutions/shell/#download-files-from-internet-sources-curl","text":"Use curl to download files and webpages from internet sources. By default curl writes to STDOUT . If you wish to save the output to a file, use the -o <file-path> command. Note that curl does not transform, encode or decode the data in any way, and it is saved exactly as received from the supplied url. By far the most common usage is to download a file. To do so use curl -o <file-path> <url> , where <file-path> is the desired local path to save and <url> is the web address of the source data.","title":"Download files from internet sources (curl)"},{"location":"workflow_solutions/shell/#create-directories-mkdir","text":"Use mkdir <directory-name> . Example below is tested using ls .","title":"Create directories (mkdir)"},{"location":"workflow_solutions/shell/#create-files-touch","text":"Use touch <file-path> . Example below is tested using ls .","title":"Create files (touch)"},{"location":"workflow_solutions/shell/#edit-plain-text-files-nano","text":"Use nano <file-path> . If the file exists, it will be opened. If the file does not exist, it will be stored in memory until saved. If the contents are not saved they will be discarded and cannot be recovered. You may also use just nano , without a file path, to create a new empty file. The character ^ is the Ctrl key, so ^y would require pressing Ctrl + Y . To save use Ctrl + X to open the exit prompt. Press Y for yes to bring up the save prompt. If you opened nano using an existing file, or passed in a file path, then the File Name to Write prompt will be autofilled with the given file name. If not, you will need to enter it here. When you have the file name entered as desired, press Enter to save. Below is a demonstration of the shell with the new file listed using ls . Example below is tested using ls .","title":"Edit plain-text files (nano)"},{"location":"workflow_solutions/shell/#searching-for-text-in-nano","text":"Use the key combination Ctrl + W to search for text. Follow the prompts as they appear to navigate.","title":"Searching for text in nano"},{"location":"workflow_solutions/shell/#count-lines-words-and-characters-wc","text":"Lines: wc -l <file-path> . Words: wc -w <file-path> . Characters: wc -m <file-path> . All: wc <file-path> . Below are example outputs of a file named newfile containing the text hello world followed by a blank line. Note the file is redirected to wc from STDIN .","title":"Count lines, words and characters (wc)"},{"location":"workflow_solutions/shell/#display-file-contents-cat-less","text":"Use cat <file-path> to display the contents of the file at <file-path> . If the contents are too long to be displayed on one screen, you can pipe the output to less to add scrolling functionality by using cat <file-path> | less . Use the keyboard key Q to stop using less . Below are two images showing what less looks like. Note the last line of each image, which indicates you are using the less environment. The first image is an example where there is more text below the visible text. The second image is an example at the bottom of the text.","title":"Display file contents (cat, less)"},{"location":"workflow_solutions/shell/#examine-start-and-end-of-file-head-tail","text":"To display only the first two lines use head -n 2 <file-path> . Use tail instead of head for the last two lines. Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.","title":"Examine start and end of file (head, tail)"},{"location":"workflow_solutions/shell/#sort-file-contents-sort","text":"Alphabetical: sort <file-path> . Numeric: sort -n <file-path> . Ignore case: sort -i <file-path> . Lines in the input file are returned in sorted order. The results are displayed in stdout. To write the sorted result to a file use sort <file-path> > <new-file-path> . Below is an image showing the use of head and tail on a file with four lines. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks.","title":"Sort file contents (sort)"},{"location":"workflow_solutions/shell/#test-a-command-echo","text":"Use echo \"<command>\" to see the expanded command without executing it. For example echo \"cp -a $USER_DATA /mydir $HOME \" # prints cp -a /data/user/<username>/mydir /home/<username> Use the -e flag if you need to interpret escaped characters such as \\t for tab or \\n for new line. The command being examined in the example below is cp","title":"Test a command (echo)"},{"location":"workflow_solutions/shell/#search-for-text-grep","text":"Use grep \"<pattern>\" \"<file-path>\" to search for <pattern> in the file at <file-path> . Use the -n flag to display line numbers with results. Below is an example of grep -n on a file. The number at the start of the result line is the line number of the pattern match. The matched portion is shown in red while other text on the same line is shown in white. The file was created by redirecting the echo command to a file, using the special newline character to add line breaks. We are looking for the literal text \"echo\" within the file.","title":"Search for text (grep)"},{"location":"workflow_solutions/shell/#close-the-session-exit","text":"Use exit .","title":"Close the session (exit)"},{"location":"workflow_solutions/shell/#clear-the-shell-display-clear","text":"Use clear .","title":"Clear the shell display (clear)"},{"location":"workflow_solutions/shell/#where-is-a-command-located-which","text":"Use which <command-name> . The command being searched for is ls .","title":"Where is a command located? (which)"},{"location":"workflow_solutions/shell/#what-does-a-command-do-whatis-man","text":"For builtin command and aliases there are two distinct options for learning more. The command being examined is ls . Use whatis <command-name> to get a brief summary of the command. Use man <command-name> to get the full help file in a less environment. Use q on your keyboard to exit less .","title":"What does a command do? (whatis, man)"},{"location":"workflow_solutions/shell/#remotely-access-shell-on-other-machines-ssh","text":"See our SSH Section for more detailed information.","title":"Remotely access shell on other machines (ssh)"},{"location":"workflow_solutions/shell/#remotely-access-or-transfer-files-between-machines-sftp","text":"See our Remote Access - Data Transfer Section for more detailed information.","title":"Remotely access or transfer files between machines (sftp)"},{"location":"workflow_solutions/shell/#submit-and-manage-jobs-on-cheaha","text":"See our Slurm Section for more detailed information.","title":"Submit and manage jobs on Cheaha"},{"location":"workflow_solutions/shell/#manage-permissions-of-files-and-directores-chmod","text":"Use chmod with the least permissions needed to accomplish a task. Permission management is an important part of managing access and control of files and directories. Danger Please carefully consider security when working in shared spaces like Cheaha. Setting private directories or files as readable by other users can inadvertently expose sensitive or protected information and may violate IT policy, FERPA, HIPAA or some combination. There are legitimate use cases for truly shared spaces. Please Contact Us if you need to share information with other users or collaborators and aren't sure of how to do so securely.","title":"Manage permissions of files and directores (chmod)"},{"location":"workflow_solutions/shell/#what-permissions-do","text":"Setting the permissions of a file affect the contents of only that file. A read-only file can still be deleted by users with write permissions in its parent directory. Read permissions allow viewing and copying contents of a file. Write permissions allow changing the contents of a file, including deleting all of the contents. Execute permissions allow using the file as an executable. Helpful for scripts and compiled programs. Scripts and interpreted language files, like Python, must also have read permission set. Setting the permissions of directories affects what can be done with contained files and directories. Read permission allows the use of ls within the directory. cp may be used to copy files from the directory to somewhere else. Write permission allows creation of files and directories, as well as the use of touch , mv and rm on files and directories, within the directory. cp may be used to copy files into the directory. Executable permission allows setting the directory as working directory and the use of cd into the directory. Permissions are not inherited from their parent directory.","title":"What Permissions Do"},{"location":"workflow_solutions/shell/#how-to-check-permissions","text":"Use ls -ald <path> to see the permissions on the file or directory at <path> . The -d flag lists directories instead of their contents.","title":"How to Check Permissions"},{"location":"workflow_solutions/shell/#patterns-for-setting-permissions","text":"Two separate patterns can be used to set or change permissions on files and directories. Either may be used, but they cannot be combined in a single use of chmod . In the example images below, the command ls is used to check permissions and the command cat is used to display the contents of the script . Symbols The letter and symbol pattern is in the form a=r . There are three parts. A collection of letters denoting who, e.g. a in this case. Multiple letters may be used. u the owner of the file or directory g the owner's group members o users outside the owner's group a all users (same as ugo ). A symbol indicating how to change the permissions = set permissions - remove permissions + add permissions A collection of letters denoting which permissions to change. Multiple letters may be used. r read w write (change the contents) x execute To add executable permission for only the owner chmod u+x <file-path> . Useful for custom scripts and compiled executables you will use directly from the command line. To set read-only permission for everyone use chmod a=r <file-path> . Note Using = to set permissions will both add and remove permissions. Using a=r will take away existing write and execute permissions. Below is an example of chmod used symbolically to set user execute permissions on a script. Note the error before permissions are set. Numerals Bit mask patterns are in the form 755 . Each digit is the sum of three binary bits. The bits are 4 read 2 write 1 execute The left digit is the owner's permissions. Middle digit is the owner's group. Right digit is users outside the owner's group. Setting chmod 755 means the following: For the owner, set 4 read, 2 write and 1 execute. 4+2+1=7 . For the owner's group, set 4 read, 1 execute. 4+1=5 . For other users, set 4 read, 1 execute. 4+1=5 . Setting 755 is a common pattern for system-wide scripts. Because the 4 read and 1 executable bits are set for all users, it can be called from anywhere by any other script, and not just the owner. However, the 2 write bit is set only for the owner, so other users cannot modify the contents of the script. Below is an example of chmod used numerically to add execution permissions to all users. Note the error before permissions are set.","title":"Patterns for Setting Permissions"},{"location":"workflow_solutions/shell/#examples","text":"chmod u+x script.sh adds execute permission for you, to a script. chmod 755 script.sh makes a script readable and executable by all, but only writeable by you. This is a common permission for non-sensitive files and directories. chmod 740 sensitive_directory make a directory readable by you and your group, and writeable and executable by only you. Other users cannot delete files in this folder. chmod ug=r notes.txt followed by chmod o-rwx notes.txt makes a file read-only for you and your group and removes all permissions for other users.","title":"Examples"},{"location":"workflow_solutions/shell/#manage-group-ownership-chgrp","text":"To change group ownership of files and directories use chgrp . The command chgrp may only be used on a file or directory if you own it, or if you are a member of its current group owner and a member of its new group owner. For single files use chgrp <new-group> <file> . To change a directory and all of its contents recursively use chgrp -hR <new-group> <file> . The -h flag will avoid walking through the targets of symbolic links. Warning When using chgrp -R , the default behavior is to walk through the contents of symbolic links. If this is not desired, use -hR .","title":"Manage group ownership (chgrp)"},{"location":"workflow_solutions/shell/#manage-researcher-access-to-files-and-directories-getfacl-setfacl","text":"Construction Under construction.","title":"Manage researcher access to files and directories (getfacl, setfacl)"},{"location":"workflow_solutions/using_anaconda/","text":"Anaconda \u00b6 Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is the standard in Python package management for scientific research. Benefits of Anaconda: Shareability: environments can be shared via human-readable text-based YAML files. Maintainability: the same YAML files can be version controlled using git. Repeatability: environments can be rebuilt using those same YAML files. Simplicity: dependency matrices are computed and solved by Anaconda, and libraries are pre-built and stored on remote servers for download instead of being built on your local machine. Ubiquity: nearly all Python developers are aware of the usage of Anaconda, especially in scientific research, so there are many resources available for learning how to use it, and what to do if something goes wrong. Anaconda can also install Pip and record which Pip packages are installed, so Anaconda can do everything Pip can, and more. Important If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions. What is my best solution for installing Anaconda? \u00b6 If you are using a local machine or doing general purpose software development, or have a particular package in mind, go here to install Anaconda. If you are using a virtual machine or container, go here to install Miniconda. If you are using Cheaha, go here for how to use Anaconda on Cheaha. Installing Anaconda \u00b6 The full Anaconda install is a good choice if you are using a local machine, or doing general Python development work, or have a particular scientific package in mind. Anaconda installation instructions are located here: https://docs.anaconda.com/anaconda/install/index.html . For best performance, be sure to set the default solver to libmamba using conda config --set solver libmamba . For more information see: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default . Installing Miniconda \u00b6 Miniconda is a lightweight version of Anaconda. While Anaconda's base environment comes with Python, the Scipy stack, and other common packages pre-installed, Miniconda comes with no packages installed. This is an excellent alternative to the full Anaconda installation for environments where minimal space is available or where setup time is important, like virtual machines and containers . Miniconda installation instructions are located here: https://docs.conda.io/en/latest/miniconda.html . For best performance, be sure to set the default solver to libmamba using conda config --set solver libmamba . For more information see: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default . Using Anaconda \u00b6 Anaconda is a package manager, meaning it handles all of the difficult mathematics and logistics of figuring out exactly what versions of which packages should be downloaded to meet your needs, or inform you if there is a conflict. Anaconda is structured around environments. Environments are self-contained collections of researcher-selected packages. Environments can be changed out using a simple package without requiring tedious installing and uninstalling of packages or software, and avoiding dependency conflicts with each other. Environments allow researchers to work and collaborate on multiple projects, each with different requirements, all on the same computer. Environments can be installed from the command line, from pre-designed or shared YAML files, and can be modified or updated as needed. The following subsections detail some of the more common commands and use cases for Anaconda usage. More complete information on this process can be found at the Anaconda documentation . Important If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions. Create an Environment \u00b6 In order to create a basic environment with the default packages, use the conda create command: # create a base environment. Replace <env> with an environment name conda create -n <env> If you are trying to replicate a pipeline or analysis from another person, you can also recreate an environment using a YAML file, if they have provided one. To replicate an environment using a YAML file, use: # replicate an environment from a YAML file named env.yml conda create -n <env> -f <path/to/env.yml> By default, all of your conda environments are stored in /home/<user>/.conda/envs . Activate an Environment \u00b6 From here, you can activate the environment using either source or conda : # activate the virtual environment using source source activate <env> # or using conda conda activate <env> To know your environment has loaded, the command line should look like: (<env>) [blazerid@c0XXX ~]$ Once the environment is activated, you are allowed to install whichever python libraries you need for your analysis. Install Packages \u00b6 To install packages using Anaconda, use the conda install command. The -c or --channel command can be used to select a specific package channel to install from. The anaconda channel is a curated collection of high-quality packages, but the very latest versions may not be available on this channel. The conda-forge channel is more open, less carefully curated, and has more recent versions. # install most recent version of a package conda install <package> # install a specific version conda install <package> = version # install from a specific conda channel conda install -c <channel> <package>< = version> Generally, if a package needs to be downloaded from a specific conda channel, it will mention that in its installation instructions. Installing Packages with Pip \u00b6 Some packages are not available through Anaconda. Often these packages are available via PyPi and thus using the Python built-in Pip package manager. Pip may also be used to install locally-available packages as well. # install most recent version of a package pip install \\< package \\> # install a specific version, note the double equals sign pip install \\< package \\> == version # install a list of packages from a text file pip install -r packages.txt Finding Packages \u00b6 You may use the Anaconda page to search for packages on Anaconda, or use Google with something like <package name> conda . To find packages in PyPi, either use the PyPi page to search, or use Google with something like <package name> pip . Packages for Jupyter \u00b6 If you are using Anaconda with Jupyter, you will need to be sure to install the ipykernel package for your environment to be recognized by the Jupyter Server . If you are using Jupyter in Open OnDemand then you do not need to install the jupyter package. Deactivating an Environment \u00b6 An environment can be deactivated using the following command. # Using conda conda deactivate Anaconda may say that using source deactivate is deprecated, but environment will still be deactivated. Closing the terminal will also close out the environment. Working with Environment YAML Files \u00b6 Exporting an Environment \u00b6 To easily share environments with other researchers or replicate it on a new machine, it is useful to create an environment YAML file. You can do this using: # activate the environment if it is not active already conda activate <env> # export the environment to a YAML file conda env export > env.yml Creating an Environment from a YAML File \u00b6 To create an environment from a YAML file env.yml , use the following command. conda env create --file env.yml Replicability versus Portability \u00b6 An environment with only python 3.10.4 , numpy 1.21.5 and jinja2 2.11.2 installed will output something like the following file when conda env export is used. This file may be used to precisely replicate the environment as it exists on the machine where conda env export was run. Note that the versioning for each package contains two = signs. The code like he774522_0 after the second = sign contains hyper-specific build information for the compiled libraries for that package. Sharing this exact file with collaborators may result in frustration if they do not have the exact same operating system and hardware as you, and they would not be able to build this environment. We would say that this environment file is not very portable. There are other portability issues: The prefix: C:\\... line is not used by conda in any way and is deprecated. It also shares system information about file locations which is potentially sensitive information. The channels: group uses - defaults , which may vary depending on how you or your collaborator has customized their Anaconda installation. It may result in packages not being found, resulting in environment creation failure. name : test-env channels : - defaults dependencies : - blas=1.0=mkl - bzip2=1.0.8=he774522_0 - ca-certificates=2022.4.26=haa95532_0 - certifi=2021.5.30=py310haa95532_0 - intel-openmp=2021.4.0=haa95532_3556 - jinja2=2.11.2=pyhd3eb1b0_0 - libffi=3.4.2=h604cdb4_1 - markupsafe=2.1.1=py310h2bbff1b_0 - mkl=2021.4.0=haa95532_640 - mkl-service=2.4.0=py310h2bbff1b_0 - mkl_fft=1.3.1=py310ha0764ea_0 - mkl_random=1.2.2=py310h4ed8f06_0 - numpy=1.21.5=py310h6d2d95c_2 - numpy-base=1.21.5=py310h206c741_2 - openssl=1.1.1o=h2bbff1b_0 - pip=21.2.4=py310haa95532_0 - python=3.10.4=hbb2ffb3_0 - setuptools=61.2.0=py310haa95532_0 - six=1.16.0=pyhd3eb1b0_1 - sqlite=3.38.3=h2bbff1b_0 - tk=8.6.11=h2bbff1b_1 - tzdata=2022a=hda174b7_0 - vc=14.2=h21ff451_1 - vs2015_runtime=14.27.29016=h5e58377_2 - wheel=0.37.1=pyhd3eb1b0_0 - wincertstore=0.2=py310haa95532_2 - xz=5.2.5=h8cc25b3_1 - zlib=1.2.12=h8cc25b3_2 prefix : C:\\Users\\user\\Anaconda3\\envs\\test-env To make this a more portable file, suitable for collaboration, some planning is required. Instead of using conda env export we can build our own file. Create a new file called env.yml using your favorite text editor and add the following. Note we've only listed exactly the packages we installed, and their version numbers, only. This allows Anaconda the flexibility to choose dependencies which do not conflict and do not contain unusable hyper-specific library build information. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - numpy=1.21.5 - python=3.10.4 This is a much more readable and portable file suitable for sharing with collaborators. We aren't quite finished though! Some scientific packages on the conda-forge channel, and on other channels, can contain dependency errors. Those packages may accidentally pull a version of a dependency that breaks their code. For example, the package markupsafe made a not-backward-compatible change (a breaking change) to their code between 2.0.1 and 2.1.1 . Dependent packages expected 2.1.1 to be backward compatible, so their packages allowed 2.1.1 as a substitute for 2.0.1 . Since Anaconda chooses the most recent version allowable, package installs broke. To work around this for our environment, we would need to modify the environment to \"pin\" that package at a specific version, even though we didn't explicitly install it. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - markupsafe=2.0.1 - numpy=1.21.5 - python=3.10.4 Now we can be sure that the correct versions of the software will be installed on our collaborator's machines. Note The example above is provided only for illustration purposes. The error has since been fixed, but the example above really happened and is helpful to explain version pinning. Good Software Development Practice \u00b6 Building on the example above, we can bring in good software development practices to ensure we don't lose track of how our environment is changing as we develop our software or our workflows. If you've ever lost a lot of hard work by accidentally deleting an important file, or forgetting what changes you've made that need to be rolled back, this section is for you. Efficient software developers live the mantra \"Don't repeat yourself\". Part of not repeating yourself is keeping a detailed and meticulous record of changes made as your software grows over time. Git is a way to have the computer keep track of those changes digitally. Git can be used to save changes to environment files as they change over time. Remember that each time your environment changes to commit the output of Exporting your Environment to a repository for your project. Speeding Things up with Mamba \u00b6 Use of Mamba has been deprecated on Cheaha. On Cheaha, use module load Anaconda3 and the usual conda commands instead. The backend of conda has been set to use libmamba and is now equally performant. If you are using Mamba on a local machine and have Anaconda installed, you can set libmamba as your default solver using conda config --set solver libmamba as described here: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default","title":"Using Anaconda"},{"location":"workflow_solutions/using_anaconda/#anaconda","text":"Python is a high level programming language that is widely used in many branches of science. As a result, many scientific packages have been developed in Python, leading to the development of a package manager called Anaconda. Anaconda is the standard in Python package management for scientific research. Benefits of Anaconda: Shareability: environments can be shared via human-readable text-based YAML files. Maintainability: the same YAML files can be version controlled using git. Repeatability: environments can be rebuilt using those same YAML files. Simplicity: dependency matrices are computed and solved by Anaconda, and libraries are pre-built and stored on remote servers for download instead of being built on your local machine. Ubiquity: nearly all Python developers are aware of the usage of Anaconda, especially in scientific research, so there are many resources available for learning how to use it, and what to do if something goes wrong. Anaconda can also install Pip and record which Pip packages are installed, so Anaconda can do everything Pip can, and more. Important If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions.","title":"Anaconda"},{"location":"workflow_solutions/using_anaconda/#what-is-my-best-solution-for-installing-anaconda","text":"If you are using a local machine or doing general purpose software development, or have a particular package in mind, go here to install Anaconda. If you are using a virtual machine or container, go here to install Miniconda. If you are using Cheaha, go here for how to use Anaconda on Cheaha.","title":"What is my best solution for installing Anaconda?"},{"location":"workflow_solutions/using_anaconda/#installing-anaconda","text":"The full Anaconda install is a good choice if you are using a local machine, or doing general Python development work, or have a particular scientific package in mind. Anaconda installation instructions are located here: https://docs.anaconda.com/anaconda/install/index.html . For best performance, be sure to set the default solver to libmamba using conda config --set solver libmamba . For more information see: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default .","title":"Installing Anaconda"},{"location":"workflow_solutions/using_anaconda/#installing-miniconda","text":"Miniconda is a lightweight version of Anaconda. While Anaconda's base environment comes with Python, the Scipy stack, and other common packages pre-installed, Miniconda comes with no packages installed. This is an excellent alternative to the full Anaconda installation for environments where minimal space is available or where setup time is important, like virtual machines and containers . Miniconda installation instructions are located here: https://docs.conda.io/en/latest/miniconda.html . For best performance, be sure to set the default solver to libmamba using conda config --set solver libmamba . For more information see: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default .","title":"Installing Miniconda"},{"location":"workflow_solutions/using_anaconda/#using-anaconda","text":"Anaconda is a package manager, meaning it handles all of the difficult mathematics and logistics of figuring out exactly what versions of which packages should be downloaded to meet your needs, or inform you if there is a conflict. Anaconda is structured around environments. Environments are self-contained collections of researcher-selected packages. Environments can be changed out using a simple package without requiring tedious installing and uninstalling of packages or software, and avoiding dependency conflicts with each other. Environments allow researchers to work and collaborate on multiple projects, each with different requirements, all on the same computer. Environments can be installed from the command line, from pre-designed or shared YAML files, and can be modified or updated as needed. The following subsections detail some of the more common commands and use cases for Anaconda usage. More complete information on this process can be found at the Anaconda documentation . Important If using Anaconda on Cheaha, please see our Anaconda on Cheaha page for important details and restrictions.","title":"Using Anaconda"},{"location":"workflow_solutions/using_anaconda/#create-an-environment","text":"In order to create a basic environment with the default packages, use the conda create command: # create a base environment. Replace <env> with an environment name conda create -n <env> If you are trying to replicate a pipeline or analysis from another person, you can also recreate an environment using a YAML file, if they have provided one. To replicate an environment using a YAML file, use: # replicate an environment from a YAML file named env.yml conda create -n <env> -f <path/to/env.yml> By default, all of your conda environments are stored in /home/<user>/.conda/envs .","title":"Create an Environment"},{"location":"workflow_solutions/using_anaconda/#activate-an-environment","text":"From here, you can activate the environment using either source or conda : # activate the virtual environment using source source activate <env> # or using conda conda activate <env> To know your environment has loaded, the command line should look like: (<env>) [blazerid@c0XXX ~]$ Once the environment is activated, you are allowed to install whichever python libraries you need for your analysis.","title":"Activate an Environment"},{"location":"workflow_solutions/using_anaconda/#install-packages","text":"To install packages using Anaconda, use the conda install command. The -c or --channel command can be used to select a specific package channel to install from. The anaconda channel is a curated collection of high-quality packages, but the very latest versions may not be available on this channel. The conda-forge channel is more open, less carefully curated, and has more recent versions. # install most recent version of a package conda install <package> # install a specific version conda install <package> = version # install from a specific conda channel conda install -c <channel> <package>< = version> Generally, if a package needs to be downloaded from a specific conda channel, it will mention that in its installation instructions.","title":"Install Packages"},{"location":"workflow_solutions/using_anaconda/#installing-packages-with-pip","text":"Some packages are not available through Anaconda. Often these packages are available via PyPi and thus using the Python built-in Pip package manager. Pip may also be used to install locally-available packages as well. # install most recent version of a package pip install \\< package \\> # install a specific version, note the double equals sign pip install \\< package \\> == version # install a list of packages from a text file pip install -r packages.txt","title":"Installing Packages with Pip"},{"location":"workflow_solutions/using_anaconda/#finding-packages","text":"You may use the Anaconda page to search for packages on Anaconda, or use Google with something like <package name> conda . To find packages in PyPi, either use the PyPi page to search, or use Google with something like <package name> pip .","title":"Finding Packages"},{"location":"workflow_solutions/using_anaconda/#packages-for-jupyter","text":"If you are using Anaconda with Jupyter, you will need to be sure to install the ipykernel package for your environment to be recognized by the Jupyter Server . If you are using Jupyter in Open OnDemand then you do not need to install the jupyter package.","title":"Packages for Jupyter"},{"location":"workflow_solutions/using_anaconda/#deactivating-an-environment","text":"An environment can be deactivated using the following command. # Using conda conda deactivate Anaconda may say that using source deactivate is deprecated, but environment will still be deactivated. Closing the terminal will also close out the environment.","title":"Deactivating an Environment"},{"location":"workflow_solutions/using_anaconda/#working-with-environment-yaml-files","text":"","title":"Working with Environment YAML Files"},{"location":"workflow_solutions/using_anaconda/#exporting-an-environment","text":"To easily share environments with other researchers or replicate it on a new machine, it is useful to create an environment YAML file. You can do this using: # activate the environment if it is not active already conda activate <env> # export the environment to a YAML file conda env export > env.yml","title":"Exporting an Environment"},{"location":"workflow_solutions/using_anaconda/#creating-an-environment-from-a-yaml-file","text":"To create an environment from a YAML file env.yml , use the following command. conda env create --file env.yml","title":"Creating an Environment from a YAML File"},{"location":"workflow_solutions/using_anaconda/#replicability-versus-portability","text":"An environment with only python 3.10.4 , numpy 1.21.5 and jinja2 2.11.2 installed will output something like the following file when conda env export is used. This file may be used to precisely replicate the environment as it exists on the machine where conda env export was run. Note that the versioning for each package contains two = signs. The code like he774522_0 after the second = sign contains hyper-specific build information for the compiled libraries for that package. Sharing this exact file with collaborators may result in frustration if they do not have the exact same operating system and hardware as you, and they would not be able to build this environment. We would say that this environment file is not very portable. There are other portability issues: The prefix: C:\\... line is not used by conda in any way and is deprecated. It also shares system information about file locations which is potentially sensitive information. The channels: group uses - defaults , which may vary depending on how you or your collaborator has customized their Anaconda installation. It may result in packages not being found, resulting in environment creation failure. name : test-env channels : - defaults dependencies : - blas=1.0=mkl - bzip2=1.0.8=he774522_0 - ca-certificates=2022.4.26=haa95532_0 - certifi=2021.5.30=py310haa95532_0 - intel-openmp=2021.4.0=haa95532_3556 - jinja2=2.11.2=pyhd3eb1b0_0 - libffi=3.4.2=h604cdb4_1 - markupsafe=2.1.1=py310h2bbff1b_0 - mkl=2021.4.0=haa95532_640 - mkl-service=2.4.0=py310h2bbff1b_0 - mkl_fft=1.3.1=py310ha0764ea_0 - mkl_random=1.2.2=py310h4ed8f06_0 - numpy=1.21.5=py310h6d2d95c_2 - numpy-base=1.21.5=py310h206c741_2 - openssl=1.1.1o=h2bbff1b_0 - pip=21.2.4=py310haa95532_0 - python=3.10.4=hbb2ffb3_0 - setuptools=61.2.0=py310haa95532_0 - six=1.16.0=pyhd3eb1b0_1 - sqlite=3.38.3=h2bbff1b_0 - tk=8.6.11=h2bbff1b_1 - tzdata=2022a=hda174b7_0 - vc=14.2=h21ff451_1 - vs2015_runtime=14.27.29016=h5e58377_2 - wheel=0.37.1=pyhd3eb1b0_0 - wincertstore=0.2=py310haa95532_2 - xz=5.2.5=h8cc25b3_1 - zlib=1.2.12=h8cc25b3_2 prefix : C:\\Users\\user\\Anaconda3\\envs\\test-env To make this a more portable file, suitable for collaboration, some planning is required. Instead of using conda env export we can build our own file. Create a new file called env.yml using your favorite text editor and add the following. Note we've only listed exactly the packages we installed, and their version numbers, only. This allows Anaconda the flexibility to choose dependencies which do not conflict and do not contain unusable hyper-specific library build information. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - numpy=1.21.5 - python=3.10.4 This is a much more readable and portable file suitable for sharing with collaborators. We aren't quite finished though! Some scientific packages on the conda-forge channel, and on other channels, can contain dependency errors. Those packages may accidentally pull a version of a dependency that breaks their code. For example, the package markupsafe made a not-backward-compatible change (a breaking change) to their code between 2.0.1 and 2.1.1 . Dependent packages expected 2.1.1 to be backward compatible, so their packages allowed 2.1.1 as a substitute for 2.0.1 . Since Anaconda chooses the most recent version allowable, package installs broke. To work around this for our environment, we would need to modify the environment to \"pin\" that package at a specific version, even though we didn't explicitly install it. name : test-env channels : - anaconda dependencies : - jinja2=2.11.2 - markupsafe=2.0.1 - numpy=1.21.5 - python=3.10.4 Now we can be sure that the correct versions of the software will be installed on our collaborator's machines. Note The example above is provided only for illustration purposes. The error has since been fixed, but the example above really happened and is helpful to explain version pinning.","title":"Replicability versus Portability"},{"location":"workflow_solutions/using_anaconda/#good-software-development-practice","text":"Building on the example above, we can bring in good software development practices to ensure we don't lose track of how our environment is changing as we develop our software or our workflows. If you've ever lost a lot of hard work by accidentally deleting an important file, or forgetting what changes you've made that need to be rolled back, this section is for you. Efficient software developers live the mantra \"Don't repeat yourself\". Part of not repeating yourself is keeping a detailed and meticulous record of changes made as your software grows over time. Git is a way to have the computer keep track of those changes digitally. Git can be used to save changes to environment files as they change over time. Remember that each time your environment changes to commit the output of Exporting your Environment to a repository for your project.","title":"Good Software Development Practice"},{"location":"workflow_solutions/using_anaconda/#speeding-things-up-with-mamba","text":"Use of Mamba has been deprecated on Cheaha. On Cheaha, use module load Anaconda3 and the usual conda commands instead. The backend of conda has been set to use libmamba and is now equally performant. If you are using Mamba on a local machine and have Anaconda installed, you can set libmamba as your default solver using conda config --set solver libmamba as described here: https://conda.github.io/conda-libmamba-solver/getting-started/#set-as-default","title":"Speeding Things up with Mamba"},{"location":"workflow_solutions/using_workflow_managers/","text":"Workflow Managers \u00b6 Construction This page is a stub and is under construction. Snakemake \u00b6 Pegasus \u00b6 Nextflow \u00b6","title":"Using Workflow Managers"},{"location":"workflow_solutions/using_workflow_managers/#workflow-managers","text":"Construction This page is a stub and is under construction.","title":"Workflow Managers"},{"location":"workflow_solutions/using_workflow_managers/#snakemake","text":"","title":"Snakemake"},{"location":"workflow_solutions/using_workflow_managers/#pegasus","text":"","title":"Pegasus"},{"location":"workflow_solutions/using_workflow_managers/#nextflow","text":"","title":"Nextflow"}]}